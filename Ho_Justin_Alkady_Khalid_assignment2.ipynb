{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second programming assignment for CSCE478/878 Introduction to Machine Learning on Linear Regression. This notebook is divided into 3 sections, namely\n",
    "1. **Part A (Model Code)**\n",
    "1. **Part B (Data Processing)**\n",
    "1. **Part C (Model Evaluation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the following function that generates the polynomial and interaction features for a given degree of the polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes the feature matrix X and the polynomial degree and returns\n",
    "X with features of polynomial degrees and pairwise interaction\n",
    "terms\n",
    "\n",
    "Input:\n",
    "    X - 2D np.ndarray feature matrix\n",
    "    degree - integer > 0\n",
    "    \n",
    "Output:\n",
    "    X - 2D np.ndarray polynomial feature matrix \n",
    "\"\"\"\n",
    "# Doesn't Include the Bias Term, as it will be added in the regression functions\n",
    "def polynomialFeatures(X, degree):\n",
    "    \n",
    "    X_original=X.copy()\n",
    "     #Create polynomials of degree n\n",
    "    if degree > 1:\n",
    "        for d in range(2, degree+1):\n",
    "     #Create combinations of interaction terms\n",
    "            interactions =  lambda x: np.multiply.reduce(np.array(list(itertools.combinations_with_replacement(x.T,d))),1).T\n",
    "            X = np.concatenate((X,interactions(X_original)), axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the following function to calculate and return the mean squared error (mse) of two vectors.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates the mean squared error (mse) between the true and predicted labels\n",
    "Input: \n",
    "    true: array_like type vector of true labels\n",
    "    pred: array_like type vector of predicted labels\n",
    "Output:\n",
    "    mean squared error: accuracy expressed in decimal\n",
    "\"\"\"\n",
    "def MSE_error(true,pred):\n",
    "    diff = true - pred\n",
    "    error = np.dot(diff,diff)/true.shape[0]\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Implement the following function to plot the training and validation root mean square error (rmse) values of the data matrix X for various polynomial degree starting from 1 up to the value set by the argument “maxPolynomialDegree” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polynomial_model_complexity(estimator,X,y,cv,max_degree):\n",
    "    \n",
    "    range_degree=np.array([d for d in range(1,max_degree+1)]) \n",
    "    X_original=X.copy()\n",
    "    avg_score_train=[]\n",
    "    avg_score_val=[]\n",
    "    #Running for every degree\n",
    "    for d in range(1,max_degree+1):\n",
    "        \n",
    "        X2=polynomialFeatures(X_original,d)\n",
    "        size = len(X2)\n",
    "        size_fold = int(size/cv)\n",
    "        range_index = [j for j in range(0,len(X2))]\n",
    "        score_val=[]\n",
    "        score_train=[]\n",
    "        #Data Standardization\n",
    "        r,c=np.shape(X2)\n",
    "        for col in range(0,c):\n",
    "            m=np.mean(X2[:,col])\n",
    "            s=np.std(X2[:,col])\n",
    "            if s>0:\n",
    "                X2[:,col]=(X2[:,col]-m)/s\n",
    "    #Partitioning of the data    \n",
    "        for i in range(cv):\n",
    "            init=0+i*size_fold\n",
    "            fin=(i+1)*size_fold\n",
    "            partition_range_index = [j for j in range(init,fin)]\n",
    "            \n",
    "            #Feature and label data of the Fold\n",
    "            X_partition = X2[partition_range_index]\n",
    "            y_partition = y[partition_range_index]\n",
    "            \n",
    "            #Feature and label data of  1-Fold        \n",
    "            remainder_index = list(set(range_index).difference(set(partition_range_index)))\n",
    "            X_remainder=X2[remainder_index]\n",
    "            y_remainder=y[remainder_index]\n",
    "            \n",
    "            #Fit the model to the 1-Fold data\n",
    "            estimator.fit(X_remainder, y_remainder)\n",
    "            \n",
    "            #Test the model on the 1-Fold data\n",
    "            pred=estimator.predict(X_remainder)\n",
    "            rmse=math.sqrt(MSE_error(y_remainder,pred))\n",
    "            score_train.append(rmse) \n",
    "            \n",
    "            #Test the model on the fold data\n",
    "            pred=estimator.predict(X_partition)\n",
    "            rmse=math.sqrt(MSE_error(y_partition,pred))\n",
    "            score_val.append(rmse) \n",
    "        avg_score_train.append(np.mean(score_train))\n",
    "        avg_score_val.append(np.mean(score_val))\n",
    "    \n",
    "           \n",
    "    #generate the plot\n",
    "    plt.figure()\n",
    "    plt.plot(range_degree,avg_score_train)\n",
    "    plt.plot(range_degree,avg_score_val)\n",
    "    plt.xlabel(\"Polynomial Degree\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title('Polynomial Model Complexity')\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Implement a Linear_Regression model class. It should have the following three methods. Note the that “fit” method should implement the batch gradient descent algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear Regression that takes optional hyperparameters. Contains methods fit using Batch Gradient Descent to\n",
    "estimate the weight parameters of the model.\n",
    "Methods:\n",
    "    fit - Given X and y, using batch gradient descent to find w\n",
    "    predict - Given X, using w to provide prediction\n",
    "'''\n",
    "class Linear_Regression():\n",
    "    def __init__(self, learning_rate=0.01, epochs=100, tol=None, regularizer=None, lambd=0.0, **kwargs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.tol = tol\n",
    "        self.regularizer = regularizer\n",
    "        self.lambd = lambd\n",
    "        self.w = None\n",
    "        self.mse = []\n",
    "        self.mse_test = []\n",
    "        self.learning_curve = False\n",
    "        if \"learning_curve\" in kwargs:\n",
    "            self.learning_curve = True\n",
    "            self.X_test, self.y_test = kwargs[\"learning_curve\"]\n",
    "            \n",
    "            x_0 = np.ones((self.X_test.shape[0],1))\n",
    "            self.X_test = np.concatenate((x_0,self.X_test), axis=1)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Add x_0 = 1 for all for intercept and concat data\n",
    "        x_0 = np.ones((X.shape[0],1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        \n",
    "        # Init prev_cost with 0\n",
    "        prev_cost = 0\n",
    "        \n",
    "        # Number of training samples\n",
    "        m = len(X)\n",
    "        # Initialize all weights to 0\n",
    "        self.w = np.zeros((X.shape[1],))\n",
    "        \n",
    "        # Run batch gradient descent up to self.epoch times\n",
    "        for i in range(self.epochs):\n",
    "            # Calculate mse with current weights\n",
    "            new_cost=0.5*MSE_error(X.dot(self.w),y)\n",
    "            self.mse.append(2*new_cost)\n",
    "             # Calculate MSE of test data\n",
    "            if self.learning_curve:\n",
    "                self.mse_test.append(MSE_error(self.X_test.dot(self.w),self.y_test))\n",
    "            \n",
    "            if self.regularizer == \"l2\":\n",
    "                regularized_term = 0.5*self.lambd*np.dot(self.w[1:],self.w[1:])/m # Exclude the bias term\n",
    "                new_cost=new_cost + regularized_term\n",
    "                \n",
    "            elif self.regularizer == 'l1':\n",
    "                regularized_term = 0.5*self.lambd*np.sum(abs(self.w[1:]))/m  # Exclude the bias term\n",
    "                new_cost=new_cost + regularized_term\n",
    "            \n",
    "            # Break if absolute cost of previous cost and current cost is smaller than self.tol\n",
    "            if self.tol is not None:\n",
    "                if abs(prev_cost - new_cost) > self.tol:\n",
    "                    prev_cost = new_cost\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Calculate gradient\n",
    "            grad = (X.T.dot(X.dot(self.w)-y))\n",
    "            \n",
    "            # Apply Regularization term to gradient\n",
    "            if self.regularizer == \"l2\":\n",
    "                regularized_term = self.lambd*self.w\n",
    "                regularized_term[0] = 0 # Exclude the bias term\n",
    "                grad = grad + regularized_term\n",
    "                \n",
    "            elif self.regularizer == 'l1':\n",
    "                regularized_term = self.lambd * np.sign(self.w)\n",
    "                regularized_term[0] = 0 # Exclude the bias term\n",
    "                grad = grad + regularized_term\n",
    "            \n",
    "            # Update weights\n",
    "            self.w = self.w - (self.learning_rate/m)*grad\n",
    "            \n",
    "           \n",
    "                \n",
    "    def predict(self, X):\n",
    "        \n",
    "        x_0 = np.ones((X.shape[0],1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        \n",
    "        pred = X.dot(self.w)\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read in the winequality-red.csv file as a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use the techniques from the recitation to summarize each of the variables in the dataset in terms of mean, standard deviation, and quartiles. Include this in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Shuffle the rows of your data. You can use def = df.sample(frac=1) as an idiomatic way to shuffle the data in Pandas without losing column names. Create a test dataset by randomly sampling 20% of the data. Remaining data should be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(X, y, t):\n",
    "    # Determine sizes of sample, training and test set\n",
    "    n = len(y)\n",
    "    size_train = int(t * n)\n",
    "    size_test = 1 - size_train\n",
    "    \n",
    "    # Generate list of all index\n",
    "    range_index = [x for x in range(0,n)]\n",
    "    # Generate list of random index with the size of training set\n",
    "    train_index = random.sample(range(0, n), size_train)\n",
    "    # Obtain the set difference between all the training for test \n",
    "    test_index = list(set(range_index).difference(set(train_index)))\n",
    "    \n",
    "    # Subsetting train and test\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(columns=['quality'],axis=1))\n",
    "y = np.array(df['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = partition(X, y, 0.8)\n",
    "X_train_poly=X_train.copy()  #To be used for polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Standardization\n",
    "r,c=np.shape(X_train)\n",
    "for col in range(0,c):\n",
    "    m=np.mean(X_train[:,col])\n",
    "    s=np.std(X_train[:,col])\n",
    "    X_train[:,col]=(X_train[:,col]-m)/s\n",
    "    # standardize test Data on the mean and sd of Train Data  \n",
    "    X_test[:,col]=(X_test[:,col]-m)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model selection via Hyperparameter tuning: Use the kFold function (known as sFold function from previous assignment) to evaluate the performance of your model over each combination of lambd, learning_rate and regularizer hyperparameters from the following sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sFold(estimator,X,y,scoring,cv):\n",
    "    size = len(X)\n",
    "    size_fold = int(size/cv)\n",
    "    range_index = [j for j in range(0,len(X))]\n",
    "    score=[]\n",
    "    #Partitioning of the data\n",
    "    for i in range(cv):\n",
    "        init=0+i*size_fold\n",
    "        fin=(i+1)*size_fold\n",
    "        partition_range_index = [j for j in range(init,fin)]\n",
    "        \n",
    "        #Feature and label data of the Fold\n",
    "        X_partition = X[partition_range_index]\n",
    "        y_partition = y[partition_range_index]\n",
    "        \n",
    "        #Feature and label data of  1-Fold        \n",
    "        remainder_index = list(set(range_index).difference(set(partition_range_index)))\n",
    "        X_remainder=X[remainder_index]\n",
    "        y_remainder=y[remainder_index]\n",
    "        \n",
    "        #Fit the model to the 1-Fold data\n",
    "        estimator.fit(X_remainder, y_remainder) \n",
    "        \n",
    "        #Test the model on the fold data\n",
    "        pred=estimator.predict(X_partition)\n",
    "        if scoring=='MSE':\n",
    "            mse=MSE_error(y_partition,pred)\n",
    "            score.append(mse) \n",
    "            \n",
    "    avg_score = np.mean(score)    \n",
    "    return avg_score,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (0.42922668679087234, [0.40751479918971445, 0.4918655659186371, 0.40869272644051075, 0.4156492445370881, 0.3664179679611008, 0.5398787725843951, 0.3898261210027934, 0.4363968754479039, 0.4452005829113305, 0.3908242119152487]), 'Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (0.42929916985910427, [0.40749684436408945, 0.49234150619884676, 0.40819141067700393, 0.4152276947872646, 0.36632058287866653, 0.5406969328674569, 0.38990845581736244, 0.4358548414952959, 0.44613151785348315, 0.39082191165157293]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.4283926767673087, [0.4077365829523824, 0.49182019630375634, 0.4066638434451165, 0.41657934579244554, 0.3648488425851671, 0.5388469735986391, 0.3868970366881918, 0.43603667336583024, 0.44442775630648695, 0.39006951663507083]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4283624878218998, [0.40770701124951303, 0.4919298592537625, 0.405908898154277, 0.4163212907621659, 0.3643465517311023, 0.5394402876920025, 0.3870478586109978, 0.43568278783337133, 0.4449733141238937, 0.3902670188079121]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (4.806398065741785, [4.573065623120347, 4.800799791262365, 4.708422207683019, 4.854068233515718, 5.161257542268255, 4.6343832922149915, 4.985335665911103, 4.8542022305063925, 4.74600726757993, 4.746438803355738]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (4.806679867474846, [4.572827953077544, 4.800999542838575, 4.708701745721218, 4.853450175708316, 5.1617890954961325, 4.634781519660912, 4.985311733425877, 4.8543664247337475, 4.7471522138249265, 4.747418270261206]), 'Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (0.429229806153134, [0.4075144672013225, 0.4918603124831872, 0.4087056148425959, 0.41564877447891696, 0.3664213041306413, 0.5398773662048894, 0.3898324858193784, 0.43640099866427645, 0.4452059603998052, 0.3908307773063267]), 'Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (0.42930238061238735, [0.4074965597802721, 0.49233804483191257, 0.40820445152422924, 0.415227302721732, 0.3663248920456378, 0.5406961832066179, 0.3899135539026795, 0.43585782692123387, 0.44613814662884654, 0.39082684456071143]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.42922662876185685, [0.40751480445332283, 0.49186563994236115, 0.408692515684987, 0.4156492519121714, 0.3664178796676882, 0.539878788813475, 0.3898260166590361, 0.4363967925375621, 0.44520048962533904, 0.3908241083226257]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.4292991176204753, [0.40749684926551544, 0.4923415620740685, 0.4081912097244453, 0.415227701512567, 0.36632050880601386, 0.5406969442169894, 0.38990836725021566, 0.4358547916141896, 0.44613140838570203, 0.3908218333550463]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.42839162678252524, [0.40773439365376524, 0.4918202967279791, 0.40666209893893573, 0.4165793490260232, 0.3648513943655243, 0.5388424562239215, 0.38689464486355624, 0.4360373595187269, 0.4444255154682893, 0.3900687590385308]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.4283613700099041, [0.4077048391730904, 0.49192983859944844, 0.40590702985751415, 0.4163217258325543, 0.3643486678432276, 0.5394354974223633, 0.38704513951584113, 0.43568382316269616, 0.4449708826412432, 0.390266256051062]), 'Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (0.42933380495571616, [0.40748325059204116, 0.4923767459665067, 0.40823266734302205, 0.4151952422829556, 0.36637855228957644, 0.5408109932937815, 0.38999184026110034, 0.43583135316753624, 0.44615543945521796, 0.39088196490542315]), 'Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (0.42933380495571616, [0.40748325059204116, 0.4923767459665067, 0.40823266734302205, 0.4151952422829556, 0.36637855228957644, 0.5408109932937815, 0.38999184026110034, 0.43583135316753624, 0.44615543945521796, 0.39088196490542315]), 'Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.4283723380546606, [0.4076934232080702, 0.4919474595132107, 0.4058892376039696, 0.4163063239706284, 0.36434912865295627, 0.5395326782707056, 0.3870791744809504, 0.435641279641147, 0.444977784504982, 0.39030689069998603]), 'Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4283723380546606, [0.4076934232080702, 0.4919474595132107, 0.4058892376039696, 0.4163063239706284, 0.36434912865295627, 0.5395326782707056, 0.3870791744809504, 0.435641279641147, 0.444977784504982, 0.39030689069998603]), 'Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (4.806688937503064, [4.572815300574711, 4.801028428324301, 4.7087262412826405, 4.853391460268582, 5.1618262405854995, 4.634791071170038, 4.985282191193913, 4.854389041602387, 4.747178518693629, 4.747460881334947]), 'Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (4.806688937503064, [4.572815300574711, 4.801028428324301, 4.7087262412826405, 4.853391460268582, 5.1618262405854995, 4.634791071170038, 4.985282191193913, 4.854389041602387, 4.747178518693629, 4.747460881334947]), 'Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (0.42933737474594497, [0.4074829460316879, 0.4923729759997058, 0.4082471470986767, 0.4151948106491404, 0.3663833814814676, 0.5408101512378545, 0.38999747387289846, 0.4358346874490718, 0.4461627200194795, 0.39088745361946703]), 'Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (0.42933737474594497, [0.4074829460316879, 0.4923729759997058, 0.4082471470986767, 0.4151948106491404, 0.3663833814814676, 0.5408101512378545, 0.38999747387289846, 0.4358346874490718, 0.4461627200194795, 0.39088745361946703]), 'Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.42933374856781314, [0.407483255689759, 0.492376805043588, 0.4082324508977679, 0.4151952494772796, 0.36637847163401277, 0.5408110056581034, 0.3899917451555094, 0.4358312990611138, 0.4461553227019058, 0.3908818803590925]), 'Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.42933374856781314, [0.407483255689759, 0.492376805043588, 0.4082324508977679, 0.4151952494772796, 0.36637847163401277, 0.5408110056581034, 0.3899917451555094, 0.4358312990611138, 0.4461553227019058, 0.3908818803590925]), 'Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4283712060360064, [0.4076912397480286, 0.49194742994053325, 0.4058873596878303, 0.4163067618832829, 0.36435123435436717, 0.5395278566746712, 0.38707642437576545, 0.4356423066298373, 0.44497533523694915, 0.3903061118287981]), 'Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.4283712060360064, [0.4076912397480286, 0.49194742994053325, 0.4058873596878303, 0.4163067618832829, 0.36435123435436717, 0.5395278566746712, 0.38707642437576545, 0.4356423066298373, 0.44497533523694915, 0.3903061118287981]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (0.4293211874295011, [0.40748522430155826, 0.4923235279833583, 0.40827720715682647, 0.41523873151389445, 0.36638140692059884, 0.5407147880078222, 0.38997337045410974, 0.4358863278584251, 0.44605730076935435, 0.3908739893290635]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (0.42933029210158946, [0.4074845946345052, 0.4923731834384227, 0.4082284356433142, 0.41519849718951074, 0.3663726595849197, 0.5407995472680884, 0.3899834249950972, 0.4358336568494509, 0.4461530145531338, 0.39087590685945167]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.42837292406627026, [0.40769670070446135, 0.4919333865062933, 0.4059652860203632, 0.4163316885234575, 0.36439785394742785, 0.5394626608080678, 0.387059709127417, 0.43567961259724597, 0.444920648635731, 0.3902816937922375]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.428371339183073, [0.4076947718757121, 0.4919456829120697, 0.405891191875919, 0.41630781730066124, 0.36434885365650693, 0.5395234155490654, 0.3870760219656786, 0.4356454205810931, 0.4449773279372334, 0.3903028881767907]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (4.806659538465337, [4.57284026463069, 4.8010052540026, 4.708695723896259, 4.853459243993694, 5.161768479686931, 4.634749989326547, 4.985287464609596, 4.8543696093543796, 4.747061003787616, 4.747358351365057]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (4.806688028836852, [4.572816564730314, 4.801025537728056, 4.708723789545299, 4.853397331826891, 5.161822523603588, 4.63479011457568, 4.985285144804717, 4.85438677794095, 4.747175885930535, 4.747456617682504]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (0.4293246522712102, [0.4074849162968028, 0.49231960618932463, 0.4082913385083795, 0.41523829389700456, 0.36638569992970105, 0.540713881294241, 0.3899790752575824, 0.4358897395139389, 0.44606438962475786, 0.39087958220036895]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (0.42933382420296207, [0.40748429211097353, 0.49236944552341627, 0.4082427644329395, 0.41519806968407347, 0.3663774340372951, 0.540798714940085, 0.38998900255135865, 0.4358369544442134, 0.44616022710145975, 0.3908813372038058]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.4293211321378171, [0.4074852294262102, 0.492323588725343, 0.4082769954892788, 0.41523873874948414, 0.3663813333501001, 0.5407148007463936, 0.38997327443313334, 0.4358862711336132, 0.44605718593490845, 0.3908739033897055]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.4293302361427148, [0.40748459971270595, 0.49237324218848594, 0.4082282207982845, 0.4151985043356547, 0.36637257961220926, 0.540799559527262, 0.3899833305638784, 0.43583360318084335, 0.44615289854831963, 0.39087582295950396]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.42837179692218463, [0.40769452017998153, 0.49193335481410383, 0.40596339660761743, 0.41633205992167643, 0.3643999802198537, 0.5394578538747498, 0.3870570029405191, 0.4356806398741159, 0.44491822468749626, 0.39028093610173215]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.42837020859213926, [0.4076925895617304, 0.49194565423481385, 0.4058893149258558, 0.416308254932695, 0.3643509604026252, 0.5395185970975584, 0.3870732749717635, 0.4356464484101614, 0.44497488045735933, 0.3903021109268292]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (0.4293325264747298, [0.40748343615149024, 0.4923714031684081, 0.4082371104432927, 0.41519957209591807, 0.3663788463793412, 0.5408013429320437, 0.38998997429753385, 0.43583683486764974, 0.44614559906800316, 0.3908811453436168]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (0.4293334531717107, [0.4074833848416257, 0.49237638932552313, 0.4082322430985301, 0.4151955678708628, 0.36637796205049683, 0.5408098482901975, 0.3899909979596274, 0.4358315830784124, 0.44615519663459696, 0.3908813585672331]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.42837238159295127, [0.4076937407969255, 0.4919460381117897, 0.40589682633534124, 0.41630883541073005, 0.36435398848123524, 0.5395256612703042, 0.38707721721308785, 0.4356451005193632, 0.4449720501958492, 0.39030435759488685]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.42837223802866725, [0.4076935579731738, 0.4919472816868804, 0.4058894329129994, 0.41630647327005227, 0.36434910097980844, 0.5395317517613832, 0.3870788590196164, 0.4356416936360461, 0.4449777387527224, 0.3903064902939897]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (4.806685994125443, [4.572817796414221, 4.801026107824248, 4.708723188428591, 4.853398235574214, 5.161820453902124, 4.634786962973169, 4.98528271785968, 4.854387089856888, 4.747166763305063, 4.747450625116247]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (4.806688846619802, [4.572815426979327, 4.801028139244188, 4.70872599608708, 4.853392047424552, 5.161825868862551, 4.634790975496163, 4.9852824865488765, 4.854388815216489, 4.747178255394553, 4.747460454944245]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (0.4293360828423169, [0.4074831312396808, 0.4923676179840805, 0.4082515496860002, 0.4151991398425756, 0.366383598588124, 0.54080049432909, 0.3899956150132672, 0.43584017686499194, 0.44615286043567987, 0.39088664443967924]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (0.4293370191745326, [0.40748308048533766, 0.4923726225761532, 0.40824670768419946, 0.4151951366516541, 0.3663827857399959, 0.5408090072121358, 0.3899966259399615, 0.4358349136725603, 0.4461624703679707, 0.39088684141535784]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.42933247025215415, [0.40748344125198727, 0.49237146241237, 0.40823689448346057, 0.41519957929496254, 0.3663787669826848, 0.5408013553343566, 0.389989879104149, 0.4358367804971144, 0.4461454824918377, 0.390881060668619]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.42933339682685484, [0.40748338993739364, 0.4923764483698338, 0.4082320268138198, 0.4151955750603559, 0.3663778814634704, 0.5408098606439666, 0.38999090292168304, 0.43583152901592825, 0.44615507995633724, 0.3908812740857599]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4283712506709908, [0.4076915574193692, 0.49194600898529584, 0.40589494914004887, 0.41630927237147497, 0.3643560961849229, 0.5395208418075352, 0.3870744698419194, 0.43564612840992417, 0.4449696027451471, 0.3903035798042708]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.4283711061528557, [0.4076913746278149, 0.4919472522037807, 0.40588755509350866, 0.41630691115468116, 0.36435120678572425, 0.5395269304799213, 0.38707610922566227, 0.4356427207088378, 0.4449752896636031, 0.3903057115850227]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (0.4293336769636354, [0.407483269029871, 0.4923762114766989, 0.4082331115222765, 0.41519567507315047, 0.3663785820397962, 0.5408100279592767, 0.3899916534749152, 0.4358319011798579, 0.4461544551513102, 0.3908818827292009]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (0.42933376977232446, [0.40748326401545165, 0.49237671029852387, 0.4082326249078132, 0.41519527484271945, 0.3663784932559713, 0.5408108787894121, 0.38999175602319774, 0.43583137615404494, 0.4461554151698488, 0.3908819042662618]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.42837234227625737, [0.4076934548653487, 0.49194731723206125, 0.40588999634659884, 0.41630657498011964, 0.36434961449980036, 0.5395319764181237, 0.3870789787058236, 0.43564166162110857, 0.4449772108576162, 0.3903066372359723]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4283723280506725, [0.4076934366835639, 0.49194744172891497, 0.40588925713369073, 0.4163063389002349, 0.3643491258839059, 0.5395325856174005, 0.3870791429327184, 0.43564132103964553, 0.44497777992880067, 0.3903068506578492]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (4.806688643130368, [4.572815550136112, 4.8010281961163255, 4.708725935975685, 4.853392137768477, 5.161825661892727, 4.6347906603250095, 4.985282243883847, 4.85438884640883, 4.747177343115799, 4.747459855680864]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (4.806688928414575, [4.572815313215069, 4.801028399416093, 4.708726216762868, 4.8533915189841945, 5.161826203412958, 4.63479106160251, 4.985282220729343, 4.85438901896361, 4.747178492363485, 4.747460838695625]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (0.4293372453590206, [0.40748296443430193, 0.49237243998779756, 0.4082475871918727, 0.4151952433771704, 0.3663834030447514, 0.5408091852478341, 0.38999728779695436, 0.43583523623275805, 0.44616173379565627, 0.39088737248110933]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (0.42933733918362665, [0.4074829594755087, 0.4923729406535889, 0.40824710314573304, 0.4151948432503827, 0.3663833218973375, 0.5408100368313221, 0.3899973890715893, 0.4358347100666537, 0.44616269505072753, 0.39088739239342296]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.42933362059192326, [0.4074832741278678, 0.49237627057047223, 0.40823289515466615, 0.4151956822679492, 0.3663785014768806, 0.5408100403274029, 0.3899915583609445, 0.43583184704703243, 0.44615433841600965, 0.3908817981700073]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.42933371338872756, [0.40748326911297494, 0.49237676937232744, 0.4082324084786185, 0.41519528203656025, 0.3663784126072637, 0.5408108911526778, 0.38999166092437326, 0.43583132205201797, 0.44615529842404356, 0.39088181972641894]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.42837121035569636, [0.4076912714135913, 0.4919472876893208, 0.4058881184810138, 0.41630701273569704, 0.36435172041362196, 0.5395271550354739, 0.3870762288151562, 0.43564268870001455, 0.44497476178077877, 0.3903058584922955]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.42837119604630336, [0.4076912532349926, 0.49194741216519583, 0.405887379227217, 0.41630677681008715, 0.36435123159576827, 0.5395277640528249, 0.38707639285865647, 0.4356423480367467, 0.44497533067866096, 0.39030607180288435]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (0.42933379215490036, [0.4074832524346428, 0.4923766925154258, 0.40823271176285775, 0.41519528556006396, 0.3663785552631146, 0.5408108967573477, 0.38999182158058365, 0.4358314079671916, 0.44615534102217536, 0.39088195668560055]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (0.4293338014373271, [0.40748325193436663, 0.4923767423996698, 0.40823266309939354, 0.4151952455389417, 0.3663785463861189, 0.5408109818433046, 0.3899918318372325, 0.43583135546614143, 0.44615543702664806, 0.39088195884145366]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.4283723384754361, [0.40769342637278233, 0.4919474452836858, 0.40588931347692747, 0.4163063490702322, 0.3643491772362814, 0.5395326080839216, 0.38707915490202066, 0.43564131783806426, 0.44497772713839595, 0.39030686535204967]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4283723370542479, [0.4076934245556096, 0.49194745773476445, 0.4058892395569299, 0.41630632546358565, 0.36434912837603406, 0.5395326690053506, 0.38707917132610653, 0.43564128378098665, 0.4449777840473543, 0.39030688669575697]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (4.80668890806554, [4.572815325530629, 4.801028405103289, 4.708726210751721, 4.85339152801827, 5.1618261827159815, 4.634791030085293, 4.985282196462719, 4.854389022082842, 4.747178401135445, 4.747460778769207]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (4.806688936594211, [4.5728153018387445, 4.8010284254334845, 4.708726238830655, 4.8533914661401445, 5.161826236868239, 4.634791070213278, 4.985282194147465, 4.85438903933851, 4.74717851606059, 4.747460877071007]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (0.4293373618052869, [0.4074829478707675, 0.4923729223964116, 0.4082471911063411, 0.41519485392003014, 0.36638338363632217, 0.540810054635861, 0.38999745526340424, 0.4358347423258613, 0.4461626213944427, 0.3908874455034274]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (0.42933737118966137, [0.4074829473760544, 0.49237297246505657, 0.40824714270326745, 0.41519481390927443, 0.3663833755229547, 0.5408101397971616, 0.3899974653926875, 0.43583468971078226, 0.4461627175225682, 0.3908874474968062]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.42933373576884026, [0.40748325753238845, 0.4923767515941766, 0.4082324953225168, 0.4151952927544354, 0.36637847462190315, 0.5408109091220502, 0.38999172647415464, 0.43583135385812893, 0.4461552242706642, 0.39088187213798353]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.4293337450498547, [0.4074832570320654, 0.492376801476423, 0.4082324466557454, 0.4151952527332175, 0.36637846573124006, 0.5408109942075211, 0.3899917367323182, 0.43583130136015846, 0.4461553202740865, 0.3908818742957717]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.42837120646606214, [0.4076912429135688, 0.4919474157140025, 0.40588743556584417, 0.41630678696098633, 0.36435128295983077, 0.539527786509226, 0.38707640481828887, 0.43564234483577624, 0.4449752778894844, 0.3903060864936137]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.42837120503702186, [0.40769124109671323, 0.4919474281629828, 0.40588736164175776, 0.41630676337596006, 0.364351234078489, 0.5395278474124626, 0.38707642122403324, 0.435642310770518, 0.44497533478111123, 0.3903061078261912])}\n"
     ]
    }
   ],
   "source": [
    "lambd =[1.0,0,0.1,0.01,0.001,0.0001] \n",
    "epoch=[1000,10000]\n",
    "learning_rate =[0.1, 0.01, 0.001, 0.001]\n",
    "regularizer = ['l1', 'l2']\n",
    "cv=10\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lam in lambd:\n",
    "    for epo in epoch:\n",
    "        for eta in learning_rate:\n",
    "            for reg in regularizer:\n",
    "                label ='Lambda'+'_'+str(lam) + '_'+'Learning Rate'+'_'+ str(eta) +'_'+'Regularizer'+'_'+str(reg)+'_'+'Epochs'+'_'+ str(epo)\n",
    "                model=Linear_Regression(learning_rate = eta, epochs = epo, regularizer = reg, lambd = lam)\n",
    "                results[label] = sFold(model,X_train,y_train,scoring='MSE',cv=cv)\n",
    "            \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error 0.42922668679087234\n",
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error 0.42929916985910427\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.4283926767673087\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4283624878218998\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 4.806398065741785\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 4.806679867474846\n",
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error 0.429229806153134\n",
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error 0.42930238061238735\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.42922662876185685\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.4292991176204753\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.42839162678252524\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.4283613700099041\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error 0.42933380495571616\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error 0.42933380495571616\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.4283723380546606\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4283723380546606\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 4.806688937503064\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 4.806688937503064\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error 0.42933737474594497\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error 0.42933737474594497\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.42933374856781314\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.42933374856781314\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4283712060360064\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.4283712060360064\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error 0.4293211874295011\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error 0.42933029210158946\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.42837292406627026\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.428371339183073\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 4.806659538465337\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 4.806688028836852\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error 0.4293246522712102\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error 0.42933382420296207\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.4293211321378171\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.4293302361427148\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.42837179692218463\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.42837020859213926\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error 0.4293325264747298\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error 0.4293334531717107\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.42837238159295127\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.42837223802866725\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 4.806685994125443\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 4.806688846619802\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error 0.4293360828423169\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error 0.4293370191745326\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.42933247025215415\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.42933339682685484\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4283712506709908\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.4283711061528557\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error 0.4293336769636354\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error 0.42933376977232446\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.42837234227625737\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4283723280506725\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 4.806688643130368\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 4.806688928414575\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error 0.4293372453590206\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error 0.42933733918362665\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.42933362059192326\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.42933371338872756\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.42837121035569636\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.42837119604630336\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error 0.42933379215490036\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error 0.4293338014373271\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.4283723384754361\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4283723370542479\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 4.80668890806554\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 4.806688936594211\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error 0.4293373618052869\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error 0.42933737118966137\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.42933373576884026\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.4293337450498547\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.42837120646606214\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.42837120503702186\n",
      "0.4283613700099041\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_10000\n"
     ]
    }
   ],
   "source": [
    "min_MSE =100000000\n",
    "min_config = ''\n",
    "for key,pair in results.items():\n",
    "    print(key + \": MSE Error \" + str(pair[0]))\n",
    "    if pair[0] <min_MSE:\n",
    "        min_MSE=pair[0]\n",
    "        min_config=key\n",
    "print(min_MSE)\n",
    "print(min_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Evaluate your model on the test data and report the mean squared error.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE: 0.393\n"
     ]
    }
   ],
   "source": [
    "#Best Model\n",
    "#Regularizer: L1\n",
    "#Learning Rate: 0.001\n",
    "#Lambda: 1\n",
    "#Epochs: 10000\n",
    "\n",
    "model = Linear_Regression(learning_rate = 0.001, epochs = 10000, regularizer = 'l1', lambd = 1)\n",
    "model.fit(X_train, y_train)\n",
    "pred=model.predict(X_test)\n",
    "\n",
    "#MSE\n",
    "mse=MSE_error(y_test,pred)\n",
    "print(\"\\nMSE: %0.3f\" %mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Determine the best model hyperparameter values for the training data matrix with polynomial degree 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-baec7e06a90f>:52: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  regularized_term = 0.5*self.lambd*np.sum(abs(self.w[1:]))/m  # Exclude the bias term\n",
      "<ipython-input-5-baec7e06a90f>:48: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  regularized_term = 0.5*self.lambd*np.dot(self.w[1:],self.w[1:])/m # Exclude the bias term\n",
      "<ipython-input-5-baec7e06a90f>:67: RuntimeWarning: invalid value encountered in multiply\n",
      "  regularized_term = self.lambd*self.w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.44586567758193646, [0.45067316881338254, 0.6504388101888289, 0.333005643689517, 0.4301220156546404, 0.41576528400676827, 0.3945753572345116, 0.39033792673818224, 0.4893283876669584, 0.4687109758145018, 0.43569920601207385]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4526522015953243, [0.45893694191634926, 0.7046292514165995, 0.330619933004911, 0.4255002049093751, 0.41853952149634055, 0.3958081374284533, 0.3920016212641921, 0.4938485408993933, 0.47263627064585123, 0.43400159297177815]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (5.088346424122396, [4.9187852248705966, 6.152078539432384, 5.072203531357946, 4.791479552528291, 4.719029656674813, 4.969097310196336, 4.794072585746791, 5.121107824839747, 5.102981781185524, 5.242628234391524]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (5.10324067999511, [4.938170483732408, 6.191414695193878, 5.081456438450664, 4.797441718729241, 4.725976917243828, 4.977064583898164, 4.804899700272754, 5.1490592148606185, 5.112990765561926, 5.253932282007615]), 'Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.4385317927121424, [0.4710295784446514, 0.5505215262946407, 0.32435421960808625, 0.41434676397601994, 0.4014977716570739, 0.38391039649348463, 0.38609821983354153, 0.5406701211320846, 0.4759340664197316, 0.4369552632621095]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.45306878735629236, [0.5590675350508537, 0.5760313432836514, 0.3221710058879183, 0.4118058745806746, 0.3972927926900272, 0.38026216518082157, 0.3885930315991127, 0.5793860104991, 0.4821815884723547, 0.4338965263184091]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4458794343837405, [0.4506871034177568, 0.6505480438632737, 0.33301117697435606, 0.43012715872357776, 0.4157736218733599, 0.3945729485545662, 0.39034350912848964, 0.48932739921635027, 0.46870506539593976, 0.4356983166897346]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.45265990016462937, [0.4589473653745043, 0.7047123371883279, 0.3306197616661455, 0.4254986642956618, 0.4185386216523603, 0.3958083764309112, 0.3920034856187412, 0.4938477217294637, 0.4726279864622428, 0.43399468122793455]), 'Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.4529240767979246, [0.45919167332188426, 0.706845723708469, 0.33057321616193097, 0.425422472528494, 0.4185851507070172, 0.3958192127307538, 0.39209585998417035, 0.49398755412634493, 0.4727249621195621, 0.43399494259061894]), 'Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4529240767979246, [0.45919167332188426, 0.706845723708469, 0.33057321616193097, 0.425422472528494, 0.4185851507070172, 0.3958192127307538, 0.39209585998417035, 0.49398755412634493, 0.4727249621195621, 0.43399494259061894]), 'Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (5.103411411889807, [4.938334004658399, 6.192121020283266, 5.081522233036957, 4.797485198783098, 4.726022815439205, 4.977118960551709, 4.8050333670066845, 5.1493938330099525, 5.113062571784864, 5.254020114343931]), 'Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (5.103411411889807, [4.938334004658399, 6.192121020283266, 5.081522233036957, 4.797485198783098, 4.726022815439205, 4.977118960551709, 4.8050333670066845, 5.1493938330099525, 5.113062571784864, 5.254020114343931]), 'Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.45486024050329227, [0.5660802949114342, 0.5837004996281723, 0.3220061757239405, 0.4116801454260942, 0.3969606186042176, 0.37994446964591805, 0.3889237006378144, 0.5823548852635375, 0.4830796156497082, 0.43387199954208566]), 'Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.45486024050329227, [0.5660802949114342, 0.5837004996281723, 0.3220061757239405, 0.4116801454260942, 0.3969606186042176, 0.37994446964591805, 0.3889237006378144, 0.5823548852635375, 0.4830796156497082, 0.43387199954208566]), 'Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4529317782860377, [0.45920207225657317, 0.7069289320691914, 0.3305730436487233, 0.4254208936232562, 0.41858419513723866, 0.3958194111858346, 0.39209770206913025, 0.49398691405354406, 0.4727166217003382, 0.4339879971165471]), 'Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.4529317782860377, [0.45920207225657317, 0.7069289320691914, 0.3305730436487233, 0.4254208936232562, 0.41858419513723866, 0.3958194111858346, 0.39209770206913025, 0.49398691405354406, 0.4727166217003382, 0.4339879971165471]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.4520767626379141, [0.4579638203877135, 0.7004053152882066, 0.3308035005918032, 0.4258653002145385, 0.4181755537717758, 0.3956674491264504, 0.39186657478100434, 0.4934202695867433, 0.4723165017948922, 0.4342833408360125]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4528967393970776, [0.4591660700293815, 0.7066229821424616, 0.33057789539111354, 0.42543023333614394, 0.41858056457452797, 0.39581809080995245, 0.39208640025166763, 0.49397348988154743, 0.47271606116532133, 0.43399560638865836]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (5.101879720392462, [4.936361008660527, 6.188064555675166, 5.0805396028924, 4.796851063742896, 4.725311454359812, 4.976288700802596, 4.803937789786398, 5.146550696563892, 5.112020888405409, 5.252871443035522]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (5.103394332668696, [4.938317646762768, 6.192050362084845, 5.081515651859226, 4.797480849464727, 4.726018224037135, 4.977113520992153, 4.805019996543724, 5.1493603578751905, 5.113055389015424, 5.254011328051765]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.45231111421799025, [0.5544284654959089, 0.5742828149343436, 0.3217239556932799, 0.4113970810574026, 0.3974301176515805, 0.3803351953868076, 0.38779514017257993, 0.5803686769743611, 0.48173178450996623, 0.43361791030367275]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.45467359506010696, [0.5653581713543447, 0.5828944379691388, 0.32202148819589654, 0.41169142360130423, 0.39699333289009414, 0.37997621919038116, 0.38888859075638377, 0.5820526437366309, 0.48298613678013536, 0.43387350612676007]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4520843575265264, [0.4579739976476162, 0.7004881727043444, 0.3308030793926513, 0.4258643291233786, 0.41817455932043274, 0.3956677896066068, 0.3918683458642018, 0.49341775002117694, 0.47230868623864086, 0.4342768653462136]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.45290444056278834, [0.45917647145771384, 0.7067061777810675, 0.3305777230083455, 0.42542865829811727, 0.41857961460243376, 0.3958182933475218, 0.3920882445866211, 0.49397283186064544, 0.47270772638109937, 0.43398866430431743]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.4528370903298075, [0.4590648422159717, 0.7061924018523784, 0.3305945668534468, 0.4254657408083859, 0.4185428433345463, 0.3958041039065505, 0.39207251956011363, 0.493925864069875, 0.47268307281687333, 0.43402494787993373]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4529213415523635, [0.4591891116853934, 0.7068234385633784, 0.33057368416029337, 0.4254232484833647, 0.4185846918608659, 0.3958191003941054, 0.3920949136510068, 0.493986146062481, 0.47272407170500813, 0.4339950089577381]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (5.103258007669355, [4.938136577110192, 6.191715069719775, 5.081423477660792, 4.797421323825832, 4.725951716806115, 4.9770357642980745, 4.80492376445323, 5.1491091389375825, 5.112958201622176, 5.25390504225978]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (5.103409703907362, [4.938332368810798, 6.192113954206466, 5.081521574901994, 4.797484763838106, 4.726022356283176, 4.977118416576809, 4.805032029922476, 5.149390485363234, 5.113061853486448, 5.2540192356841136]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.45461823300159665, [0.5649293465727907, 0.5827271568005814, 0.32197232252125874, 0.41166137117635254, 0.39698936781560346, 0.379977881887716, 0.38880255918165896, 0.582349947244732, 0.48294374015835506, 0.4338286366569177]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.45484149832513776, [0.5660078692673273, 0.5836194860545004, 0.32200769449266553, 0.41168125965204366, 0.396963884474463, 0.3799476441933558, 0.3889201683021069, 0.5823246070948472, 0.4830702296846592, 0.43387214003540797]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4528447813025903, [0.45907515956689965, 0.7062756305377135, 0.330594350638671, 0.4254642242205342, 0.41854189542419673, 0.3958043242505869, 0.3920743742718475, 0.4939250652513953, 0.4726747528154573, 0.4340180360486018]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.45292904300792475, [0.45919951086986055, 0.7069066456471966, 0.3305735116602598, 0.4254216699652285, 0.41858373685110584, 0.395819299257713, 0.3920967559611965, 0.4939855041944819, 0.472715731849399, 0.43398806382280614]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.4529153576794086, [0.4591789426450431, 0.7067803378444288, 0.3305753190363969, 0.4254268026967008, 0.41858092177185596, 0.3958176992865578, 0.39209352551605803, 0.49398133664372207, 0.4727207665661299, 0.43399792478719307]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.45292380325830733, [0.4591914171451572, 0.7068434950840302, 0.33057326296252204, 0.4254225501227207, 0.4185851048200725, 0.39581920149564326, 0.3920957653472538, 0.49398741330355456, 0.4727248730749141, 0.4339949492272042]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (5.103396069215515, [4.938314260376929, 6.192080422080905, 5.081512352910627, 4.79747880693994, 4.726015706907509, 4.977110639596561, 4.805022404894673, 5.149365359616846, 5.1130521337688135, 5.25400860506235]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (5.103411241090963, [4.938333841073069, 6.19212031367304, 5.081522167223264, 4.797485155288457, 4.726022769523454, 4.977118906154024, 4.805033233297897, 5.149393498243929, 5.113062499954821, 5.254020026477673]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.45483636391418647, [0.5659655314856467, 0.5836036750190928, 0.3220029074668125, 0.41167837822273473, 0.39696339661123675, 0.37994782077533257, 0.38891153919777766, 0.5823566219743832, 0.48306598274226753, 0.4338677856465802]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.4548583655064272, [0.5660730502092438, 0.5836923941802488, 0.3220063274752269, 0.411680256712108, 0.3969609451351107, 0.3799447870963763, 0.3889233471898203, 0.5823518569060384, 0.48307867667106746, 0.43387201348903165]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4529230581892249, [0.45918933539437984, 0.7068635488751699, 0.3305751400862096, 0.4254252300471217, 0.4185799673886129, 0.39581789879013035, 0.39209536818653673, 0.4939806806906206, 0.47271242981937045, 0.4339909826140972]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.452931504743162, [0.45920181610482813, 0.7069267033170209, 0.33057309045063227, 0.42542097125619655, 0.4185841493062974, 0.3958193999915783, 0.3920976074547386, 0.49398677305122835, 0.4727165327120528, 0.4339880037870466]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.452923204700921, [0.45919039995643984, 0.7068391846358852, 0.330573426035432, 0.42542290558554574, 0.4185847277972947, 0.39581906133313854, 0.3920956265437601, 0.4939869319007685, 0.472724542574124, 0.43399524064682227]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4529240494438123, [0.459191647704081, 0.7068455008449254, 0.3305732208419979, 0.4254224802879045, 0.41858514611829983, 0.3958192116072285, 0.3920958505204427, 0.49398754004390205, 0.47272495321506497, 0.4339949432542763]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (5.10340987760163, [4.938332030255419, 6.192116960408474, 5.081521244961348, 4.7974845595386, 4.7260221046455655, 4.977118128450813, 4.805032270849745, 5.149390985595955, 5.113061527910723, 5.254018963399659]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (5.10341139480992, [4.9383339882998785, 6.192120949622234, 5.081522226455575, 4.797485194433627, 4.7260228108476285, 4.977118955111933, 4.805033353635813, 5.149393799533322, 5.113062564601863, 5.254020105557332]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.45485785461928, [0.5660688196933487, 0.5836908040659374, 0.3220058491608283, 0.4116799723310971, 0.3969608953495262, 0.3799448048759499, 0.3889224843201726, 0.5823550843958721, 0.4830782518530323, 0.4338715801470355]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.45486005299581234, [0.5660795704198319, 0.5836996890424562, 0.32200619089781257, 0.411680156553329, 0.3969606512567441, 0.37994450139092084, 0.38892366529087014, 0.582354582422381, 0.4830795217480214, 0.43387200093575523]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.45293090607229025, [0.4592007980658097, 0.7069223933707154, 0.33057325276269695, 0.4254213273140202, 0.4185837724505849, 0.39581925990469974, 0.39209746867462886, 0.4939862901951978, 0.4727162025407421, 0.4339882954438069]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.4529317509315997, [0.4592020466412683, 0.7069287091928778, 0.33057304832892087, 0.42542090138653726, 0.418584190554121, 0.3958194100663943, 0.3920976926076552, 0.49398689995314765, 0.4727166128014784, 0.4339879977835959])}\n"
     ]
    }
   ],
   "source": [
    "lambd =[1.0,0,0.1,0.01,0.001,0.0001] \n",
    "epoch=[1000,10000]\n",
    "learning_rate =[0.1, 0.01, 0.001, 0.001]\n",
    "regularizer = ['l1', 'l2']\n",
    "cv=10\n",
    "X_train_2=polynomialFeatures(X_train_poly,3)\n",
    "#Data Standardization\n",
    "r,c=np.shape(X_train_2)\n",
    "for col in range(0,c):\n",
    "    m=np.mean(X_train_2[:,col])\n",
    "    s=np.std(X_train_2[:,col])\n",
    "    if s>0:\n",
    "        X_train_2[:,col]=(X_train_2[:,col]-m)/s\n",
    "results = {}\n",
    "\n",
    "for lam in lambd:\n",
    "    for epo in epoch:\n",
    "        for eta in learning_rate:\n",
    "            for reg in regularizer:\n",
    "                label ='Lambda'+'_'+str(lam) + '_'+'Learning Rate'+'_'+ str(eta) +'_'+'Regularizer'+'_'+str(reg)+'_'+'Epochs'+'_'+ str(epo)\n",
    "                model=Linear_Regression(learning_rate = eta, epochs = epo, regularizer = reg, lambd = lam)\n",
    "                results[label] = sFold(model,X_train_2,y_train,scoring='MSE',cv=cv)\n",
    "            \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.44586567758193646\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4526522015953243\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 5.088346424122396\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 5.10324067999511\n",
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.4385317927121424\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.45306878735629236\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4458794343837405\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.45265990016462937\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.4529240767979246\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4529240767979246\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 5.103411411889807\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 5.103411411889807\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.45486024050329227\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.45486024050329227\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4529317782860377\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.4529317782860377\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.4520767626379141\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4528967393970776\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 5.101879720392462\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 5.103394332668696\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.45231111421799025\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.45467359506010696\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4520843575265264\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.45290444056278834\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.4528370903298075\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4529213415523635\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 5.103258007669355\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 5.103409703907362\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.45461823300159665\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.45484149832513776\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4528447813025903\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.45292904300792475\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.4529153576794086\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.45292380325830733\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 5.103396069215515\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 5.103411241090963\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.45483636391418647\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.4548583655064272\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4529230581892249\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.452931504743162\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.452923204700921\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4529240494438123\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 5.10340987760163\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 5.10341139480992\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.45485785461928\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.45486005299581234\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.45293090607229025\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.4529317509315997\n",
      "0.4385317927121424\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_10000\n"
     ]
    }
   ],
   "source": [
    "min_MSE =100000000\n",
    "min_config = ''\n",
    "for key,pair in results.items():\n",
    "    print(key + \": MSE Error \" + str(pair[0]))\n",
    "    if pair[0] <min_MSE:\n",
    "        min_MSE=pair[0]\n",
    "        min_config=key\n",
    "print(min_MSE)\n",
    "print(min_config)\n",
    "#Best Model \n",
    "#Regularizer: L1\n",
    "#Learning Rate: 0.01\n",
    "#Lambda: 1\n",
    "#Epochs: 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Using the plot_polynomial_model_complexity function plot the rmse values for the training and validation folds for polynomial degree 1, 2, 3, 4 and 5. Use the training data as input for this function. You need to choose the hyperparameter values judiciously to work on the higher-degree polynomial models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression(learning_rate = 0.01, epochs = 10000, regularizer = 'l2', lambd = 1)\n",
    "plot_polynomial_model_complexity(model,X_train_poly,y_train,5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implement the Stochastic Gradient Descent Linear Regression algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear Regression that takes optional hyperparameters. Contains methods fit using Stochastic Gradient Descent to\n",
    "estimate the weight parameters of the model.\n",
    "Methods:\n",
    "    fit - Given X and y, using Stochastic gradient descent to find theta\n",
    "    predict - Given X, using theta to provide prediction\n",
    "'''\n",
    "class Stochastic_Gradient():\n",
    "    def __init__(self, learning_rate=0.01, epochs=100, tol=None, regularizer=None, lambd=0.0, t0=5000, **kwargs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.tol = tol\n",
    "        self.regularizer = regularizer\n",
    "        self.lambd = lambd\n",
    "        self.theta = None\n",
    "        self.t0=t0 # Hyperparameter for the Learning Schedule\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Add x_0 = 1 for all for intercept and concat data\n",
    "        x_0 = np.ones((X.shape[0],1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        \n",
    "        #Length of the Data\n",
    "        n=len(X)\n",
    "    \n",
    "        # Init prev_cost with 0\n",
    "        init_loss = 0\n",
    "        \n",
    "        # Initialize all weights to 0\n",
    "        self.theta = np.zeros((X.shape[1],))\n",
    "        \n",
    "        # Learning Schedule\n",
    "        def learning_schedule(t):\n",
    "            t1=self.t0/self.learning_rate # Hyperparameter for the Learning Schedule\n",
    "            eta=self.t0/(t+t1)\n",
    "            return eta\n",
    "        \n",
    "        # Run for every epoch\n",
    "        for epoch in range(self.epochs):\n",
    "    \n",
    "        # Run for every data row\n",
    "            for i in range(n):\n",
    "                \n",
    "                # Update Learning Rate\n",
    "                learn_rate=learning_schedule(epoch*m+i)\n",
    "                \n",
    "                #Initialize Random Index\n",
    "                random_index = np.random.randint(n)\n",
    "                yi= y[random_index:random_index+1]\n",
    "                xi=X[random_index:random_index+1]\n",
    "    \n",
    "                # Loss Calculation\n",
    "                \n",
    "                new_loss=0.5*MSE_error(xi.dot(self.theta),yi)\n",
    "               \n",
    "                if self.regularizer == \"l2\":\n",
    "                    regularized_term = 0.5*self.lambd*np.dot(self.theta[1:],self.theta[1:]) # Exclude the bias term\n",
    "                    new_loss = new_loss + regularized_term\n",
    "                 \n",
    "                elif self.regularizer == 'l1':\n",
    "                    regularized_term = 0.5*self.lambd*np.sum(abs(self.theta[1:])) # Exclude the bias term\n",
    "                    new_loss=new_loss + regularized_term\n",
    "                    \n",
    "                    \n",
    "                # Break if absolute cost of previous cost and current cost is smaller than self.tol\n",
    "                if self.tol is not None:\n",
    "                    if abs(init_loss - new_loss) > self.tol:\n",
    "                        init_loss = new_loss\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "                # Gradient computation\n",
    "                grad = xi.T.dot(xi.dot(self.theta)-yi)\n",
    "                \n",
    "                # Gradient regularization\n",
    "                if self.regularizer == \"l2\":\n",
    "                    regularized_term = self.lambd*self.theta # Exclude the bias term\n",
    "                    regularized_term[0] = 0\n",
    "                    grad = grad + regularized_term\n",
    "                    \n",
    "                elif self.regularizer == 'l1':\n",
    "                    regularized_term = self.lambd * np.sign(self.theta)# Exclude the bias term\n",
    "                    regularized_term[0] = 0\n",
    "                    grad = grad + regularized_term\n",
    "            \n",
    "            # New theta\n",
    "            self.theta = self.theta - ((learn_rate)*grad)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        m=len(X)\n",
    "        x_0 = np.ones((m,1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        pred = np.dot(X,self.theta)\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b5f4039a702d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'Lambda'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Learning Rate'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Regularizer'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStochastic_Gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MSE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sFold' is not defined"
     ]
    }
   ],
   "source": [
    "#Cross_Validation\n",
    "lambd =[1.0,0,0.1,0.01,0.001,0.0001] \n",
    "epoch=[1000,10000]\n",
    "learning_rate =[0.1, 0.01, 0.001, 0.001]\n",
    "regularizer = ['l1', 'l2']\n",
    "cv=10\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lam in lambd:\n",
    "    for epo in epoch:\n",
    "        for eta in learning_rate:\n",
    "            for reg in regularizer:\n",
    "                label ='Lambda'+'_'+str(lam) + '_'+'Learning Rate'+'_'+ str(eta) +'_'+'Regularizer'+'_'+str(reg)+'_'+'Epochs'+'_'+ str(epo)\n",
    "                model=Stochastic_Gradient(learning_rate = eta, epochs = epo, regularizer = reg, lambd = lam,t0=5000)\n",
    "                results[label] = sFold(model,X_train,y_train,scoring='MSE',cv=cv)\n",
    "            \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Best Model\n",
    "min_MSE =100000000\n",
    "min_config = ''\n",
    "for key,pair in results.items():\n",
    "    print(key + \": MSE Error \" + str(pair[0]))\n",
    "    if pair[0] <min_MSE:\n",
    "        min_MSE=pair[0]\n",
    "        min_config=key\n",
    "print(min_MSE)\n",
    "print(min_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE: 0.468\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Stochastic_Gradient(learning_rate = 0.01, epochs = 1000, regularizer = 'l1', lambd =0.1,t0=5000)\n",
    "model.fit(X_train, y_train)\n",
    "pred=model.predict(X_test)\n",
    "\n",
    "#MSE\n",
    "mse=MSE_error(y_test,pred)\n",
    "print(\"\\nMSE: %0.3f\" %mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "Generating learning curves for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/winequality-red.csv', sep=';')\n",
    "X = np.array(df.drop(columns=['quality'],axis=1))\n",
    "y = np.array(df['quality'])\n",
    "X_train, X_test, y_train, y_test = partition(X, y, 0.8)\n",
    "# Data Standardization\n",
    "r,c=np.shape(X_train)\n",
    "for col in range(0,c):\n",
    "    m=np.mean(X_train[:,col])\n",
    "    s=np.std(X_train[:,col])\n",
    "    X_train[:,col]=(X_train[:,col]-m)/s\n",
    "    # standardize test Data on the mean and sd of Train Data  \n",
    "    X_test[:,col]=(X_test[:,col]-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = Linear_Regression(learning_rate = 0.001, epochs = 10000, regularizer = 'l1', lambd = 1,learning_curve = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 960x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 9), dpi=80)\n",
    "plt.plot(m1.mse, label = \"Training MSE\")\n",
    "plt.plot(m1.mse_test, label = \"Testing MSE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Degree 1 Polynomial\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/winequality-red.csv', sep=';')\n",
    "X = np.array(df.drop(columns=['quality'],axis=1))\n",
    "y = np.array(df['quality'])\n",
    "X_train, X_test, y_train, y_test = partition(X, y, 0.8)\n",
    "\n",
    "# Change to degree 3 polynomial features\n",
    "X_train = polynomialFeatures(X_train,3)\n",
    "X_test = polynomialFeatures(X_test,3)\n",
    "\n",
    "# Data Standardization\n",
    "r,c=np.shape(X_train)\n",
    "for col in range(0,c):\n",
    "    m=np.mean(X_train[:,col])\n",
    "    s=np.std(X_train[:,col])\n",
    "    X_train[:,col]=(X_train[:,col]-m)/s\n",
    "    # standardize test Data on the mean and sd of Train Data  \n",
    "    X_test[:,col]=(X_test[:,col]-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = Linear_Regression(learning_rate = 0.01, epochs = 10000, regularizer = 'l1', lambd = 1,learning_curve = (X_test,y_test))\n",
    "m1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 960x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 9), dpi=80)\n",
    "plt.plot(m1.mse, label = \"Training MSE\")\n",
    "plt.plot(m1.mse_test, label = \"Testing MSE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Degree 3 Polynomial\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
