{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second programming assignment for CSCE478/878 Introduction to Machine Learning on Linear Regression. This notebook is divided into 3 sections, namely\n",
    "1. **Part A (Model Code)**\n",
    "1. **Part B (Data Processing)**\n",
    "1. **Part C (Model Evaluation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the following function that generates the polynomial and interaction features for a given degree of the polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes the feature matrix X and the polynomial degree and returns\n",
    "X with features of polynomial degrees and pairwise interaction\n",
    "terms\n",
    "\n",
    "Input:\n",
    "    X - 2D np.ndarray feature matrix\n",
    "    degree - integer > 0\n",
    "    \n",
    "Output:\n",
    "    X - 2D np.ndarray polynomial feature matrix \n",
    "\"\"\"\n",
    "# Doesn't Include the Bias Term, as it will be added in the regression functions\n",
    "def polynomialFeatures(X, degree):\n",
    "    \n",
    "    #X_original=X.copy()\n",
    "    # Create polynomials of degree n\n",
    "    #if degree > 1:\n",
    "        #for d in range(2, degree+1):\n",
    "            # Create combinations of interaction terms\n",
    "            #interactions =  lambda x: np.multiply.reduce(np.array(list(itertools.combinations_with_replacement(x.T,d))),1).T\n",
    "            #X = np.concatenate((X,interactions(X_original)), axis=1)\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X=poly.fit_transform(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the following function to calculate and return the mean squared error (mse) of two vectors.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates the mean squared error (mse) between the true and predicted labels\n",
    "Input: \n",
    "    true: array_like type vector of true labels\n",
    "    pred: array_like type vector of predicted labels\n",
    "Output:\n",
    "    mean squared error: accuracy expressed in decimal\n",
    "\"\"\"\n",
    "def MSE_error(true,pred):\n",
    "    diff = true - pred\n",
    "    error = np.dot(diff,diff)/true.shape[0]\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Implement the following function to plot the training and validation root mean square error (rmse) values of the data matrix X for various polynomial degree starting from 1 up to the value set by the argument “maxPolynomialDegree” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polynomial_model_complexity(estimator,X,y,cv,max_degree):\n",
    "    \n",
    "    range_degree=np.array([d for d in range(1,max_degree+1)]) \n",
    "    X_original=X.copy()\n",
    "    avg_score_train=[]\n",
    "    avg_score_val=[]\n",
    "    #Running for every degree\n",
    "    for d in range(1,max_degree+1):\n",
    "        \n",
    "        X2=polynomialFeatures(X_original,d)\n",
    "        size = len(X2)\n",
    "        size_fold = int(size/cv)\n",
    "        range_index = [j for j in range(0,len(X2))]\n",
    "        score_val=[]\n",
    "        score_train=[]\n",
    "        #Data Standardization\n",
    "        r,c=np.shape(X2)\n",
    "        for col in range(0,c):\n",
    "            m=np.mean(X2[:,col])\n",
    "            s=np.std(X2[:,col])\n",
    "            if s>0:\n",
    "                X2[:,col]=(X2[:,col]-m)/s\n",
    "    #Partitioning of the data    \n",
    "        for i in range(cv):\n",
    "            init=0+i*size_fold\n",
    "            fin=(i+1)*size_fold\n",
    "            partition_range_index = [j for j in range(init,fin)]\n",
    "            \n",
    "            #Feature and label data of the Fold\n",
    "            X_partition = X2[partition_range_index]\n",
    "            y_partition = y[partition_range_index]\n",
    "            \n",
    "            #Feature and label data of  1-Fold        \n",
    "            remainder_index = list(set(range_index).difference(set(partition_range_index)))\n",
    "            X_remainder=X2[remainder_index]\n",
    "            y_remainder=y[remainder_index]\n",
    "            \n",
    "            #Fit the model to the 1-Fold data\n",
    "            estimator.fit(X_remainder, y_remainder)\n",
    "            \n",
    "            #Test the model on the 1-Fold data\n",
    "            pred=estimator.predict(X_remainder)\n",
    "            rmse=math.sqrt(MSE_error(y_remainder,pred))\n",
    "            score_train.append(rmse) \n",
    "            \n",
    "            #Test the model on the fold data\n",
    "            pred=estimator.predict(X_partition)\n",
    "            rmse=math.sqrt(MSE_error(y_partition,pred))\n",
    "            score_val.append(rmse) \n",
    "        avg_score_train.append(np.mean(score_train))\n",
    "        avg_score_val.append(np.mean(score_val))\n",
    "    \n",
    "           \n",
    "    #generate the plot\n",
    "    plt.figure()\n",
    "    plt.plot(range_degree,avg_score_train)\n",
    "    plt.plot(range_degree,avg_score_val)\n",
    "    plt.xlabel(\"Polynomial Degree\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title('Polynomial Model Complexity')\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear Regression that takes optional hyperparameters. Contains methods fit using Batch Gradient Descent to\n",
    "estimate the weight parameters of the model.\n",
    "Methods:\n",
    "    fit - Given X and y, using batch gradient descent to find w\n",
    "    predict - Given X, using w to provide prediction\n",
    "'''\n",
    "class Linear_Regression():\n",
    "    def __init__(self, learning_rate=0.01, epochs=100, tol=None, regularizer=None, lambd=0.0, **kwargs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.tol = tol\n",
    "        self.regularizer = regularizer\n",
    "        self.lambd = lambd\n",
    "        self.w = None\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Add x_0 = 1 for all for intercept and concat data\n",
    "        x_0 = np.ones((X.shape[0],1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        \n",
    "        # Init prev_cost with 0\n",
    "        prev_cost = 0\n",
    "        \n",
    "        # Number of training samples\n",
    "        m = len(X)\n",
    "        # Initialize all weights to 0\n",
    "        self.w = np.zeros((X.shape[1],))\n",
    "        \n",
    "        # Run batch gradient descent up to self.epoch times\n",
    "        for i in range(self.epochs):\n",
    "            # Calculate mse with current weights\n",
    "            new_cost=0.5*MSE_error(X.dot(self.w),y)\n",
    "            \n",
    "            if self.regularizer == \"l2\":\n",
    "                regularized_term = 0.5*self.lambd*np.dot(self.w[1:],self.w[1:])/m # Exclude the bias term\n",
    "                new_cost=new_cost + regularized_term\n",
    "                \n",
    "            elif self.regularizer == 'l1':\n",
    "                regularized_term = 0.5*self.lambd*np.sum(abs(self.w[1:]))/m  # Exclude the bias term\n",
    "                new_cost=new_cost + regularized_term\n",
    "            \n",
    "            # Break if absolute cost of previous cost and current cost is smaller than self.tol\n",
    "            if self.tol is not None:\n",
    "                if abs(prev_cost - new_cost) > self.tol:\n",
    "                    prev_cost = new_cost\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Calculate gradient\n",
    "            grad = (X.T.dot(X.dot(self.w)-y))\n",
    "            \n",
    "            # Apply Regularization term to gradient\n",
    "            if self.regularizer == \"l2\":\n",
    "                regularized_term = self.lambd*self.w\n",
    "                regularized_term[0] = 0 # Exclude the bias term\n",
    "                grad = grad + regularized_term\n",
    "                \n",
    "            elif self.regularizer == 'l1':\n",
    "                regularized_term = self.lambd * np.sign(self.w)\n",
    "                regularized_term[0] = 0 # Exclude the bias term\n",
    "                grad = grad + regularized_term\n",
    "            \n",
    "            # Update weights\n",
    "            self.w = self.w - (self.learning_rate/m)*grad\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        x_0 = np.ones((X.shape[0],1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        \n",
    "        pred = X.dot(self.w)\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read in the winequality-red.csv file as a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use the techniques from the recitation to summarize each of the variables in the dataset in terms of mean, standard deviation, and quartiles. Include this in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Shuffle the rows of your data. You can use def = df.sample(frac=1) as an idiomatic way to shuffle the data in Pandas without losing column names. Create a test dataset by randomly sampling 20% of the data. Remaining data should be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(X, y, t):\n",
    "    # Determine sizes of sample, training and test set\n",
    "    n = len(y)\n",
    "    size_train = int(t * n)\n",
    "    size_test = 1 - size_train\n",
    "    \n",
    "    # Generate list of all index\n",
    "    range_index = [x for x in range(0,n)]\n",
    "    # Generate list of random index with the size of training set\n",
    "    train_index = random.sample(range(0, n), size_train)\n",
    "    # Obtain the set difference between all the training for test \n",
    "    test_index = list(set(range_index).difference(set(train_index)))\n",
    "    \n",
    "    # Subsetting train and test\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(columns=['quality'],axis=1))\n",
    "y = np.array(df['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = partition(X, y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Standardization\n",
    "r,c=np.shape(X_train)\n",
    "for col in range(0,c):\n",
    "    m=np.mean(X_train[:,col])\n",
    "    s=np.std(X_train[:,col])\n",
    "    X_train[:,col]=(X_train[:,col]-m)/s\n",
    "    # standardize test Data on the mean and sd of Train Data  \n",
    "    X_test[:,col]=(X_test[:,col]-m)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model selection via Hyperparameter tuning: Use the kFold function (known as sFold function from previous assignment) to evaluate the performance of your model over each combination of lambd, learning_rate and regularizer hyperparameters from the following sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sFold(estimator,X,y,scoring,cv):\n",
    "    size = len(X)\n",
    "    size_fold = int(size/cv)\n",
    "    range_index = [j for j in range(0,len(X))]\n",
    "    score=[]\n",
    "    #Partitioning of the data\n",
    "    for i in range(cv):\n",
    "        init=0+i*size_fold\n",
    "        fin=(i+1)*size_fold\n",
    "        partition_range_index = [j for j in range(init,fin)]\n",
    "        \n",
    "        #Feature and label data of the Fold\n",
    "        X_partition = X[partition_range_index]\n",
    "        y_partition = y[partition_range_index]\n",
    "        \n",
    "        #Feature and label data of  1-Fold        \n",
    "        remainder_index = list(set(range_index).difference(set(partition_range_index)))\n",
    "        X_remainder=X[remainder_index]\n",
    "        y_remainder=y[remainder_index]\n",
    "        \n",
    "        #Fit the model to the 1-Fold data\n",
    "        estimator.fit(X_remainder, y_remainder) \n",
    "        \n",
    "        #Test the model on the fold data\n",
    "        pred=estimator.predict(X_partition)\n",
    "        if scoring=='MSE':\n",
    "            mse=MSE_error(y_partition,pred)\n",
    "            score.append(mse) \n",
    "            \n",
    "    avg_score = np.mean(score)    \n",
    "    return avg_score,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd =[1.0,0,0.1,0.01,0.001,0.0001] \n",
    "epoch=[1000,10000]\n",
    "learning_rate =[0.1, 0.01, 0.001, 0.001]\n",
    "regularizer = ['l1', 'l2']\n",
    "cv=10\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lam in lambd:\n",
    "    for epo in epoch:\n",
    "        for eta in learning_rate:\n",
    "            for reg in regularizer:\n",
    "                label ='Lambda'+'_'+str(lam) + '_'+'Learning Rate'+'_'+ str(eta) +'_'+'Regularizer'+'_'+str(reg)+'_'+'Epochs'+'_'+ str(epo)\n",
    "                model=Linear_Regression(learning_rate = eta, epochs = epo, regularizer = reg, lambd = lam)\n",
    "                results[label] = sFold(model,X_train,y_train,scoring='MSE',cv=cv)\n",
    "            \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_MSE =100000000\n",
    "min_config = ''\n",
    "for key,pair in results.items():\n",
    "    print(key + \": MSE Error \" + str(pair[0]))\n",
    "    if pair[0] <min_MSE:\n",
    "        min_MSE=pair[0]\n",
    "        min_config=key\n",
    "print(min_MSE)\n",
    "print(min_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Evaluate your model on the test data and report the mean squared error.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE: 0.444\n"
     ]
    }
   ],
   "source": [
    "#Best Model\n",
    "#Regularizer: L2\n",
    "#Learning Rate: 0.001\n",
    "#Lambda: 1\n",
    "#Epochs: 10000\n",
    "\n",
    "model = Linear_Regression(learning_rate = 0.001, epochs = 10000, regularizer = 'l2', lambd = 1)\n",
    "model.fit(X_train, y_train)\n",
    "pred=model.predict(X_test)\n",
    "\n",
    "#MSE\n",
    "mse=MSE_error(y_test,pred)\n",
    "print(\"\\nMSE: %0.3f\" %mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Determine the best model hyperparameter values for the training data matrix with polynomial degree 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-def89ad25e6f>:42: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  regularized_term = 0.5*self.lambd*np.sum(abs(self.w[1:]))/m  # Exclude the bias term\n",
      "<ipython-input-44-def89ad25e6f>:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  regularized_term = np.array(0.5*self.lambd*np.dot(self.w[1:],self.w[1:])/m) # Exclude the bias term\n",
      "<ipython-input-44-def89ad25e6f>:57: RuntimeWarning: invalid value encountered in multiply\n",
      "  regularized_term = self.lambd*self.w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.40843922677862593, [0.3558901679030535, 0.3841146614811492, 0.4489019194989011, 0.31347313116947373, 0.5426043955051693, 0.4314073926025395, 0.42077850063603156, 0.4076645078686741, 0.36146795051345715, 0.4180896406078099]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4107186475183342, [0.3557717077229644, 0.3865429144026475, 0.4476250258553494, 0.3137050840853435, 0.5512466937149707, 0.43292030036418244, 0.418119973475099, 0.4178993881270017, 0.3666989310646684, 0.41665645637111487]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (1.0451403350423316, [1.0557435266637147, 1.030081827187974, 0.9486713002710825, 0.9880920961045374, 1.1863889086389852, 1.0368695860268038, 1.0489790815649558, 1.1331289641536118, 0.9863743151548104, 1.0370737446568403]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (1.0502697677708228, [1.059580986995248, 1.035562584343673, 0.9512531612597792, 0.9908341286759248, 1.2016642440440444, 1.039500321994174, 1.051611215506565, 1.139627762518238, 0.9905943477467372, 1.0424689246238434]), 'Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.4269424693082679, [0.3555015414506035, 0.3969460649411315, 0.4514260324647296, 0.31897078044492144, 0.5539520617800777, 0.4334206022535087, 0.40973473897853696, 0.542151309984523, 0.37880115700970246, 0.428520403774945]), 'Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.4393846104179566, [0.37012984637702195, 0.4034951322904106, 0.44300141786597186, 0.31541555293311996, 0.6246504217869818, 0.432232697383744, 0.404894827577117, 0.5984155667744976, 0.39214357293485724, 0.40946706825584406]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.40843933346152583, [0.35588834735370134, 0.3841063606430941, 0.4488974758938136, 0.31347866288728854, 0.5426030834686455, 0.4314053572645147, 0.42079544426439924, 0.40765593651356696, 0.3614719504989476, 0.4180907158272867]), 'Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.41071612259906676, [0.3557767800248636, 0.3865401670135226, 0.44762202009734275, 0.3137069415775643, 0.5512400944374696, 0.43291470615533695, 0.4181202206934869, 0.4178926979275584, 0.3666894982388269, 0.41665809982469604]), 'Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.4107697084826699, [0.355645018700156, 0.3865766273674553, 0.4478657020942928, 0.31364572043468036, 0.5513870353359098, 0.43303245913834143, 0.4181180391684269, 0.4179834664026172, 0.3667874849668217, 0.416655531217998]), 'Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4107697084826699, [0.355645018700156, 0.3865766273674553, 0.4478657020942928, 0.31364572043468036, 0.5513870353359098, 0.43303245913834143, 0.4181180391684269, 0.4179834664026172, 0.3667874849668217, 0.416655531217998]), 'Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (1.049130986785856, [1.0583963621556456, 1.0344126826292257, 0.9502148710231414, 0.9896463284797402, 1.200628151238481, 1.0383568415653432, 1.0504524972837, 1.1384128177032118, 0.9894390292683648, 1.0413502865117086]), 'Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (1.049130986785856, [1.0583963621556456, 1.0344126826292257, 0.9502148710231414, 0.9896463284797402, 1.200628151238481, 1.0383568415653432, 1.0504524972837, 1.1384128177032118, 0.9894390292683648, 1.0413502865117086]), 'Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.44181595994588363, [0.3708308598987246, 0.40476506924815425, 0.44321080138573016, 0.31562025826216294, 0.6333940717134589, 0.4324126518773099, 0.40508402237369645, 0.6100231761423168, 0.39360701912535107, 0.4092116694319314]), 'Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.44181595994588363, [0.3708308598987246, 0.40476506924815425, 0.44321080138573016, 0.31562025826216294, 0.6333940717134589, 0.4324126518773099, 0.40508402237369645, 0.6100231761423168, 0.39360701912535107, 0.4092116694319314]), 'Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4107671432798353, [0.3556501065319598, 0.38657385534227007, 0.4478626552196252, 0.31364756356393986, 0.5513803153407622, 0.43302679376827174, 0.4181182468686104, 0.4179767440600069, 0.3667779873267586, 0.4166571647761486]), 'Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.4107671432798353, [0.3556501065319598, 0.38657385534227007, 0.4478626552196252, 0.31364756356393986, 0.5513803153407622, 0.43302679376827174, 0.4181182468686104, 0.4179767440600069, 0.3667779873267586, 0.4166571647761486]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.4104753208317053, [0.35563312437771516, 0.38628290026851364, 0.4479636770675196, 0.31358767624360967, 0.5500718409621032, 0.432827856839709, 0.4183816446638965, 0.41676689887710444, 0.3662145373624655, 0.4170230516544164]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.41076444211112195, [0.3556575833999471, 0.38657310820047, 0.44784139411223955, 0.31365152848430233, 0.5513728348609075, 0.43302105094404436, 0.4181180775838122, 0.41797488544821837, 0.36677846840243317, 0.4166554896748439]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (1.048714113029447, [1.0580945533722754, 1.0339633071301757, 0.9500512744792471, 0.9894821192883287, 1.1991459952759154, 1.038202190068959, 1.0502857853845475, 1.1378722312891696, 0.989119682437497, 1.040923991568356]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (1.0492448499517002, [1.0585148112544644, 1.034527658596561, 0.9503186812627945, 0.9897650946199719, 1.2007317431236608, 1.0384711742778603, 1.0505683537515715, 1.1385343004253112, 0.9895545465909716, 1.0414621356138345]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.4394720145901112, [0.36775790490621496, 0.40320603670048516, 0.44422736247344957, 0.31574864092606475, 0.6207595944618062, 0.4325298343720958, 0.40505405659805727, 0.6032605029199495, 0.3916159419708561, 0.4105602705721325]), 'Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.4415623246669478, [0.3707580616639453, 0.4046331252848964, 0.44318791067069446, 0.315598527237947, 0.6324765504799551, 0.43239334953342523, 0.4050615489153001, 0.6088219940150731, 0.3934557336414573, 0.4092364452267838]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.41047274714450144, [0.3556379786409645, 0.3862803351139599, 0.4479609209769153, 0.3135895961012451, 0.5500647904200138, 0.4328221923746351, 0.4183814622376769, 0.4167597408829902, 0.36620546212390026, 0.41702499257271325]), 'Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.4107618809480198, [0.3556626696774179, 0.386570338635995, 0.44783835136509526, 0.31365337305948615, 0.5513661269851975, 0.4330153927179454, 0.4181182892631658, 0.41796816629577754, 0.36676897724517377, 0.41665712423494317]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.41073944194388823, [0.3556443765752556, 0.38654630523361905, 0.4478793819245138, 0.31363947016508853, 0.5512462909577068, 0.4330117127909246, 0.4181450512047715, 0.4178591429102201, 0.36672911628392585, 0.4166935713928563]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.41076918023056697, [0.355646274117294, 0.38657627396041655, 0.4478632688783915, 0.3136462999448939, 0.5513856136135832, 0.433031316382304, 0.41811804144771525, 0.41798260656277003, 0.36678658168658185, 0.41665552571172015]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (1.0490891994945082, [1.0583660317028465, 1.0343676637481347, 0.9501983948370618, 0.9896298967743911, 1.2004795833801598, 1.038341365171994, 1.0504356258194003, 1.1383586806693198, 0.9894068125650777, 1.0413079402766974]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (1.04914237295289, [1.0584082069314598, 1.0344241800836904, 0.950225251859028, 0.9896582049547431, 1.2006385102528148, 1.0383682746830694, 1.0504640827767178, 1.1384249658576127, 0.9894505808551531, 1.0413614712746087]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.44158617334708944, [0.3705852662253795, 0.4045835555164338, 0.44332103308756193, 0.31562283881675174, 0.6321301617902474, 0.4324268779880541, 0.4050762326941187, 0.6093707497537353, 0.3933963927859478, 0.4093486248126646]), 'Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.4417904875767185, [0.3708235521286687, 0.40475182358099066, 0.44320849191995004, 0.31561807192174235, 0.6333018711374904, 0.4324107078655118, 0.4050817377907742, 0.6099026413263453, 0.39359183947732573, 0.4092141386183857]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.41073687316174345, [0.35564944587255354, 0.3865435922866694, 0.4478763590332954, 0.313641307205169, 0.5512395382027024, 0.43300606985985934, 0.4181452338037127, 0.41785229390514617, 0.36671966088253344, 0.4166952305657933]), 'Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.41076661543181914, [0.35565136179365153, 0.38657350218127257, 0.44786022241663315, 0.313648143218843, 0.5513788948308572, 0.4330256517269101, 0.4181182495460896, 0.4179758845389319, 0.3667770846948117, 0.41665715937019]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.41076667491928137, [0.3556449600933551, 0.3865735813694749, 0.4478670743652101, 0.3136450984153459, 0.5513729256088987, 0.4330303758286633, 0.41812074288828216, 0.4179709888776265, 0.3667816468092348, 0.41665935493672196]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.41076965564129786, [0.35564514423133087, 0.38657659201183625, 0.4478654587485116, 0.31364577837274255, 0.5513868931469168, 0.43303234484335823, 0.4181180393807211, 0.4179833804011753, 0.36678739462254756, 0.41665553065383903]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (1.0491268076764275, [1.0583933277773543, 1.0344081801507958, 0.9502132219618741, 0.9896446852577563, 1.2006132908808616, 1.0383552938872616, 1.0504508088529785, 1.1384074031753897, 0.9894358063143248, 1.0413460585056795]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (1.0491321254010635, [1.0583975466318891, 1.0344138323732526, 0.950215909104852, 0.9896475161258392, 1.200629187138172, 1.0383579848755784, 1.0504536558314554, 1.1384140325174688, 0.9894401844255938, 1.0413514049865353]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.44179307629898756, [0.37080656073515983, 0.40474681921818545, 0.44322221969444897, 0.31562037956784716, 0.6332692018266896, 0.4324140244125742, 0.405082985442785, 0.6099583467453811, 0.3935856344269697, 0.40922459091983476]), 'Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.4418134116166019, [0.3708301288412454, 0.404763744166908, 0.4432105702343105, 0.3156200394950852, 0.6333848471539202, 0.43241245733762196, 0.40508379354128016, 0.6100111184819734, 0.3936055006478113, 0.4092119162658636]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4107641093658495, [0.3556500458589192, 0.3865708104633307, 0.4478640299290341, 0.3136469419238371, 0.5513662064288235, 0.4330247137050923, 0.4181209481162405, 0.4179642526888905, 0.36677215420248244, 0.41666099034184495]), 'Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.410767090478873, [0.355650232047589, 0.3865738200112546, 0.4478624119151374, 0.3136476215164724, 0.5513801732730165, 0.4330266795447589, 0.41811824712072604, 0.4179766580904383, 0.36677789704731373, 0.41665716422202265]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_1000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_1000': (0.41076940505060683, [0.3556450128098417, 0.3865763226952805, 0.44786583938373453, 0.3136456584041849, 0.5513856239287153, 0.4330322506104631, 0.4181183094923819, 0.4179822179750803, 0.3667869011870573, 0.41665591401932894]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_1000': (0.4107697031983711, [0.3556450312531681, 0.38657662383174407, 0.44786567775947256, 0.3136457262283572, 0.5513870211168427, 0.43303244770864924, 0.41811803918949975, 0.4179834578022988, 0.3667874759322319, 0.41665553116144677]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_1000': (1.0491305688683197, [1.058396058718378, 1.0344122323745504, 0.9502147061133058, 0.9896461641316537, 1.2006266652294089, 1.0383566867778948, 1.050452328427092, 1.1384122762627853, 0.9894387069508545, 1.0413498636972742]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_1000': (1.0491311006473625, [1.058396480603254, 1.0344127976036142, 0.950214974831295, 0.9896464472443389, 1.2006282548284284, 1.0383569558963512, 1.050452613138456, 1.13841293918463, 0.9894391447840807, 1.0413503983591779]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_10000': (nan, [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_10000': (0.44181367132219684, [0.3708284328688437, 0.40476324350063775, 0.4432119439781507, 0.31562026865365206, 0.633381583129198, 0.4324127885537509, 0.40508391424683066, 0.6100166978312689, 0.39360487711489994, 0.40921296334473534]), 'Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_10000': (0.4418157051020278, [0.37083078679017084, 0.4047649367348823, 0.4432107782685385, 0.31562023638412434, 0.6333931492124684, 0.4324126324219555, 0.40508399948671175, 0.6100219703344818, 0.39360686727246763, 0.4092116941144767]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_10000': (0.4107668398216061, [0.35565010051061396, 0.3865735507418863, 0.447862792762896, 0.3136475014667138, 0.5513789042659372, 0.43302658548771683, 0.41811851702982206, 0.4179754944460836, 0.3667774039948475, 0.4166575475095445]), 'Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_10000': (0.4107671379995774, [0.3556501190834171, 0.3865738518090192, 0.447862630888936, 0.3136475693590637, 0.55138030113382, 0.4330267823457265, 0.4181182468936655, 0.41797673546287445, 0.3667779782986514, 0.4166571647206007])}\n"
     ]
    }
   ],
   "source": [
    "lambd =[1.0,0,0.1,0.01,0.001,0.0001] \n",
    "epoch=[1000,10000]\n",
    "learning_rate =[0.1, 0.01, 0.001, 0.001]\n",
    "regularizer = ['l1', 'l2']\n",
    "cv=10\n",
    "X2=polynomialFeatures(X,3)\n",
    "X_train2, X_test2, y_train2, y_test2 = partition(X2, y, 0.8)\n",
    "#Data Standardization\n",
    "r,c=np.shape(X_train2)\n",
    "for col in range(0,c):\n",
    "    m=np.mean(X_train2[:,col])\n",
    "    s=np.std(X_train2[:,col])\n",
    "    if s>0:\n",
    "        X_train2[:,col]=(X_train2[:,col]-m)/s\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lam in lambd:\n",
    "    for epo in epoch:\n",
    "        for eta in learning_rate:\n",
    "            for reg in regularizer:\n",
    "                label ='Lambda'+'_'+str(lam) + '_'+'Learning Rate'+'_'+ str(eta) +'_'+'Regularizer'+'_'+str(reg)+'_'+'Epochs'+'_'+ str(epo)\n",
    "                model=Linear_Regression(learning_rate = eta, epochs = epo, regularizer = reg, lambd = lam)\n",
    "                results[label] = sFold(model,X_train2,y_train2,scoring='MSE',cv=cv)\n",
    "            \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.40843922677862593\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4107186475183342\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 1.0451403350423316\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 1.0502697677708228\n",
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_1.0_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.4269424693082679\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.4393846104179566\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.40843933346152583\n",
      "Lambda_1.0_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.41071612259906676\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.4107697084826699\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4107697084826699\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 1.049130986785856\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 1.049130986785856\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.44181595994588363\n",
      "Lambda_0_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.44181595994588363\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4107671432798353\n",
      "Lambda_0_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.4107671432798353\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.4104753208317053\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.41076444211112195\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 1.048714113029447\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 1.0492448499517002\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0.1_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.4394720145901112\n",
      "Lambda_0.1_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.4415623246669478\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.41047274714450144\n",
      "Lambda_0.1_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.4107618809480198\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.41073944194388823\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.41076918023056697\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 1.0490891994945082\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 1.04914237295289\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0.01_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.44158617334708944\n",
      "Lambda_0.01_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.4417904875767185\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.41073687316174345\n",
      "Lambda_0.01_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.41076661543181914\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.41076667491928137\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.41076965564129786\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 1.0491268076764275\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 1.0491321254010635\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0.001_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.44179307629898756\n",
      "Lambda_0.001_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.4418134116166019\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4107641093658495\n",
      "Lambda_0.001_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.410767090478873\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_1000: MSE Error nan\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_1000: MSE Error nan\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_1000: MSE Error 0.41076940505060683\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_1000: MSE Error 0.4107697031983711\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_1000: MSE Error 1.0491305688683197\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_1000: MSE Error 1.0491311006473625\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l1_Epochs_10000: MSE Error nan\n",
      "Lambda_0.0001_Learning Rate_0.1_Regularizer_l2_Epochs_10000: MSE Error nan\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l1_Epochs_10000: MSE Error 0.44181367132219684\n",
      "Lambda_0.0001_Learning Rate_0.01_Regularizer_l2_Epochs_10000: MSE Error 0.4418157051020278\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l1_Epochs_10000: MSE Error 0.4107668398216061\n",
      "Lambda_0.0001_Learning Rate_0.001_Regularizer_l2_Epochs_10000: MSE Error 0.4107671379995774\n",
      "0.40843922677862593\n",
      "Lambda_1.0_Learning Rate_0.01_Regularizer_l1_Epochs_1000\n"
     ]
    }
   ],
   "source": [
    "min_MSE =100000000\n",
    "min_config = ''\n",
    "for key,pair in results.items():\n",
    "    print(key + \": MSE Error \" + str(pair[0]))\n",
    "    if pair[0] <min_MSE:\n",
    "        min_MSE=pair[0]\n",
    "        min_config=key\n",
    "print(min_MSE)\n",
    "print(min_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Using the plot_polynomial_model_complexity function plot the rmse values for the training and validation folds for polynomial degree 1, 2, 3, 4 and 5. Use the training data as input for this function. You need to choose the hyperparameter values judiciously to work on the higher-degree polynomial models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Linear_Regression(learning_rate = 0.001, epochs = 1000, regularizer = 'l2', lambd = 1)\n",
    "plot_polynomial_model_complexity(model,X_train,y_train,5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implement the Stochastic Gradient Descent Linear Regression algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear Regression that takes optional hyperparameters. Contains methods fit using Stochastic Gradient Descent to\n",
    "estimate the weight parameters of the model.\n",
    "Methods:\n",
    "    fit - Given X and y, using Stochastic gradient descent to find theta\n",
    "    predict - Given X, using theta to provide prediction\n",
    "'''\n",
    "class Stochastic_Gradient():\n",
    "    def __init__(self, learning_rate=0.01, epochs=100, tol=None, regularizer=None, lambd=0.0, t0=5, t1=50, **kwargs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.tol = tol\n",
    "        self.regularizer = regularizer\n",
    "        self.lambd = lambd\n",
    "        self.theta = None\n",
    "        self.t0=t0 # Hyperparameter for the Learning Schedule\n",
    "        self.t1=t1 # Hyperparameter for the Learning Schedule\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Add x_0 = 1 for all for intercept and concat data\n",
    "        x_0 = np.ones((X.shape[0],1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        \n",
    "        #Length of the Data\n",
    "        n=len(X)\n",
    "    \n",
    "        # Init prev_cost with 0\n",
    "        init_loss = 0\n",
    "        \n",
    "        # Initialize all weights to 0\n",
    "        self.theta = np.zeros((X.shape[1],))\n",
    "        \n",
    "        # Learning Schedule\n",
    "        def learning_schedule(t):\n",
    "            #eta=self.t0/(t+self.t1)\n",
    "            eta=self.learning_rate\n",
    "            return eta\n",
    "        \n",
    "        # Run for every epoch\n",
    "        for epoch in range(self.epochs):\n",
    "    \n",
    "        # Run for every data row\n",
    "            for i in range(n):\n",
    "                \n",
    "                # Update Learning Rate\n",
    "                self.learning_rate=learning_schedule(epoch*m+i)\n",
    "                \n",
    "                #Initialize Random Index\n",
    "                random_index = np.random.randint(n)\n",
    "                yi= y[random_index:random_index+1]\n",
    "                xi=X[random_index:random_index+1]\n",
    "    \n",
    "                # Loss Calculation\n",
    "                \n",
    "                new_loss=0.5*MSE_error(xi.dot(self.theta),yi)\n",
    "               \n",
    "                if self.regularizer == \"l2\":\n",
    "                    regularized_term = 0.5*self.lambd*np.dot(self.theta[1:],self.theta[1:]) # Exclude the bias term\n",
    "                    new_loss = new_loss + regularized_term\n",
    "                 \n",
    "                elif self.regularizer == 'l1':\n",
    "                    regularized_term = 0.5*self.lambd*np.sum(abs(self.theta[1:])) # Exclude the bias term\n",
    "                    new_loss=new_loss + regularized_term\n",
    "                    \n",
    "                    \n",
    "                # Break if absolute cost of previous cost and current cost is smaller than self.tol\n",
    "                if self.tol is not None:\n",
    "                    if abs(init_loss - new_loss) > self.tol:\n",
    "                        init_loss = new_loss\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "                # Gradient computation\n",
    "                grad = xi.T.dot(xi.dot(self.theta)-yi)\n",
    "                \n",
    "                # Gradient regularization\n",
    "                if self.regularizer == \"l2\":\n",
    "                    regularized_term = self.lambd*self.theta # Exclude the bias term\n",
    "                    regularized_term[0] = 0\n",
    "                    grad = grad + regularized_term\n",
    "                    \n",
    "                elif self.regularizer == 'l1':\n",
    "                    regularized_term = self.lambd * np.sign(self.theta)# Exclude the bias term\n",
    "                    regularized_term[0] = 0\n",
    "                    grad = grad + regularized_term\n",
    "            \n",
    "            # New theta\n",
    "            self.theta = self.theta - ((self.learning_rate)*grad)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        m=len(X)\n",
    "        x_0 = np.ones((m,1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        pred = np.dot(X,self.theta)\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross_Validation\n",
    "lambd =[1.0,0,0.1,0.01,0.001,0.0001] \n",
    "epoch=[1000,100]\n",
    "learning_rate =[0.1, 0.01, 0.001, 0.001]\n",
    "regularizer = ['l1', 'l2']\n",
    "cv=10\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lam in lambd:\n",
    "    for epo in epoch:\n",
    "        for eta in learning_rate:\n",
    "            for reg in regularizer:\n",
    "                label ='Lambda'+'_'+str(lam) + '_'+'Learning Rate'+'_'+ str(eta) +'_'+'Regularizer'+'_'+str(reg)+'_'+'Epochs'+'_'+ str(epo)\n",
    "                model=Stochastic_Gradient(learning_rate = eta, epochs = epo, regularizer = reg, lambd = lam)\n",
    "                results[label] = sFold(model,X_train,y_train,scoring='MSE',cv=cv)\n",
    "            \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Model\n",
    "min_MSE =100000000\n",
    "min_config = ''\n",
    "for key,pair in results.items():\n",
    "    print(key + \": MSE Error \" + str(pair[0]))\n",
    "    if pair[0] <min_MSE:\n",
    "        min_MSE=pair[0]\n",
    "        min_config=key\n",
    "print(min_MSE)\n",
    "print(min_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE: 0.472\n"
     ]
    }
   ],
   "source": [
    "#Testing on the Test Matrix\n",
    "#Best Model\n",
    "#Regularizer: L2\n",
    "#Learning Rate: 0.001\n",
    "#Lambda: 1\n",
    "#Epochs: 10000\n",
    "\n",
    "model = Stochastic_Gradient(learning_rate = 0.01, epochs = 1000, regularizer = 'l2', lambd = 0.01)\n",
    "model.fit(X_train, y_train)\n",
    "pred=model.predict(X_test)\n",
    "\n",
    "#MSE\n",
    "mse=MSE_error(y_test,pred)\n",
    "print(\"\\nMSE: %0.3f\" %mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
