{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "In this Jupyter Notebook, we code a Multi-Layer Perceptron (MLP) for classification and regression tasks using Stochastic Gradient Descent for training the model. The MLP would be able to perform multiclass as well as binary classification. <br>\n",
    "\n",
    "The model would also allow customization of activation function used, as well as training schedule for optimization purposes. The MLP is designed with a single hidden layer architecture. <br>\n",
    "\n",
    "This notebook is dvided into three sections\n",
    "1. Section 1 - Helper functions and multiclass classification\n",
    "1. Section 2 - Binary classification\n",
    "1. Section 3 - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "from math import exp\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the following function that creates a weight matrix and initializes it with small random real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeights(input_neurons,output_neurons):\n",
    "    W = np.random.randn(output_neurons,input_neurons) * 0.01\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01002579, -0.0149114 , -0.00947348],\n",
       "       [ 0.00378412,  0.00077575, -0.00418174],\n",
       "       [-0.00070011, -0.00901642, -0.00869967],\n",
       "       [ 0.01319274, -0.00018026, -0.03522069]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initializeWeights(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the logistic sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement the ReLU (rectified linear unit) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement the tanh (hyperbolic tangent) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implement a MLP Classifier model class for performing multi-class classification.\n",
    "\n",
    "### The MLP Classifier has a single hidden layer. It should have the following four methods.The model uses the back-propagation algorithm for learning the weights of the features/neurons. Note the that “fit” method should implement the Stochastic Gradient Descent algorithm for optimizing the weight update process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function that determines the number of input layers\n",
    "and the number of layers\n",
    "Input:\n",
    "    X - feature matrix\n",
    "    y - target vector\n",
    "    n_h - number of hidden layers\n",
    "Output:\n",
    "    n_o - tuple containing the values\n",
    "'''\n",
    "def size_of_layers(X,y,n_h):\n",
    "    n_i = X.shape[1]\n",
    "    n_o = len(set(y))\n",
    "    \n",
    "    return n_i, n_h, n_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function to initialize parameters of the MLP\n",
    "This assumes a single hidden layer architecture\n",
    "Bias is included\n",
    "Input:\n",
    "    n_i - size of input layer (# of X_train columns)\n",
    "    n_h - size of hidden layer neurons\n",
    "    n_o - size of output layer neurons (# number of classes)\n",
    "Output:\n",
    "    parameters - Dictionary of Parameters\n",
    "'''\n",
    "def initializeParameters(n_i, n_h, n_o):\n",
    "    # Weights of each layer\n",
    "    W_i = initializeWeights(n_i,n_h)\n",
    "    W_h = initializeWeights(n_h,n_o)\n",
    "    \n",
    "    # Bias of each layer\n",
    "    b_i = np.zeros((n_h,1))\n",
    "    b_h = np.zeros((n_o,1))\n",
    "    \n",
    "    parameters = {\n",
    "        'W_input': W_i,\n",
    "        'b_input': b_i,\n",
    "        'W_hidden': W_h,\n",
    "        'b_hidden': b_h\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = size_of_layers(X,y,4)\n",
    "params = initializeParameters(dim[0],dim[1],dim[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Read the handwritten digits datasetusing the sklearn.datasets.load_digits function for performing multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Partition the data into train and test set. Use the “Partition” function from your previous assignment or from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Hyperparameter tuning based on certain fixed-values hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Report on performance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Implement binary classification module in theMLPClassifier. Then, performbinary classification on the handwritten digits dataset to recognize the digits “5” and “not-5”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Hyperparameter tuning based on certain fixed-values hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Report performance on model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 (Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implement a Multi-Layer Perceptron regressor model (a MLP Regressor class) with a single hidden layer. The model implements the backpropagation algorithm. To optimize the process of updating the weight matrices, it uses the Stochastic Gradient Descent (SGD) algorithm with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Hyperparameter tuning based on certain fixed-values hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Report performance on model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
