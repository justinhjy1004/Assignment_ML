{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the third programming assignment for CSCE478/878 Introduction to Machine Learning on Logistic Regression. This notebook is divided into 2 main sections, and 3 subsections each\n",
    "1. **Part A Logistic Regression - Binary Classification**\n",
    "    1. Model Code\n",
    "    1. Load, Partition and Scale Data\n",
    "    1. Model Evaluation\n",
    "1. **Part B Logistic Regression - Multiclass Classification**\n",
    "    1. Model Code\n",
    "    1. Load, Partition and Scale Data\n",
    "    1. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "from math import exp\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A-I Model Code Logistic Regression - Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the following function that computes the sigmoid score of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the following function to compute the binary cross-entropy loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Implement the following function to compute the l2 regularized binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement the following function to compute the l1 regularized binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Implement a Logistic_Regression_Binary model class. It should have the following three methods. Note the that “fit” method should implement the batch gradient descent algorithm (based on the 1st order derivative of the loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A-II: Load, Partition, and Scale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Read the Iris data using the sklearn.datasets.load_iris function. Create a data matrix X (numpy ndarray) and 1D column vector Y (numpy 1D array) containing the binary labels. Create Y by putting the value 1 if the target value Iris-Virginica, else put 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(map(lambda x: 1 if x==2 else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Shuffle the rows of X. You may use a sklearn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Partition the data into train and test set. Use the partition function from your previous assignment or from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Standardize the train and test set. Use the standardization function from your previous assignment or from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A-III: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Model selection via Hyperparameter tuning: Use the kFold function from previous assignment or from sklearn to evaluate the performance of your model over each combination of parameters from the following sets. You may vary the range of values, if needed, and also for more experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Evaluate your best model on the test data and report the accuracy and confusion matrix. You may use the sklearn.metrics.confusion_matrix function for generating the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Using the best model, report the following in the Jupyter notebook: total number of iterations before the model converged, the final weightvector, and the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B-I: Model Code Logistic Regression-Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Implement the following function to convert the vector of class labels into a matrix containing a one-hot vector for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_labels(Y):\n",
    "    \n",
    "    # Get unique labels in Y and number of observations\n",
    "    unique_labels = np.unique(Y)\n",
    "    n = len(Y)\n",
    "    \n",
    "    # Create one hot matrix\n",
    "    labels = np.array(list(unique_labels)).reshape(len(unique_labels),1)\n",
    "    one_hot_matrix = np.apply_along_axis(lambda x: np.full((n,),x),1,labels)\n",
    "    one_hot_matrix = np.apply_along_axis(lambda x: (x==Y).astype(int),1,one_hot_matrix).T\n",
    "    \n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(['hi', 'no', 'no', 'hi','yes','yes','yes','bye','bye','no','hi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(Y).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = one_hot_labels(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = one_hot_labels(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Implement the following function that computes the softmax score or the normalized exponential of the score of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(score):\n",
    "    score = np.apply_along_axis(np.exp,0,score)\n",
    "    score = np.apply_along_axis(lambda x: x/(sum(x)),1,score)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Implement the following function to compute the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(Y_one_hot, Y_proba):\n",
    "    loss = -np.sum(np.multiply(Y_one_hot,np.log(Y_proba)))/Y_one_hot.shape[0]\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1223701514369318"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(Y_train,softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Implement the following function to compute the l2 regularized cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_l2(Y_one_hot, Y_proba, Theta, lambd):\n",
    "    loss = cross_entropy_loss(Y_one_hot, np.log(Y_proba)) + 0.5*lambd*np.sum(np.square(Theta))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Implement the following function to compute the l1 regularized cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_l1(Y_one_hot, Y_proba, Theta, lambd):\n",
    "    loss = cross_entropy_loss(Y_one_hot, np.log(Y_proba)) + 0.5*lambd*np.sum(np.sign(Theta))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Implement a Logistic_Regression_Multiclass model class that uses the softmax regression technique for performing multi-class classification. The model should have the following three methods. Note the that “fit” method should implement the batch gradient descent algorithm (based on the 1st order derivative of the loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression_Multiclass():\n",
    "    def __init__(self, multi_class='softmax', learning_rate=0.01, epochs=100, tol=0.0001, regularizer=None, lambd=0.0, early_stopping=False, validation_fraction=0.1, plotLearningCurve=True, **kwargs):\n",
    "        self.multi_class = multi_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.tol = tol\n",
    "        self.regularizer = regularizer\n",
    "        self.lambd = lambd\n",
    "        self.early_stopping = early_stopping\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.plotLearningCurve = plotLearningCurve\n",
    "        self.W = None\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        # Add 1 for bias term\n",
    "        x_0 = np.ones((X.shape[0],1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        \n",
    "        # Set number of training data\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        # Initiate Weight Matrix\n",
    "        W = np.zeros((X.shape[1],Y.shape[1]))\n",
    "        \n",
    "        loss = []\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            loss.append(cross_entropy_loss(Y,softmax(X.dot(W))))\n",
    "            \n",
    "            # Calculate gradient \n",
    "            G = X.T.dot(softmax(X.dot(W))-Y)\n",
    "            \n",
    "            # Update weights\n",
    "            W = W - (self.learning_rate/n)*G            \n",
    "            \n",
    "        self.W = W\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def predict(self,X):\n",
    "        # Append bias term\n",
    "        x_0 = np.ones((X.shape[0],1))\n",
    "        X = np.concatenate((x_0,X), axis=1)\n",
    "        \n",
    "        return softmax(X.dot(self.W))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = Logistic_Regression_Multiclass(learning_rate=0.01, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = logit.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7758827456020223"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[0] - loss[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B-II: Load, Partition, and Scale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Read the Iris data using the sklearn.datasets.load_iris function. Create a data matrix X and 1D column vector Y containing the multi-lass labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Shuffle the rows of X. You may use a sklearn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Partition the data into train and test set. Use the partition function from your previous assignment or from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. Standardize the train and test set. Use the standardization function from your previous assignment or from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B-III: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. Model selection via Hyperparameter tuning: Use the kFold function from previous assignment or from sklearn to evaluate the performance of your model over each combination of parameters from the following sets. You may vary the range of values, if needed, and also for more experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. Evaluate your best model on the test data and report the accuracy and confusion matrix. You may use the sklearn.metrics.confusion_matrix function for generating the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. Using the best model, report the following in the Jupyter notebook: total number of iterations before the model converged, the final weightvector, and the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. Implement the one-vs-all (OvA) technique for performing multi-class classification in the Logistic_Regression_Multiclass model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Implement the Stochastic Gradient Descent Logistic Regression algorithm for performing multi-class classification. It should use a suitable learning schedule function for adapting the learning rate. Using cross-validation determine the best model. Evaluate your model on test data and report the accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
