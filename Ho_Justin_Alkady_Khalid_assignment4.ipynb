{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "In this Jupyter Notebook, we code a Multi-Layer Perceptron (MLP) for classification and regression tasks using Stochastic Gradient Descent for training the model. The MLP would be able to perform multiclass as well as binary classification. <br>\n",
    "\n",
    "The model would also allow customization of activation function used, as well as training schedule for optimization purposes. The MLP is designed with a single hidden layer architecture. <br>\n",
    "\n",
    "This notebook is dvided into three sections\n",
    "1. Section 1 - Helper functions and multiclass classification\n",
    "1. Section 2 - Binary classification\n",
    "1. Section 3 - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "import json\n",
    "import timeit\n",
    "from math import exp\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the following function that creates a weight matrix and initializes it with small random real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeights(input_neurons,output_neurons):\n",
    "    W = np.random.randn(input_neurons,output_neurons) * 0.01\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the logistic sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement the ReLU (rectified linear unit) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement the tanh (hyperbolic tangent) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implement a MLP Classifier model class for performing multi-class classification.\n",
    "\n",
    "### The MLP Classifier has a single hidden layer. It should have the following four methods.The model uses the back-propagation algorithm for learning the weights of the features/neurons. Note the that “fit” method should implement the Stochastic Gradient Descent algorithm for optimizing the weight update process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper functions for the derivative of activation functions\n",
    "'''\n",
    "def d_logistic(z):\n",
    "    dz = logistic(z) * (1 - logistic(z))\n",
    "    return dz\n",
    "\n",
    "def d_tanh(z):\n",
    "    dz = (1-np.power(np.tanh(z),2))\n",
    "    return dz\n",
    "\n",
    "def d_relu(z):\n",
    "    dz = np.where(z==0,0,1)\n",
    "    return dz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function for one_hot_labels\n",
    "Input:\n",
    "    Y - target vector\n",
    "Output:\n",
    "    one_hot_matrix\n",
    "'''\n",
    "def one_hot_labels(Y):\n",
    "    \n",
    "    # Get unique labels in Y and number of observations\n",
    "    unique_labels = np.unique(Y)\n",
    "    n = len(Y)\n",
    "    \n",
    "    # Create one hot matrix\n",
    "    labels = np.array(list(unique_labels)).reshape(len(unique_labels),1)\n",
    "    one_hot_matrix = np.apply_along_axis(lambda x: np.full((n,),x),1,labels)\n",
    "    one_hot_matrix = np.apply_along_axis(lambda x: (x==Y).astype(int),1,one_hot_matrix).T\n",
    "    \n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function for softmax calculation\n",
    "Given matrix A (output from hidden layer), outputs probabilties   \n",
    "'''\n",
    "def softmax(A):\n",
    "    A = np.apply_along_axis(np.exp,0,A)\n",
    "    A = np.apply_along_axis(lambda x: x/(sum(x)),1,A)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPLETE IMPLEMENTATION OF CLASSIFIER IS AT QUESTION 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Read the handwritten digits datasetusing the sklearn.datasets.load_digits function for performing multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "X,y = shuffle(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Partition the data into train and test set. Use the “Partition” function from your previous assignment or from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Hyperparameter tuning based on certain fixed-values hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed values of tuning\n",
    "regularizer = 'l2'\n",
    "verbose = True\n",
    "early_stopping = True\n",
    "validation_fraction = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to Change\n",
    "hidden_layer_neurons = [5,8,13,21]\n",
    "activations = ['logistic', 'relu', 'tanh']\n",
    "alphas = [0.001, 0.0001]\n",
    "learning_rates = ['adaptive', 'constant']\n",
    "learning_rate_inits = [0.01,0.001]\n",
    "max_iters = [100,150,250]\n",
    "tols = [0.001, 0.0001,0.00001]\n",
    "n_iter_no_changes = [10,15,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sFold(estimator,X,y,scoring,cv):\n",
    "    size = len(X)\n",
    "    size_fold = int(size/cv)\n",
    "    range_index = [j for j in range(0,len(X))]\n",
    "    score=[]\n",
    "    \n",
    "    #Partitioning of the data\n",
    "    for i in range(cv):\n",
    "        init=0+i*size_fold\n",
    "        fin=(i+1)*size_fold\n",
    "        partition_range_index = [j for j in range(init,fin)]\n",
    "        \n",
    "        #Feature and label data of the Fold\n",
    "        X_partition = X[partition_range_index]\n",
    "        y_partition = y[partition_range_index]\n",
    "        \n",
    "        #Feature and label data of  1-Fold        \n",
    "        remainder_index = list(set(range_index).difference(set(partition_range_index)))\n",
    "        X_remainder=X[remainder_index]\n",
    "        y_remainder=y[remainder_index]\n",
    "        \n",
    "        #Fit the model to the 1-Fold data\n",
    "        estimator.fit(X_remainder, y_remainder) \n",
    "        \n",
    "        #Test the model on the fold data\n",
    "        pred=estimator.predict(X_partition)\n",
    "      \n",
    "        if scoring=='accuracy':\n",
    "            acc=accuracy_score(y_partition,pred)\n",
    "            score.append(acc) \n",
    "            \n",
    "    avg_score = np.mean(score)    \n",
    "    return avg_score,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 of 100: Train Loss = 2.300026 | Val Loss = 2.320648 | Train Acc = 0.094281 | Val Acc = 0.069930 | Eta = 0.004359\n",
      "\n",
      "Epoch 1 of 100: Train Loss = 1.986353 | Val Loss = 1.565497 | Train Acc = 0.389490 | Val Acc = 0.587413 | Eta = 0.002787\n",
      "\n",
      "Epoch 2 of 100: Train Loss = 1.189969 | Val Loss = 1.100209 | Train Acc = 0.753478 | Val Acc = 0.699301 | Eta = 0.002048\n",
      "\n",
      "Epoch 3 of 100: Train Loss = 0.927998 | Val Loss = 0.891574 | Train Acc = 0.809892 | Val Acc = 0.783217 | Eta = 0.001619\n",
      "\n",
      "Epoch 4 of 100: Train Loss = 0.776716 | Val Loss = 0.814571 | Train Acc = 0.855487 | Val Acc = 0.776224 | Eta = 0.001339\n",
      "\n",
      "Epoch 5 of 100: Train Loss = 0.727577 | Val Loss = 0.748248 | Train Acc = 0.853941 | Val Acc = 0.832168 | Eta = 0.001141\n",
      "\n",
      "Epoch 6 of 100: Train Loss = 0.703674 | Val Loss = 0.701096 | Train Acc = 0.866306 | Val Acc = 0.888112 | Eta = 0.000994\n",
      "\n",
      "Epoch 7 of 100: Train Loss = 0.684565 | Val Loss = 0.676288 | Train Acc = 0.865533 | Val Acc = 0.874126 | Eta = 0.000881\n",
      "\n",
      "Epoch 8 of 100: Train Loss = 0.617612 | Val Loss = 0.669630 | Train Acc = 0.894127 | Val Acc = 0.846154 | Eta = 0.000791\n",
      "\n",
      "Epoch 9 of 100: Train Loss = 0.608066 | Val Loss = 0.658752 | Train Acc = 0.892581 | Val Acc = 0.839161 | Eta = 0.000717\n",
      "\n",
      "Epoch 10 of 100: Train Loss = 0.597077 | Val Loss = 0.634683 | Train Acc = 0.892581 | Val Acc = 0.881119 | Eta = 0.000656\n",
      "\n",
      "Epoch 11 of 100: Train Loss = 0.583005 | Val Loss = 0.624843 | Train Acc = 0.896445 | Val Acc = 0.881119 | Eta = 0.000605\n",
      "\n",
      "Epoch 12 of 100: Train Loss = 0.567115 | Val Loss = 0.614509 | Train Acc = 0.901855 | Val Acc = 0.881119 | Eta = 0.000561\n",
      "\n",
      "Epoch 13 of 100: Train Loss = 0.569846 | Val Loss = 0.602648 | Train Acc = 0.904173 | Val Acc = 0.888112 | Eta = 0.000523\n",
      "\n",
      "Epoch 14 of 100: Train Loss = 0.562705 | Val Loss = 0.599513 | Train Acc = 0.907264 | Val Acc = 0.881119 | Eta = 0.000490\n",
      "\n",
      "Epoch 15 of 100: Train Loss = 0.548134 | Val Loss = 0.586538 | Train Acc = 0.914219 | Val Acc = 0.888112 | Eta = 0.000461\n",
      "\n",
      "Epoch 16 of 100: Train Loss = 0.558846 | Val Loss = 0.579733 | Train Acc = 0.908810 | Val Acc = 0.909091 | Eta = 0.000435\n",
      "\n",
      "Epoch 17 of 100: Train Loss = 0.542806 | Val Loss = 0.581115 | Train Acc = 0.926584 | Val Acc = 0.902098 | Eta = 0.000412\n",
      "\n",
      "Epoch 18 of 100: Train Loss = 0.535271 | Val Loss = 0.578481 | Train Acc = 0.931994 | Val Acc = 0.902098 | Eta = 0.000391\n",
      "\n",
      "Epoch 19 of 100: Train Loss = 0.529181 | Val Loss = 0.572756 | Train Acc = 0.928130 | Val Acc = 0.916084 | Eta = 0.000372\n",
      "\n",
      "Epoch 20 of 100: Train Loss = 0.539195 | Val Loss = 0.572517 | Train Acc = 0.926584 | Val Acc = 0.916084 | Eta = 0.000355\n",
      "\n",
      "Epoch 21 of 100: Train Loss = 0.511906 | Val Loss = 0.567310 | Train Acc = 0.938176 | Val Acc = 0.916084 | Eta = 0.000339\n",
      "\n",
      "Epoch 22 of 100: Train Loss = 0.503493 | Val Loss = 0.566931 | Train Acc = 0.938176 | Val Acc = 0.895105 | Eta = 0.000325\n",
      "\n",
      "Epoch 23 of 100: Train Loss = 0.522730 | Val Loss = 0.563741 | Train Acc = 0.924266 | Val Acc = 0.909091 | Eta = 0.000312\n",
      "\n",
      "Epoch 24 of 100: Train Loss = 0.514350 | Val Loss = 0.555366 | Train Acc = 0.941267 | Val Acc = 0.923077 | Eta = 0.000300\n",
      "\n",
      "Epoch 25 of 100: Train Loss = 0.521345 | Val Loss = 0.554296 | Train Acc = 0.919629 | Val Acc = 0.916084 | Eta = 0.000289\n",
      "\n",
      "Epoch 26 of 100: Train Loss = 0.494125 | Val Loss = 0.555008 | Train Acc = 0.947450 | Val Acc = 0.909091 | Eta = 0.000278\n",
      "\n",
      "Epoch 27 of 100: Train Loss = 0.508182 | Val Loss = 0.548557 | Train Acc = 0.930448 | Val Acc = 0.923077 | Eta = 0.000269\n",
      "\n",
      "Epoch 28 of 100: Train Loss = 0.511191 | Val Loss = 0.544035 | Train Acc = 0.933539 | Val Acc = 0.930070 | Eta = 0.000260\n",
      "\n",
      "Epoch 29 of 100: Train Loss = 0.494189 | Val Loss = 0.541591 | Train Acc = 0.942813 | Val Acc = 0.930070 | Eta = 0.000251\n",
      "\n",
      "Epoch 30 of 100: Train Loss = 0.484599 | Val Loss = 0.539560 | Train Acc = 0.948223 | Val Acc = 0.923077 | Eta = 0.000243\n",
      "\n",
      "Epoch 31 of 100: Train Loss = 0.491358 | Val Loss = 0.541114 | Train Acc = 0.939722 | Val Acc = 0.916084 | Eta = 0.000236\n",
      "\n",
      "Epoch 32 of 100: Train Loss = 0.482127 | Val Loss = 0.542297 | Train Acc = 0.947450 | Val Acc = 0.916084 | Eta = 0.000229\n",
      "\n",
      "Epoch 33 of 100: Train Loss = 0.489928 | Val Loss = 0.543300 | Train Acc = 0.940495 | Val Acc = 0.902098 | Eta = 0.000222\n",
      "\n",
      "Epoch 34 of 100: Train Loss = 0.461909 | Val Loss = 0.541243 | Train Acc = 0.955178 | Val Acc = 0.909091 | Eta = 0.000216\n",
      "\n",
      "Epoch 35 of 100: Train Loss = 0.486464 | Val Loss = 0.541708 | Train Acc = 0.942813 | Val Acc = 0.909091 | Eta = 0.000210\n",
      "\n",
      "Epoch 36 of 100: Train Loss = 0.491381 | Val Loss = 0.537664 | Train Acc = 0.944359 | Val Acc = 0.909091 | Eta = 0.000205\n",
      "\n",
      "Epoch 37 of 100: Train Loss = 0.474479 | Val Loss = 0.536935 | Train Acc = 0.945904 | Val Acc = 0.916084 | Eta = 0.000199\n",
      "\n",
      "Epoch 38 of 100: Train Loss = 0.462052 | Val Loss = 0.536076 | Train Acc = 0.953632 | Val Acc = 0.930070 | Eta = 0.000194\n",
      "\n",
      "Epoch 39 of 100: Train Loss = 0.479643 | Val Loss = 0.535387 | Train Acc = 0.945131 | Val Acc = 0.916084 | Eta = 0.000190\n",
      "\n",
      "Epoch 40 of 100: Train Loss = 0.479782 | Val Loss = 0.535773 | Train Acc = 0.952087 | Val Acc = 0.916084 | Eta = 0.000185\n",
      "\n",
      "Epoch 41 of 100: Train Loss = 0.478486 | Val Loss = 0.533813 | Train Acc = 0.942813 | Val Acc = 0.930070 | Eta = 0.000181\n",
      "\n",
      "Epoch 42 of 100: Train Loss = 0.480877 | Val Loss = 0.530150 | Train Acc = 0.948223 | Val Acc = 0.930070 | Eta = 0.000177\n",
      "\n",
      "Epoch 43 of 100: Train Loss = 0.478869 | Val Loss = 0.527872 | Train Acc = 0.943586 | Val Acc = 0.930070 | Eta = 0.000173\n",
      "\n",
      "Epoch 44 of 100: Train Loss = 0.475406 | Val Loss = 0.527501 | Train Acc = 0.937403 | Val Acc = 0.916084 | Eta = 0.000169\n",
      "\n",
      "Epoch 45 of 100: Train Loss = 0.455635 | Val Loss = 0.526913 | Train Acc = 0.952859 | Val Acc = 0.923077 | Eta = 0.000165\n",
      "\n",
      "Epoch 46 of 100: Train Loss = 0.470996 | Val Loss = 0.527407 | Train Acc = 0.942813 | Val Acc = 0.923077 | Eta = 0.000162\n",
      "\n",
      "Epoch 47 of 100: Train Loss = 0.466998 | Val Loss = 0.523691 | Train Acc = 0.945131 | Val Acc = 0.937063 | Eta = 0.000158\n",
      "\n",
      "Epoch 48 of 100: Train Loss = 0.480005 | Val Loss = 0.524732 | Train Acc = 0.935858 | Val Acc = 0.937063 | Eta = 0.000155\n",
      "\n",
      "Epoch 49 of 100: Train Loss = 0.468856 | Val Loss = 0.524023 | Train Acc = 0.945131 | Val Acc = 0.937063 | Eta = 0.000152\n",
      "\n",
      "Epoch 50 of 100: Train Loss = 0.463961 | Val Loss = 0.522082 | Train Acc = 0.942040 | Val Acc = 0.916084 | Eta = 0.000149\n",
      "\n",
      "Epoch 51 of 100: Train Loss = 0.474616 | Val Loss = 0.521021 | Train Acc = 0.945131 | Val Acc = 0.909091 | Eta = 0.000146\n",
      "\n",
      "Epoch 52 of 100: Train Loss = 0.457653 | Val Loss = 0.520471 | Train Acc = 0.955178 | Val Acc = 0.916084 | Eta = 0.000144\n",
      "\n",
      "Epoch 53 of 100: Train Loss = 0.465498 | Val Loss = 0.521058 | Train Acc = 0.948223 | Val Acc = 0.930070 | Eta = 0.000141\n",
      "\n",
      "Epoch 54 of 100: Train Loss = 0.476538 | Val Loss = 0.520434 | Train Acc = 0.944359 | Val Acc = 0.923077 | Eta = 0.000139\n",
      "\n",
      "Epoch 55 of 100: Train Loss = 0.461307 | Val Loss = 0.521034 | Train Acc = 0.952087 | Val Acc = 0.916084 | Eta = 0.000136\n",
      "\n",
      "Epoch 56 of 100: Train Loss = 0.443219 | Val Loss = 0.519677 | Train Acc = 0.955951 | Val Acc = 0.909091 | Eta = 0.000134\n",
      "\n",
      "Epoch 57 of 100: Train Loss = 0.450156 | Val Loss = 0.520325 | Train Acc = 0.947450 | Val Acc = 0.909091 | Eta = 0.000131\n",
      "\n",
      "Epoch 58 of 100: Train Loss = 0.462591 | Val Loss = 0.516314 | Train Acc = 0.946677 | Val Acc = 0.923077 | Eta = 0.000129\n",
      "\n",
      "Epoch 59 of 100: Train Loss = 0.441725 | Val Loss = 0.517928 | Train Acc = 0.954405 | Val Acc = 0.916084 | Eta = 0.000127\n",
      "\n",
      "Epoch 60 of 100: Train Loss = 0.462410 | Val Loss = 0.515772 | Train Acc = 0.950541 | Val Acc = 0.916084 | Eta = 0.000125\n",
      "\n",
      "Epoch 61 of 100: Train Loss = 0.456689 | Val Loss = 0.515604 | Train Acc = 0.945904 | Val Acc = 0.909091 | Eta = 0.000123\n",
      "\n",
      "Epoch 62 of 100: Train Loss = 0.480463 | Val Loss = 0.515828 | Train Acc = 0.938176 | Val Acc = 0.916084 | Eta = 0.000121\n",
      "\n",
      "Epoch 63 of 100: Train Loss = 0.446980 | Val Loss = 0.518207 | Train Acc = 0.954405 | Val Acc = 0.909091 | Eta = 0.000119\n",
      "\n",
      "Epoch 64 of 100: Train Loss = 0.472745 | Val Loss = 0.516565 | Train Acc = 0.938176 | Val Acc = 0.909091 | Eta = 0.000117\n",
      "\n",
      "Epoch 65 of 100: Train Loss = 0.450574 | Val Loss = 0.517630 | Train Acc = 0.946677 | Val Acc = 0.909091 | Eta = 0.000116\n",
      "\n",
      "Epoch 66 of 100: Train Loss = 0.443332 | Val Loss = 0.514531 | Train Acc = 0.949768 | Val Acc = 0.916084 | Eta = 0.000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67 of 100: Train Loss = 0.449818 | Val Loss = 0.514658 | Train Acc = 0.948223 | Val Acc = 0.916084 | Eta = 0.000112\n",
      "\n",
      "Epoch 68 of 100: Train Loss = 0.442142 | Val Loss = 0.513251 | Train Acc = 0.958269 | Val Acc = 0.916084 | Eta = 0.000111\n",
      "\n",
      "Epoch 69 of 100: Train Loss = 0.452546 | Val Loss = 0.510533 | Train Acc = 0.946677 | Val Acc = 0.916084 | Eta = 0.000109\n",
      "\n",
      "Epoch 70 of 100: Train Loss = 0.467415 | Val Loss = 0.511230 | Train Acc = 0.943586 | Val Acc = 0.916084 | Eta = 0.000108\n",
      "\n",
      "Epoch 71 of 100: Train Loss = 0.455914 | Val Loss = 0.511680 | Train Acc = 0.947450 | Val Acc = 0.916084 | Eta = 0.000106\n",
      "\n",
      "Epoch 72 of 100: Train Loss = 0.449768 | Val Loss = 0.510915 | Train Acc = 0.955178 | Val Acc = 0.916084 | Eta = 0.000105\n",
      "\n",
      "Epoch 73 of 100: Train Loss = 0.443187 | Val Loss = 0.511721 | Train Acc = 0.955178 | Val Acc = 0.916084 | Eta = 0.000103\n",
      "\n",
      "Epoch 74 of 100: Train Loss = 0.433129 | Val Loss = 0.511394 | Train Acc = 0.956723 | Val Acc = 0.916084 | Eta = 0.000102\n",
      "\n",
      "Epoch 75 of 100: Train Loss = 0.445613 | Val Loss = 0.512514 | Train Acc = 0.949768 | Val Acc = 0.916084 | Eta = 0.000101\n",
      "\n",
      "Early Stopping because the validation accuracy change between two consecutive epochs is less than 0.001000 over the last 10 iterations\n",
      "\n",
      "Epoch 0 of 100: Train Loss = 2.302472 | Val Loss = 2.302982 | Train Acc = 0.102009 | Val Acc = 0.111888 | Eta = 0.000436\n",
      "\n",
      "Epoch 1 of 100: Train Loss = 2.301262 | Val Loss = 2.306442 | Train Acc = 0.108964 | Val Acc = 0.111888 | Eta = 0.000279\n",
      "\n",
      "Epoch 2 of 100: Train Loss = 2.299670 | Val Loss = 2.313017 | Train Acc = 0.102009 | Val Acc = 0.132867 | Eta = 0.000205\n",
      "\n",
      "Epoch 3 of 100: Train Loss = 2.301727 | Val Loss = 2.308079 | Train Acc = 0.116692 | Val Acc = 0.069930 | Eta = 0.000162\n",
      "\n",
      "Epoch 4 of 100: Train Loss = 2.297013 | Val Loss = 2.308264 | Train Acc = 0.108964 | Val Acc = 0.069930 | Eta = 0.000134\n",
      "\n",
      "Epoch 5 of 100: Train Loss = 2.291422 | Val Loss = 2.302658 | Train Acc = 0.108192 | Val Acc = 0.069930 | Eta = 0.000114\n",
      "\n",
      "Epoch 6 of 100: Train Loss = 2.286662 | Val Loss = 2.297098 | Train Acc = 0.106646 | Val Acc = 0.069930 | Eta = 0.000099\n",
      "\n",
      "Epoch 7 of 100: Train Loss = 2.273938 | Val Loss = 2.290923 | Train Acc = 0.162287 | Val Acc = 0.069930 | Eta = 0.000088\n",
      "\n",
      "Epoch 8 of 100: Train Loss = 2.269473 | Val Loss = 2.284247 | Train Acc = 0.115147 | Val Acc = 0.153846 | Eta = 0.000079\n",
      "\n",
      "Epoch 9 of 100: Train Loss = 2.264585 | Val Loss = 2.273366 | Train Acc = 0.143740 | Val Acc = 0.174825 | Eta = 0.000072\n",
      "\n",
      "Epoch 10 of 100: Train Loss = 2.252605 | Val Loss = 2.262953 | Train Acc = 0.217929 | Val Acc = 0.188811 | Eta = 0.000066\n",
      "\n",
      "Epoch 11 of 100: Train Loss = 2.243676 | Val Loss = 2.250020 | Train Acc = 0.217156 | Val Acc = 0.188811 | Eta = 0.000061\n",
      "\n",
      "Epoch 12 of 100: Train Loss = 2.233746 | Val Loss = 2.239515 | Train Acc = 0.214838 | Val Acc = 0.188811 | Eta = 0.000056\n",
      "\n",
      "Epoch 13 of 100: Train Loss = 2.222058 | Val Loss = 2.226048 | Train Acc = 0.235703 | Val Acc = 0.265734 | Eta = 0.000052\n",
      "\n",
      "Epoch 14 of 100: Train Loss = 2.210341 | Val Loss = 2.213045 | Train Acc = 0.262751 | Val Acc = 0.258741 | Eta = 0.000049\n",
      "\n",
      "Epoch 15 of 100: Train Loss = 2.199803 | Val Loss = 2.202585 | Train Acc = 0.250386 | Val Acc = 0.230769 | Eta = 0.000046\n",
      "\n",
      "Epoch 16 of 100: Train Loss = 2.192104 | Val Loss = 2.190793 | Train Acc = 0.300618 | Val Acc = 0.244755 | Eta = 0.000043\n",
      "\n",
      "Epoch 17 of 100: Train Loss = 2.172876 | Val Loss = 2.178940 | Train Acc = 0.323802 | Val Acc = 0.265734 | Eta = 0.000041\n",
      "\n",
      "Epoch 18 of 100: Train Loss = 2.158676 | Val Loss = 2.168315 | Train Acc = 0.343122 | Val Acc = 0.293706 | Eta = 0.000039\n",
      "\n",
      "Epoch 19 of 100: Train Loss = 2.143320 | Val Loss = 2.158289 | Train Acc = 0.358578 | Val Acc = 0.286713 | Eta = 0.000037\n",
      "\n",
      "Epoch 20 of 100: Train Loss = 2.125754 | Val Loss = 2.148455 | Train Acc = 0.333849 | Val Acc = 0.286713 | Eta = 0.000035\n",
      "\n",
      "Epoch 21 of 100: Train Loss = 2.128045 | Val Loss = 2.137961 | Train Acc = 0.343895 | Val Acc = 0.314685 | Eta = 0.000034\n",
      "\n",
      "Epoch 22 of 100: Train Loss = 2.114725 | Val Loss = 2.127592 | Train Acc = 0.377898 | Val Acc = 0.342657 | Eta = 0.000033\n",
      "\n",
      "Epoch 23 of 100: Train Loss = 2.105299 | Val Loss = 2.118701 | Train Acc = 0.391808 | Val Acc = 0.349650 | Eta = 0.000031\n",
      "\n",
      "Epoch 24 of 100: Train Loss = 2.097391 | Val Loss = 2.109517 | Train Acc = 0.393354 | Val Acc = 0.293706 | Eta = 0.000030\n",
      "\n",
      "Epoch 25 of 100: Train Loss = 2.096120 | Val Loss = 2.099689 | Train Acc = 0.367079 | Val Acc = 0.286713 | Eta = 0.000029\n",
      "\n",
      "Epoch 26 of 100: Train Loss = 2.083847 | Val Loss = 2.090250 | Train Acc = 0.352396 | Val Acc = 0.314685 | Eta = 0.000028\n",
      "\n",
      "Epoch 27 of 100: Train Loss = 2.059483 | Val Loss = 2.081929 | Train Acc = 0.387944 | Val Acc = 0.286713 | Eta = 0.000027\n",
      "\n",
      "Epoch 28 of 100: Train Loss = 2.060366 | Val Loss = 2.073302 | Train Acc = 0.355487 | Val Acc = 0.321678 | Eta = 0.000026\n",
      "\n",
      "Epoch 29 of 100: Train Loss = 2.052233 | Val Loss = 2.065916 | Train Acc = 0.366306 | Val Acc = 0.321678 | Eta = 0.000025\n",
      "\n",
      "Epoch 30 of 100: Train Loss = 2.044647 | Val Loss = 2.058132 | Train Acc = 0.367852 | Val Acc = 0.356643 | Eta = 0.000024\n",
      "\n",
      "Epoch 31 of 100: Train Loss = 2.036558 | Val Loss = 2.050774 | Train Acc = 0.395672 | Val Acc = 0.363636 | Eta = 0.000024\n",
      "\n",
      "Epoch 32 of 100: Train Loss = 2.031509 | Val Loss = 2.043840 | Train Acc = 0.397218 | Val Acc = 0.370629 | Eta = 0.000023\n",
      "\n",
      "Epoch 33 of 100: Train Loss = 2.024265 | Val Loss = 2.035962 | Train Acc = 0.418083 | Val Acc = 0.391608 | Eta = 0.000022\n",
      "\n",
      "Epoch 34 of 100: Train Loss = 2.003021 | Val Loss = 2.029293 | Train Acc = 0.473725 | Val Acc = 0.377622 | Eta = 0.000022\n",
      "\n",
      "Epoch 35 of 100: Train Loss = 1.993883 | Val Loss = 2.022754 | Train Acc = 0.455951 | Val Acc = 0.398601 | Eta = 0.000021\n",
      "\n",
      "Epoch 36 of 100: Train Loss = 1.992810 | Val Loss = 2.016212 | Train Acc = 0.445904 | Val Acc = 0.405594 | Eta = 0.000020\n",
      "\n",
      "Epoch 37 of 100: Train Loss = 1.994338 | Val Loss = 2.009010 | Train Acc = 0.447450 | Val Acc = 0.419580 | Eta = 0.000020\n",
      "\n",
      "Epoch 38 of 100: Train Loss = 1.980407 | Val Loss = 2.002803 | Train Acc = 0.465997 | Val Acc = 0.419580 | Eta = 0.000019\n",
      "\n",
      "Epoch 39 of 100: Train Loss = 1.986234 | Val Loss = 1.996685 | Train Acc = 0.436631 | Val Acc = 0.412587 | Eta = 0.000019\n",
      "\n",
      "Epoch 40 of 100: Train Loss = 1.975099 | Val Loss = 1.990362 | Train Acc = 0.465997 | Val Acc = 0.419580 | Eta = 0.000019\n",
      "\n",
      "Epoch 41 of 100: Train Loss = 1.962246 | Val Loss = 1.984734 | Train Acc = 0.486862 | Val Acc = 0.433566 | Eta = 0.000018\n",
      "\n",
      "Epoch 42 of 100: Train Loss = 1.973213 | Val Loss = 1.978833 | Train Acc = 0.472179 | Val Acc = 0.426573 | Eta = 0.000018\n",
      "\n",
      "Epoch 43 of 100: Train Loss = 1.932716 | Val Loss = 1.973833 | Train Acc = 0.521638 | Val Acc = 0.433566 | Eta = 0.000017\n",
      "\n",
      "Epoch 44 of 100: Train Loss = 1.946975 | Val Loss = 1.968309 | Train Acc = 0.476043 | Val Acc = 0.447552 | Eta = 0.000017\n",
      "\n",
      "Epoch 45 of 100: Train Loss = 1.936171 | Val Loss = 1.963344 | Train Acc = 0.481453 | Val Acc = 0.440559 | Eta = 0.000017\n",
      "\n",
      "Epoch 46 of 100: Train Loss = 1.939015 | Val Loss = 1.958505 | Train Acc = 0.482998 | Val Acc = 0.440559 | Eta = 0.000016\n",
      "\n",
      "Epoch 47 of 100: Train Loss = 1.940281 | Val Loss = 1.953016 | Train Acc = 0.487635 | Val Acc = 0.440559 | Eta = 0.000016\n",
      "\n",
      "Epoch 48 of 100: Train Loss = 1.917446 | Val Loss = 1.948641 | Train Acc = 0.523184 | Val Acc = 0.461538 | Eta = 0.000016\n",
      "\n",
      "Epoch 49 of 100: Train Loss = 1.907363 | Val Loss = 1.943893 | Train Acc = 0.519320 | Val Acc = 0.468531 | Eta = 0.000015\n",
      "\n",
      "Epoch 50 of 100: Train Loss = 1.916589 | Val Loss = 1.939380 | Train Acc = 0.513138 | Val Acc = 0.468531 | Eta = 0.000015\n",
      "\n",
      "Epoch 51 of 100: Train Loss = 1.911776 | Val Loss = 1.934707 | Train Acc = 0.524730 | Val Acc = 0.461538 | Eta = 0.000015\n",
      "\n",
      "Epoch 52 of 100: Train Loss = 1.908456 | Val Loss = 1.930429 | Train Acc = 0.531685 | Val Acc = 0.475524 | Eta = 0.000014\n",
      "\n",
      "Epoch 53 of 100: Train Loss = 1.892854 | Val Loss = 1.926015 | Train Acc = 0.563369 | Val Acc = 0.475524 | Eta = 0.000014\n",
      "\n",
      "Epoch 54 of 100: Train Loss = 1.896303 | Val Loss = 1.921587 | Train Acc = 0.544822 | Val Acc = 0.475524 | Eta = 0.000014\n",
      "\n",
      "Epoch 55 of 100: Train Loss = 1.887586 | Val Loss = 1.917525 | Train Acc = 0.550232 | Val Acc = 0.496503 | Eta = 0.000014\n",
      "\n",
      "Epoch 56 of 100: Train Loss = 1.900927 | Val Loss = 1.913241 | Train Acc = 0.529366 | Val Acc = 0.503497 | Eta = 0.000013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 57 of 100: Train Loss = 1.893424 | Val Loss = 1.908785 | Train Acc = 0.544822 | Val Acc = 0.503497 | Eta = 0.000013\n",
      "\n",
      "Epoch 58 of 100: Train Loss = 1.865292 | Val Loss = 1.904602 | Train Acc = 0.570325 | Val Acc = 0.503497 | Eta = 0.000013\n",
      "\n",
      "Epoch 59 of 100: Train Loss = 1.871141 | Val Loss = 1.901303 | Train Acc = 0.555641 | Val Acc = 0.503497 | Eta = 0.000013\n",
      "\n",
      "Epoch 60 of 100: Train Loss = 1.873646 | Val Loss = 1.896894 | Train Acc = 0.561051 | Val Acc = 0.503497 | Eta = 0.000013\n",
      "\n",
      "Epoch 61 of 100: Train Loss = 1.876810 | Val Loss = 1.892782 | Train Acc = 0.544822 | Val Acc = 0.503497 | Eta = 0.000012\n",
      "\n",
      "Epoch 62 of 100: Train Loss = 1.848725 | Val Loss = 1.889171 | Train Acc = 0.580371 | Val Acc = 0.503497 | Eta = 0.000012\n",
      "\n",
      "Epoch 63 of 100: Train Loss = 1.855649 | Val Loss = 1.885261 | Train Acc = 0.552550 | Val Acc = 0.503497 | Eta = 0.000012\n",
      "\n",
      "Epoch 64 of 100: Train Loss = 1.843366 | Val Loss = 1.881844 | Train Acc = 0.571097 | Val Acc = 0.510490 | Eta = 0.000012\n",
      "\n",
      "Epoch 65 of 100: Train Loss = 1.853221 | Val Loss = 1.878112 | Train Acc = 0.565688 | Val Acc = 0.517483 | Eta = 0.000012\n",
      "\n",
      "Epoch 66 of 100: Train Loss = 1.839035 | Val Loss = 1.874642 | Train Acc = 0.571870 | Val Acc = 0.517483 | Eta = 0.000011\n",
      "\n",
      "Epoch 67 of 100: Train Loss = 1.840403 | Val Loss = 1.871264 | Train Acc = 0.551005 | Val Acc = 0.517483 | Eta = 0.000011\n",
      "\n",
      "Epoch 68 of 100: Train Loss = 1.841497 | Val Loss = 1.867839 | Train Acc = 0.562597 | Val Acc = 0.517483 | Eta = 0.000011\n",
      "\n",
      "Epoch 69 of 100: Train Loss = 1.838871 | Val Loss = 1.864725 | Train Acc = 0.568006 | Val Acc = 0.517483 | Eta = 0.000011\n",
      "\n",
      "Epoch 70 of 100: Train Loss = 1.837265 | Val Loss = 1.861274 | Train Acc = 0.574189 | Val Acc = 0.517483 | Eta = 0.000011\n",
      "\n",
      "Epoch 71 of 100: Train Loss = 1.815540 | Val Loss = 1.858164 | Train Acc = 0.601236 | Val Acc = 0.517483 | Eta = 0.000011\n",
      "\n",
      "Epoch 72 of 100: Train Loss = 1.839124 | Val Loss = 1.854366 | Train Acc = 0.560278 | Val Acc = 0.517483 | Eta = 0.000010\n",
      "\n",
      "Epoch 73 of 100: Train Loss = 1.812202 | Val Loss = 1.851138 | Train Acc = 0.600464 | Val Acc = 0.517483 | Eta = 0.000010\n",
      "\n",
      "Epoch 74 of 100: Train Loss = 1.803941 | Val Loss = 1.848128 | Train Acc = 0.585008 | Val Acc = 0.517483 | Eta = 0.000010\n",
      "\n",
      "Early Stopping because the validation accuracy change between two consecutive epochs is less than 0.001000 over the last 10 iterations\n",
      "\n",
      "Epoch 0 of 100: Train Loss = 2.301546 | Val Loss = 2.290297 | Train Acc = 0.114374 | Val Acc = 0.174825 | Eta = 0.010000\n",
      "\n",
      "Epoch 1 of 100: Train Loss = 1.336760 | Val Loss = 0.825828 | Train Acc = 0.600464 | Val Acc = 0.797203 | Eta = 0.010000\n",
      "\n",
      "Epoch 2 of 100: Train Loss = 0.613445 | Val Loss = 0.567065 | Train Acc = 0.900309 | Val Acc = 0.895105 | Eta = 0.010000\n",
      "\n",
      "Epoch 3 of 100: Train Loss = 0.515285 | Val Loss = 0.521353 | Train Acc = 0.925811 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 4 of 100: Train Loss = 0.476561 | Val Loss = 0.514550 | Train Acc = 0.939722 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 5 of 100: Train Loss = 0.463103 | Val Loss = 0.498397 | Train Acc = 0.944359 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 6 of 100: Train Loss = 0.455031 | Val Loss = 0.566343 | Train Acc = 0.945904 | Val Acc = 0.874126 | Eta = 0.010000\n",
      "\n",
      "Epoch 7 of 100: Train Loss = 0.456469 | Val Loss = 0.549471 | Train Acc = 0.941267 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 8 of 100: Train Loss = 0.441211 | Val Loss = 0.555890 | Train Acc = 0.955178 | Val Acc = 0.895105 | Eta = 0.010000\n",
      "\n",
      "Epoch 9 of 100: Train Loss = 0.417074 | Val Loss = 0.531951 | Train Acc = 0.964451 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 10 of 100: Train Loss = 0.436491 | Val Loss = 0.555718 | Train Acc = 0.952087 | Val Acc = 0.888112 | Eta = 0.010000\n",
      "\n",
      "Epoch 11 of 100: Train Loss = 0.430658 | Val Loss = 0.538494 | Train Acc = 0.952859 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 12 of 100: Train Loss = 0.457517 | Val Loss = 0.570381 | Train Acc = 0.948223 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 13 of 100: Train Loss = 0.460627 | Val Loss = 0.554637 | Train Acc = 0.951314 | Val Acc = 0.895105 | Eta = 0.010000\n",
      "\n",
      "Epoch 14 of 100: Train Loss = 0.447903 | Val Loss = 0.535415 | Train Acc = 0.951314 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 15 of 100: Train Loss = 0.446268 | Val Loss = 0.572009 | Train Acc = 0.952859 | Val Acc = 0.895105 | Eta = 0.010000\n",
      "\n",
      "Epoch 16 of 100: Train Loss = 0.424908 | Val Loss = 0.514369 | Train Acc = 0.966770 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 17 of 100: Train Loss = 0.435376 | Val Loss = 0.532815 | Train Acc = 0.962133 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 18 of 100: Train Loss = 0.447814 | Val Loss = 0.490129 | Train Acc = 0.958269 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 19 of 100: Train Loss = 0.441253 | Val Loss = 0.524116 | Train Acc = 0.954405 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 20 of 100: Train Loss = 0.456395 | Val Loss = 0.545119 | Train Acc = 0.946677 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 21 of 100: Train Loss = 0.411076 | Val Loss = 0.523050 | Train Acc = 0.967543 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 22 of 100: Train Loss = 0.431172 | Val Loss = 0.507108 | Train Acc = 0.959815 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 23 of 100: Train Loss = 0.447409 | Val Loss = 0.514010 | Train Acc = 0.953632 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 24 of 100: Train Loss = 0.427948 | Val Loss = 0.531258 | Train Acc = 0.957496 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 25 of 100: Train Loss = 0.446732 | Val Loss = 0.552044 | Train Acc = 0.952859 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 26 of 100: Train Loss = 0.434401 | Val Loss = 0.505562 | Train Acc = 0.956723 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 27 of 100: Train Loss = 0.443209 | Val Loss = 0.544926 | Train Acc = 0.954405 | Val Acc = 0.895105 | Eta = 0.010000\n",
      "\n",
      "Epoch 28 of 100: Train Loss = 0.423442 | Val Loss = 0.487088 | Train Acc = 0.962133 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 29 of 100: Train Loss = 0.429513 | Val Loss = 0.501116 | Train Acc = 0.954405 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 30 of 100: Train Loss = 0.436356 | Val Loss = 0.521117 | Train Acc = 0.953632 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 31 of 100: Train Loss = 0.469606 | Val Loss = 0.553529 | Train Acc = 0.948223 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 32 of 100: Train Loss = 0.424355 | Val Loss = 0.572151 | Train Acc = 0.958269 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 33 of 100: Train Loss = 0.410478 | Val Loss = 0.547172 | Train Acc = 0.966770 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 34 of 100: Train Loss = 0.439120 | Val Loss = 0.588712 | Train Acc = 0.958269 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 35 of 100: Train Loss = 0.425667 | Val Loss = 0.559580 | Train Acc = 0.952087 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 36 of 100: Train Loss = 0.422239 | Val Loss = 0.517831 | Train Acc = 0.964451 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 37 of 100: Train Loss = 0.420067 | Val Loss = 0.534617 | Train Acc = 0.961360 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 38 of 100: Train Loss = 0.413329 | Val Loss = 0.555166 | Train Acc = 0.962906 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 39 of 100: Train Loss = 0.419317 | Val Loss = 0.536321 | Train Acc = 0.962133 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 40 of 100: Train Loss = 0.446761 | Val Loss = 0.534481 | Train Acc = 0.945131 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 41 of 100: Train Loss = 0.392516 | Val Loss = 0.537438 | Train Acc = 0.972952 | Val Acc = 0.888112 | Eta = 0.010000\n",
      "\n",
      "Epoch 42 of 100: Train Loss = 0.408646 | Val Loss = 0.531758 | Train Acc = 0.964451 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 43 of 100: Train Loss = 0.428772 | Val Loss = 0.524493 | Train Acc = 0.949768 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 44 of 100: Train Loss = 0.434990 | Val Loss = 0.561932 | Train Acc = 0.954405 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 45 of 100: Train Loss = 0.430547 | Val Loss = 0.545498 | Train Acc = 0.960587 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 46 of 100: Train Loss = 0.453187 | Val Loss = 0.532666 | Train Acc = 0.950541 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 47 of 100: Train Loss = 0.436034 | Val Loss = 0.458956 | Train Acc = 0.957496 | Val Acc = 0.930070 | Eta = 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48 of 100: Train Loss = 0.433113 | Val Loss = 0.506724 | Train Acc = 0.952087 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 49 of 100: Train Loss = 0.431252 | Val Loss = 0.493695 | Train Acc = 0.957496 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 50 of 100: Train Loss = 0.394376 | Val Loss = 0.479310 | Train Acc = 0.967543 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 51 of 100: Train Loss = 0.398622 | Val Loss = 0.590169 | Train Acc = 0.969088 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 52 of 100: Train Loss = 0.424233 | Val Loss = 0.558060 | Train Acc = 0.959042 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 53 of 100: Train Loss = 0.450203 | Val Loss = 0.556889 | Train Acc = 0.957496 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 54 of 100: Train Loss = 0.453299 | Val Loss = 0.561455 | Train Acc = 0.952087 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 55 of 100: Train Loss = 0.452604 | Val Loss = 0.497591 | Train Acc = 0.957496 | Val Acc = 0.937063 | Eta = 0.010000\n",
      "\n",
      "Epoch 56 of 100: Train Loss = 0.409541 | Val Loss = 0.530329 | Train Acc = 0.965224 | Val Acc = 0.937063 | Eta = 0.010000\n",
      "\n",
      "Epoch 57 of 100: Train Loss = 0.454037 | Val Loss = 0.530386 | Train Acc = 0.945904 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 58 of 100: Train Loss = 0.458515 | Val Loss = 0.527824 | Train Acc = 0.951314 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 59 of 100: Train Loss = 0.421165 | Val Loss = 0.531634 | Train Acc = 0.964451 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 60 of 100: Train Loss = 0.406546 | Val Loss = 0.514827 | Train Acc = 0.964451 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 61 of 100: Train Loss = 0.401742 | Val Loss = 0.510751 | Train Acc = 0.967543 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 62 of 100: Train Loss = 0.418190 | Val Loss = 0.506141 | Train Acc = 0.953632 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 63 of 100: Train Loss = 0.416295 | Val Loss = 0.516016 | Train Acc = 0.964451 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 64 of 100: Train Loss = 0.435429 | Val Loss = 0.530303 | Train Acc = 0.952859 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 65 of 100: Train Loss = 0.415473 | Val Loss = 0.516772 | Train Acc = 0.961360 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 66 of 100: Train Loss = 0.405936 | Val Loss = 0.514195 | Train Acc = 0.965224 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 67 of 100: Train Loss = 0.411571 | Val Loss = 0.555971 | Train Acc = 0.967543 | Val Acc = 0.888112 | Eta = 0.010000\n",
      "\n",
      "Epoch 68 of 100: Train Loss = 0.443171 | Val Loss = 0.503341 | Train Acc = 0.952859 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 69 of 100: Train Loss = 0.402700 | Val Loss = 0.522276 | Train Acc = 0.967543 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 70 of 100: Train Loss = 0.409656 | Val Loss = 0.513357 | Train Acc = 0.966770 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 71 of 100: Train Loss = 0.434918 | Val Loss = 0.558424 | Train Acc = 0.954405 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 72 of 100: Train Loss = 0.408729 | Val Loss = 0.502012 | Train Acc = 0.967543 | Val Acc = 0.937063 | Eta = 0.010000\n",
      "\n",
      "Epoch 73 of 100: Train Loss = 0.427705 | Val Loss = 0.489306 | Train Acc = 0.959815 | Val Acc = 0.923077 | Eta = 0.010000\n",
      "\n",
      "Epoch 74 of 100: Train Loss = 0.431083 | Val Loss = 0.519536 | Train Acc = 0.956723 | Val Acc = 0.909091 | Eta = 0.010000\n",
      "\n",
      "Epoch 75 of 100: Train Loss = 0.457740 | Val Loss = 0.518467 | Train Acc = 0.946677 | Val Acc = 0.930070 | Eta = 0.010000\n",
      "\n",
      "Epoch 76 of 100: Train Loss = 0.423220 | Val Loss = 0.527092 | Train Acc = 0.968315 | Val Acc = 0.902098 | Eta = 0.010000\n",
      "\n",
      "Epoch 77 of 100: Train Loss = 0.441569 | Val Loss = 0.496448 | Train Acc = 0.953632 | Val Acc = 0.916084 | Eta = 0.010000\n",
      "\n",
      "Epoch 78 of 100: Train Loss = 0.454853 | Val Loss = 0.586113 | Train Acc = 0.952087 | Val Acc = 0.888112 | Eta = 0.010000\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for hidden_layer_neuron in hidden_layer_neurons:\n",
    "    for activation in activations:\n",
    "        for alpha in alphas:\n",
    "            for max_iter in max_iters:\n",
    "                for tol in tols:\n",
    "                    for n_iter_no_change in n_iter_no_changes:\n",
    "                        for learning_rate in learning_rates:\n",
    "                            for learning_rate_init in learning_rate_inits:\n",
    "                                \n",
    "                                label = 'hidden_layer_neuron_' + str(hidden_layer_neuron) + '_activation_' + str(activation) + '_alpha_' + str(alpha) + '_max_iter_' + str(max_iter) + '_tol_' + str(tol) + 'iter_no_change' + str(n_iter_no_change) + '_learning_rate_' + learning_rate + '_learning_rate_init_' + str(learning_rate_init)\n",
    "\n",
    "\n",
    "                                model = MLPClassifier(hidden_layer_neurons=hidden_layer_neuron, activation= activation, regularizer=regularizer, \n",
    "                                                 alpha=alpha, learning_rate=learning_rate, learning_rate_scheduler='timeBasedDecay',\n",
    "                                                 learning_rate_init=learning_rate_init, tol=tol, early_stopping=True, n_iter_no_change=n_iter_no_change, \n",
    "                                                 momentum=True, beta=0.9, lambda_plateau=0.5, Decay=0.001)\n",
    "                                \n",
    "                                model.fit(X_train, y_train, validation_fraction=0.1, max_iter=max_iter, verbose=True)\n",
    "                                y_pred = model.predict(X_test)\n",
    "\n",
    "                                \n",
    "                                results[label] = accuracy_calc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MultiClass_Performance.json', 'w') as fp:\n",
    "    json.dump(results, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Report on performance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Implement binary classification module in theMLPClassifier. Then, performbinary classification on the handwritten digits dataset to recognize the digits “5” and “not-5”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(Y,Y_proba):\n",
    "    loss=-(np.dot(Y,np.log(Y_proba))+np.dot((1-Y),np.log((1-Y_proba))))/Y.shape[0]\n",
    "    return loss\n",
    "\n",
    "def binary_cross_entropy_l2(Y,Y_proba,Theta_1,Theta_2,lambd):\n",
    "    loss=-(np.dot(Y,np.log(Y_proba))+np.dot((1-Y),np.log((1-Y_proba))))/Y.shape[0]+ 0.5*lambd*np.sum(np.square(Theta_1[1:]))+ 0.5*lambd*np.sum(np.square(Theta_2[1:]))\n",
    "    return loss\n",
    "\n",
    "def cross_entropy(Y_one_hot, Y_proba):\n",
    "    loss = -np.sum(np.multiply(Y_one_hot,np.log(Y_proba)))/Y_one_hot.shape[0]\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_l2(Y_one_hot, Y_proba, Theta_1, Theta_2, lambd):\n",
    "    loss = cross_entropy(Y_one_hot, Y_proba) + 0.5*lambd*np.sum(np.square(Theta_1[1:]))+ 0.5*lambd*np.sum(np.square(Theta_2[1:]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeBasedDecay(initial_eta,decay,iteration):\n",
    "    eta = initial_eta * (1 / (1 + decay * iteration))\n",
    "    return eta\n",
    "def exponentialDecay(initial_eta,decay,iteration):\n",
    "    eta = initial_eta * np.exp(-decay*iteration)\n",
    "    return eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Forward Propogation\n",
    "def Forward_prop(Xi,W_1,W_2,activation='logistic',classifier='binary'):\n",
    "    \n",
    "    x_0 = np.ones((Xi.shape[0],1))\n",
    "    a1 = np.concatenate((x_0,Xi), axis=1)\n",
    "    z2 =np.dot(a1,W_1) \n",
    "    if activation =='logistic':\n",
    "        a2 = logistic(z2)\n",
    "                    \n",
    "    elif activation =='relu':\n",
    "        a2 = relu(z2)\n",
    "                    \n",
    "    elif activation == 'tanh':\n",
    "        a2 = tanh(z2)\n",
    "    \n",
    "    x_0 = np.ones((a2.shape[0],1))\n",
    "    a2 = np.concatenate((x_0,a2), axis=1)              \n",
    "    \n",
    "    z3 =np.dot(a2,W_2)\n",
    "                \n",
    "    if classifier =='binary':\n",
    "        a3 = logistic(z3)\n",
    "               \n",
    "    elif classifier =='multi':\n",
    "        a3 = softmax(z3)\n",
    "        \n",
    "    return a1,z2,a2,z3,a3    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Propogation\n",
    "def Backward_prop(Xi,Yi,W_2,z2,z3,a3,activation='logistic',classifier='binary'):\n",
    "  \n",
    "    if classifier =='binary':\n",
    "        d_activ_z3 = d_logistic(z3)\n",
    "        \n",
    "        if Yi==1:\n",
    "            delta_3 = -(1/a3) * d_activ_z3\n",
    "            \n",
    "        elif Yi==0:\n",
    "            delta_3 = (1/(1-a3)) * d_activ_z3\n",
    "                        \n",
    "    elif classifier =='multi':\n",
    "        delta_3 = (a3 - Yi)\n",
    "                          \n",
    "    if activation =='logistic':\n",
    "        d_activ_z2 = d_logistic(z2)\n",
    "                    \n",
    "    elif activation =='relu':\n",
    "        d_activ_z2 = d_relu(z2)\n",
    "                    \n",
    "    elif activation == 'tanh':\n",
    "        d_activ_z2 = d_tanh(z2)\n",
    "                    \n",
    "    delta_2 = np.dot(delta_3,(W_2[1:]).T)*d_activ_z2\n",
    "    return delta_2,delta_3            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Forward Propogation\n",
    "def predict_proba(Xi,W_1,W_2,activation='logistic',classifier='binary'):\n",
    "    \n",
    "    x_0 = np.ones((Xi.shape[0],1))\n",
    "    a1 = np.concatenate((x_0,Xi), axis=1)\n",
    "    z2 =np.dot(a1,W_1) \n",
    "    if activation =='logistic':\n",
    "        a2 = logistic(z2)\n",
    "                    \n",
    "    elif activation =='relu':\n",
    "        a2 = relu(z2)\n",
    "                    \n",
    "    elif activation == 'tanh':\n",
    "        a2 = tanh(z2)\n",
    "    \n",
    "    x_0 = np.ones((a2.shape[0],1))\n",
    "    a2 = np.concatenate((x_0,a2), axis=1)              \n",
    "    \n",
    "    z3 =np.dot(a2,W_2)\n",
    "                \n",
    "    if classifier =='binary':\n",
    "        a3 = logistic(z3)\n",
    "               \n",
    "    elif classifier =='multi':\n",
    "        a3 = softmax(z3)\n",
    "        \n",
    "    return a3    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function for predicting the labels based on the predictd probabilities\n",
    "def predict_labels(y_pred_proba,classifier='binary'):\n",
    "    \n",
    "    if classifier=='binary':\n",
    "        y_predicted = np.array(list(map(lambda x: 1 if x>0.5 else 0, y_pred_proba)))\n",
    "        \n",
    "    elif classifier=='multi':\n",
    "        y_predicted = np.argmax(y_pred_proba, axis=1)\n",
    "        \n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates the accuracy between the true and predicted labels\n",
    "Input: \n",
    "    true: array_like type vector of true labels\n",
    "    pred: array_like type vector of predicted labels\n",
    "Output:\n",
    "    accuracy: accuracy expressed in decimal\n",
    "\"\"\"\n",
    "def accuracy_calc(true, pred):\n",
    "    accuracy= np.mean(true == pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier():\n",
    "    def __init__(self, hidden_layer_neurons=2, activation= 'logistic', regularizer=None, \n",
    "                 alpha=0.0001, learning_rate='constant', learning_rate_scheduler='timeBasedDecay',learning_rate_init=0.001, \n",
    "                 tol = 0.0001, early_stopping=False, n_iter_no_change=10, momentum=False, beta=0.9, lambda_plateau=0.5, **kwargs):\n",
    "        self.hidden_layer_neurons = hidden_layer_neurons\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.W_1 = None\n",
    "        self.W_2= None\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_plateau = lambda_plateau #lambda is value between 0 and 1 to reduce eta during plateau\n",
    "        self.learning_rate_scheduler = learning_rate_scheduler\n",
    "        self.loss_train = []\n",
    "        self.loss_validation = []\n",
    "        self.accuracy_train = []\n",
    "        self.accuracy_validation = []\n",
    "        self.early_stopping = early_stopping\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.momentum = momentum\n",
    "        self.classifier = []\n",
    "        \n",
    "        if \"Decay\" in kwargs:\n",
    "            self.decay = kwargs[\"Decay\"]\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, Y, validation_fraction=0.1, max_iter=1000, verbose=False,**kwargs):\n",
    "        \n",
    "        unique_labels = np.unique(Y)\n",
    "        if len(unique_labels)>2:\n",
    "            classifier = 'multi'\n",
    "            self.classifier = classifier\n",
    "            Y = one_hot_labels(Y)\n",
    "            output_layer_neurons = len(unique_labels)\n",
    "        else:\n",
    "            classifier = 'binary'\n",
    "            self.classifier = classifier\n",
    "            output_layer_neurons = 1\n",
    "\n",
    "        #Extract number of input layer neurons (excluding bias)    \n",
    "        input_layer_neurons = X.shape[1]\n",
    "        \n",
    "        valid_samples = math.floor(X.shape[0]*validation_fraction)\n",
    "        validation_X = X[:valid_samples]\n",
    "        X = X[valid_samples:]\n",
    "        validation_Y = Y[:valid_samples]\n",
    "        Y = Y[valid_samples:]\n",
    "        \n",
    "        # Set number of training data\n",
    "        n = X.shape[0]        \n",
    "        \n",
    "        #Weights Initialization\n",
    "        self.W_1 = initializeWeights(input_layer_neurons+1, self.hidden_layer_neurons) \n",
    "        self.W_2 = initializeWeights(self.hidden_layer_neurons+1, output_layer_neurons)\n",
    "        \n",
    "        iteration=0\n",
    "        \n",
    "        for epoch in range(max_iter):\n",
    "            loss = []\n",
    "            accu = []\n",
    "            for i in range(n):\n",
    "                iteration = iteration +1\n",
    "                random_index = np.random.randint(n)\n",
    "                Yi=Y[random_index:random_index+1]\n",
    "                Xi=X[random_index:random_index+1]\n",
    "              \n",
    "                if self.learning_rate == 'constant':\n",
    "                    eta = self.learning_rate_init\n",
    "                    \n",
    "                if self.learning_rate == 'adaptive':\n",
    "                    if self.learning_rate_scheduler == 'timeBasedDecay':\n",
    "                        eta=timeBasedDecay(self.learning_rate_init,self.decay,iteration)\n",
    "                    elif self.learning_rate_scheduler == 'exponentialDecay':\n",
    "                        eta=exponentialDecay(self.learning_rate_init,self.decay,iteration)\n",
    "                    elif self.learning_rate_scheduler == 'reduceLearningRateOnPlateau':\n",
    "                        if epoch > self.n_iter_no_change:\n",
    "                            count = 0\n",
    "                            temp = self.accuracy_validation[-self.n_iter_no_change:]\n",
    "                            for j in range(len(temp)-1):\n",
    "                                if (temp[j+1]-temp[j]) < self.tol:\n",
    "                                    count= count+1\n",
    "                            if count == self.n_iter_no_change-1:        \n",
    "                                eta = self.learning_rate_init*self.lambda_plateau\n",
    "                                    \n",
    "        \n",
    "                a1,z2,a2,z3,a3 = Forward_prop(Xi,self.W_1,self.W_2,activation=self.activation,classifier=classifier)\n",
    "                \n",
    "                delta_2,delta_3 = Backward_prop(Xi,Yi,self.W_2,z2,z3,a3,activation=self.activation,classifier=classifier)\n",
    "               \n",
    "                if classifier =='binary':\n",
    "                    y_pred=predict_labels(a3,classifier=classifier)\n",
    "                    acc=accuracy_calc(Yi,y_pred)\n",
    "                # Compute the loss using Cross Entropy\n",
    "                    if self.regularizer=='l2':\n",
    "                        L = binary_cross_entropy_l2(Yi,a3,self.W_1,self.W_2,self.alpha) \n",
    "                    else:\n",
    "                        L = binary_cross_entropy(Yi,a3) \n",
    "                        \n",
    "                elif classifier =='multi':\n",
    "                    y_pred=predict_labels(a3,classifier=classifier)\n",
    "                    y_true=np.argmax(Yi, axis=1)\n",
    "                    acc=accuracy_calc(y_true,y_pred)\n",
    "                    if self.regularizer=='l2':\n",
    "                        L = cross_entropy_l2(Yi,a3,self.W_1,self.W_2,self.alpha) \n",
    "                    else:\n",
    "                        L = cross_entropy(Yi,a3) \n",
    "                # Store the training loss in a list\n",
    "                loss.append(L)\n",
    "                accu.append(acc)\n",
    "                \n",
    "                # Gradient Computation and Weight Updates\n",
    "                if self.regularizer == 'l2':\n",
    "                    regularized_term_1 = self.alpha*self.W_1\n",
    "                    regularized_term_1[0] = 0  # Exclude the bias term\n",
    "                    regularized_term_2 = self.alpha*self.W_2\n",
    "                    regularized_term_2[0] = 0  # Exclude the bias term\n",
    "                    grad_L_for_W_2 = np.dot(a2.T,delta_3) + regularized_term_2\n",
    "                    grad_L_for_W_1 = np.dot(a1.T,delta_2) + regularized_term_1\n",
    "                else:\n",
    "                    grad_L_for_W_2 = np.dot(a2.T,delta_3)\n",
    "                    grad_L_for_W_1 = np.dot(a1.T,delta_2)\n",
    "                \n",
    "                if self.momentum is True:\n",
    "                    if epoch == 0:\n",
    "                        m_t1_1 = np.zeros((grad_L_for_W_1.shape[0],grad_L_for_W_1.shape[1]))\n",
    "                        m_t2_1 = np.zeros((grad_L_for_W_2.shape[0],grad_L_for_W_2.shape[1]))\n",
    "                    \n",
    "                    m_1 = self.beta*m_t1_1 - eta*grad_L_for_W_1\n",
    "                    m_2 = self.beta*m_t2_1 - eta*grad_L_for_W_2\n",
    "                    \n",
    "                    self.W_2 = self.W_2 + m_2 \n",
    "                    self.W_1 = self.W_1 + m_1\n",
    "                    \n",
    "                    m_t1_1 = m_1\n",
    "                    m_t2_1 = m_2\n",
    "                \n",
    "                else:\n",
    "                    self.W_2 = self.W_2 - eta* grad_L_for_W_2 \n",
    "                    self.W_1 = self.W_1 - eta* grad_L_for_W_1\n",
    "                \n",
    "            self.loss_train.append(np.mean(loss))\n",
    "            self.accuracy_train.append(np.mean(accu)) \n",
    "            \n",
    "            \n",
    "            \n",
    "            if classifier =='binary':\n",
    "                y_pred_proba = predict_proba(validation_X,self.W_1,self.W_2,activation=self.activation,classifier=classifier)\n",
    "                y_pred = predict_labels(y_pred_proba,classifier=classifier)\n",
    "                acc = accuracy_calc(validation_Y,y_pred)\n",
    "                if self.regularizer=='l2':\n",
    "                    L = binary_cross_entropy_l2(validation_Y,y_pred_proba,self.W_1,self.W_2,self.alpha) \n",
    "                else:\n",
    "                    L = binary_cross_entropy(validation_Y,y_pred_proba) \n",
    "                            \n",
    "            elif classifier =='multi':\n",
    "                y_pred_proba = predict_proba(validation_X,self.W_1,self.W_2,activation=self.activation,classifier=classifier)\n",
    "                y_pred = predict_labels(y_pred_proba,classifier=classifier)\n",
    "                y_true=np.argmax(validation_Y, axis=1)\n",
    "                acc = accuracy_calc(y_true,y_pred)\n",
    "                if self.regularizer=='l2':\n",
    "                    L = cross_entropy_l2(validation_Y,y_pred_proba,self.W_1,self.W_2,self.alpha) \n",
    "                else:\n",
    "                    L = cross_entropy(validation_Y,y_pred_proba) \n",
    "            \n",
    "            self.loss_validation.append(L)\n",
    "            self.accuracy_validation.append(acc) \n",
    "            if verbose is True:\n",
    "                print(\"\\nEpoch %d of %d: Train Loss = %f | Val Loss = %f | Train Acc = %f | Val Acc = %f | Eta = %f\" %(epoch,max_iter,self.loss_train[epoch],self.loss_validation[epoch],self.accuracy_train[epoch],self.accuracy_validation[epoch],eta))\n",
    "\n",
    "            if self.early_stopping is True:\n",
    "                if epoch > self.n_iter_no_change:\n",
    "                    temp = self.accuracy_validation[-self.n_iter_no_change:]\n",
    "                    count=0\n",
    "                    for j in range(len(temp)-1):\n",
    "                        if (temp[j+1]-temp[j]) < self.tol:\n",
    "                            count= count+1\n",
    "                    if count==self.n_iter_no_change-1:\n",
    "                        print(\"\\nEarly Stopping because the validation accuracy change between two consecutive epochs is less than %f over the last %d iterations\" %(self.tol,self.n_iter_no_change))\n",
    "                        break\n",
    "        \n",
    "            \n",
    "    \n",
    "    def  predict(self, X):\n",
    "        y_pred_proba = predict_proba(X,self.W_1,self.W_2,activation=self.activation,classifier=self.classifier)\n",
    "        y_pred = predict_labels(y_pred_proba,classifier=self.classifier)\n",
    "        return y_pred\n",
    "    \n",
    "    def plotLearningCurve(self):\n",
    "        # Plotting Learning Curve            \n",
    "        epochs_xaxis=np.linspace(1.0,len(self.loss_train),num=len(self.loss_train))\n",
    "        plt.figure(figsize=(12, 9), dpi=80)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_xaxis,self.loss_train,\"r--\", alpha=1.0, linewidth=3.0, label = \"Training\")\n",
    "        plt.plot(epochs_xaxis,self.loss_validation,\"b-\", alpha=1.0, linewidth=3.0, label = \"Validation\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Average Loss\")\n",
    "        plt.title(\"Loss VS Epochs\")\n",
    "        plt.legend()\n",
    "        \n",
    "        epochs_xaxis=np.linspace(1.0,len(self.loss_train),num=len(self.loss_train))\n",
    "        plt.figure(figsize=(12, 9), dpi=80)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_xaxis,self.accuracy_train,\"r--\", alpha=1.0, linewidth=3.0, label = \"Training\")\n",
    "        plt.plot(epochs_xaxis,self.accuracy_validation,\"b-\", alpha=1.0, linewidth=3.0, label = \"Validation\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Average Accuracy\")\n",
    "        plt.title(\"Accuracy VS Epochs\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=MLPClassifier(hidden_layer_neurons=25, activation= 'logistic', regularizer='l2', \n",
    "                 alpha=0.0001, learning_rate='constant', learning_rate_scheduler='timeBasedDecay',learning_rate_init=0.001, \n",
    "                 tol = 0.0001, early_stopping=True, n_iter_no_change=10, momentum=True, beta=0.9, lambda_plateau=0.5, Decay = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 of 1000: Train Loss = 2.303230 | Val Loss = 2.302416 | Train Acc = 0.107419 | Val Acc = 0.139860 | Eta = 0.001000\n",
      "\n",
      "Epoch 1 of 1000: Train Loss = 2.299404 | Val Loss = 2.298592 | Train Acc = 0.104328 | Val Acc = 0.174825 | Eta = 0.001000\n",
      "\n",
      "Epoch 2 of 1000: Train Loss = 2.028282 | Val Loss = 1.702209 | Train Acc = 0.400309 | Val Acc = 0.594406 | Eta = 0.001000\n",
      "\n",
      "Epoch 3 of 1000: Train Loss = 1.275726 | Val Loss = 1.095049 | Train Acc = 0.727202 | Val Acc = 0.741259 | Eta = 0.001000\n",
      "\n",
      "Epoch 4 of 1000: Train Loss = 0.845657 | Val Loss = 0.767223 | Train Acc = 0.826893 | Val Acc = 0.811189 | Eta = 0.001000\n",
      "\n",
      "Epoch 5 of 1000: Train Loss = 0.605404 | Val Loss = 0.614428 | Train Acc = 0.883308 | Val Acc = 0.853147 | Eta = 0.001000\n",
      "\n",
      "Epoch 6 of 1000: Train Loss = 0.441531 | Val Loss = 0.512607 | Train Acc = 0.910355 | Val Acc = 0.867133 | Eta = 0.001000\n",
      "\n",
      "Epoch 7 of 1000: Train Loss = 0.364807 | Val Loss = 0.410084 | Train Acc = 0.943586 | Val Acc = 0.909091 | Eta = 0.001000\n",
      "\n",
      "Epoch 8 of 1000: Train Loss = 0.280668 | Val Loss = 0.338182 | Train Acc = 0.960587 | Val Acc = 0.923077 | Eta = 0.001000\n",
      "\n",
      "Epoch 9 of 1000: Train Loss = 0.244778 | Val Loss = 0.302490 | Train Acc = 0.958269 | Val Acc = 0.937063 | Eta = 0.001000\n",
      "\n",
      "Epoch 10 of 1000: Train Loss = 0.212384 | Val Loss = 0.280302 | Train Acc = 0.970634 | Val Acc = 0.930070 | Eta = 0.001000\n",
      "\n",
      "Epoch 11 of 1000: Train Loss = 0.180972 | Val Loss = 0.257681 | Train Acc = 0.976043 | Val Acc = 0.937063 | Eta = 0.001000\n",
      "\n",
      "Epoch 12 of 1000: Train Loss = 0.173071 | Val Loss = 0.235810 | Train Acc = 0.979907 | Val Acc = 0.944056 | Eta = 0.001000\n",
      "\n",
      "Epoch 13 of 1000: Train Loss = 0.155547 | Val Loss = 0.219836 | Train Acc = 0.978362 | Val Acc = 0.944056 | Eta = 0.001000\n",
      "\n",
      "Epoch 14 of 1000: Train Loss = 0.135319 | Val Loss = 0.215397 | Train Acc = 0.984544 | Val Acc = 0.944056 | Eta = 0.001000\n",
      "\n",
      "Epoch 15 of 1000: Train Loss = 0.134790 | Val Loss = 0.199607 | Train Acc = 0.978362 | Val Acc = 0.951049 | Eta = 0.001000\n",
      "\n",
      "Epoch 16 of 1000: Train Loss = 0.134495 | Val Loss = 0.194914 | Train Acc = 0.976816 | Val Acc = 0.944056 | Eta = 0.001000\n",
      "\n",
      "Epoch 17 of 1000: Train Loss = 0.121105 | Val Loss = 0.186214 | Train Acc = 0.976043 | Val Acc = 0.944056 | Eta = 0.001000\n",
      "\n",
      "Epoch 18 of 1000: Train Loss = 0.113445 | Val Loss = 0.179027 | Train Acc = 0.986090 | Val Acc = 0.951049 | Eta = 0.001000\n",
      "\n",
      "Epoch 19 of 1000: Train Loss = 0.116952 | Val Loss = 0.173144 | Train Acc = 0.984544 | Val Acc = 0.951049 | Eta = 0.001000\n",
      "\n",
      "Epoch 20 of 1000: Train Loss = 0.090756 | Val Loss = 0.166254 | Train Acc = 0.993045 | Val Acc = 0.958042 | Eta = 0.001000\n",
      "\n",
      "Epoch 21 of 1000: Train Loss = 0.091143 | Val Loss = 0.164742 | Train Acc = 0.992272 | Val Acc = 0.951049 | Eta = 0.001000\n",
      "\n",
      "Epoch 22 of 1000: Train Loss = 0.089824 | Val Loss = 0.161996 | Train Acc = 0.993045 | Val Acc = 0.951049 | Eta = 0.001000\n",
      "\n",
      "Epoch 23 of 1000: Train Loss = 0.087800 | Val Loss = 0.159935 | Train Acc = 0.992272 | Val Acc = 0.958042 | Eta = 0.001000\n",
      "\n",
      "Epoch 24 of 1000: Train Loss = 0.110498 | Val Loss = 0.154358 | Train Acc = 0.979134 | Val Acc = 0.958042 | Eta = 0.001000\n",
      "\n",
      "Epoch 25 of 1000: Train Loss = 0.083103 | Val Loss = 0.145235 | Train Acc = 0.992272 | Val Acc = 0.951049 | Eta = 0.001000\n",
      "\n",
      "Epoch 26 of 1000: Train Loss = 0.076992 | Val Loss = 0.143549 | Train Acc = 0.991499 | Val Acc = 0.951049 | Eta = 0.001000\n",
      "\n",
      "Epoch 27 of 1000: Train Loss = 0.071244 | Val Loss = 0.144199 | Train Acc = 0.994590 | Val Acc = 0.958042 | Eta = 0.001000\n",
      "\n",
      "Epoch 28 of 1000: Train Loss = 0.082199 | Val Loss = 0.140070 | Train Acc = 0.988408 | Val Acc = 0.958042 | Eta = 0.001000\n",
      "\n",
      "Epoch 29 of 1000: Train Loss = 0.081352 | Val Loss = 0.137780 | Train Acc = 0.988408 | Val Acc = 0.958042 | Eta = 0.001000\n",
      "\n",
      "Epoch 30 of 1000: Train Loss = 0.076673 | Val Loss = 0.137680 | Train Acc = 0.993045 | Val Acc = 0.951049 | Eta = 0.001000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d05edb13e6b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-5f5d7b7b1c34>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, Y, validation_fraction, max_iter, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0macc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccuracy_calc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                         \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy_l2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                         \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mYi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-82af48a99396>\u001b[0m in \u001b[0;36mcross_entropy_l2\u001b[1;34m(Y_one_hot, Y_proba, Theta_1, Theta_2, lambd)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcross_entropy_l2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTheta_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTheta_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_one_hot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_proba\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlambd\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlambd\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2228\u001b[1;33m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[0;32m   2229\u001b[0m                           initial=initial, where=where)\n\u001b[0;32m   2230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m.fit(X_train, y_train, validation_fraction=0.1, max_iter=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plotLearningCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=m.predict(X_test)\n",
    "accuracy_calc(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X_binary = digits.data\n",
    "y_binary = digits.target\n",
    "y_binary = np.array(list(map(lambda x: 1 if x==5 else 0, y_binary)))\n",
    "X_binary,y_binary = shuffle(X_binary,y_binary)\n",
    "X_train_binary,X_test_binary,y_train_binary,y_test_binary = train_test_split(X_binary,y_binary,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_binary)\n",
    "X_train_binary = scaler.transform(X_train_binary)\n",
    "X_test_binary = scaler.transform(X_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "exists = 1 in y_train_binary\n",
    "print(exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=MLPClassifier(hidden_layer_neurons=25, activation= 'logistic', regularizer='l2', \n",
    "                 alpha=0.001, learning_rate='adaptive', learning_rate_scheduler='timeBasedDecay',learning_rate_init=0.01, \n",
    "                 tol = 0.0001, early_stopping=True, n_iter_no_change=10, momentum=True, beta=0.9, lambda_plateau=0.5, Decay =0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 of 1000: Train Loss = 0.331317 | Val Loss = 0.240799 | Train Acc = 0.893354 | Val Acc = 0.923077 | Eta = 0.004359\n",
      "\n",
      "Epoch 1 of 1000: Train Loss = 0.187408 | Val Loss = 0.091980 | Train Acc = 0.903400 | Val Acc = 0.965035 | Eta = 0.002787\n",
      "\n",
      "Epoch 2 of 1000: Train Loss = 0.098487 | Val Loss = 0.062563 | Train Acc = 0.977589 | Val Acc = 1.000000 | Eta = 0.002048\n",
      "\n",
      "Epoch 3 of 1000: Train Loss = 0.079106 | Val Loss = 0.057620 | Train Acc = 0.985317 | Val Acc = 1.000000 | Eta = 0.001619\n",
      "\n",
      "Epoch 4 of 1000: Train Loss = 0.068307 | Val Loss = 0.051934 | Train Acc = 0.991499 | Val Acc = 1.000000 | Eta = 0.001339\n",
      "\n",
      "Epoch 5 of 1000: Train Loss = 0.059007 | Val Loss = 0.049896 | Train Acc = 0.993818 | Val Acc = 1.000000 | Eta = 0.001141\n",
      "\n",
      "Epoch 6 of 1000: Train Loss = 0.054195 | Val Loss = 0.047408 | Train Acc = 0.994590 | Val Acc = 1.000000 | Eta = 0.000994\n",
      "\n",
      "Epoch 7 of 1000: Train Loss = 0.050819 | Val Loss = 0.045512 | Train Acc = 0.997682 | Val Acc = 1.000000 | Eta = 0.000881\n",
      "\n",
      "Epoch 8 of 1000: Train Loss = 0.055853 | Val Loss = 0.045109 | Train Acc = 0.993045 | Val Acc = 1.000000 | Eta = 0.000791\n",
      "\n",
      "Epoch 9 of 1000: Train Loss = 0.051224 | Val Loss = 0.045010 | Train Acc = 0.996909 | Val Acc = 1.000000 | Eta = 0.000717\n",
      "\n",
      "Epoch 10 of 1000: Train Loss = 0.049430 | Val Loss = 0.043738 | Train Acc = 0.994590 | Val Acc = 1.000000 | Eta = 0.000656\n",
      "\n",
      "Epoch 11 of 1000: Train Loss = 0.046668 | Val Loss = 0.042946 | Train Acc = 0.996136 | Val Acc = 1.000000 | Eta = 0.000605\n",
      "\n",
      "Early Stopping because the validation accuracy change between two consecutive epochs is less than 0.000100 over the last 10 iterations\n"
     ]
    }
   ],
   "source": [
    "m1.fit(X_train_binary, y_train_binary, validation_fraction=0.1, max_iter=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 960x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 960x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m1.plotLearningCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861111111111112"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=m1.predict(X_test_binary)\n",
    "accuracy_calc(y_test_binary,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = np.count_nonzero(y_pred == 1)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = np.count_nonzero(y_test_binary== 1)\n",
    "count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(m1.loss_validation).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Hyperparameter tuning based on certain fixed-values hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Report performance on model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 (Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implement a Multi-Layer Perceptron regressor model (a MLP Regressor class) with a single hidden layer. The model implements the backpropagation algorithm. To optimize the process of updating the weight matrices, it uses the Stochastic Gradient Descent (SGD) algorithm with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/winequality-red.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr = np.array(df.drop(columns=['quality'],axis=1))\n",
    "yr = np.array(df['quality'])\n",
    "yr=np.reshape(yr,(len(yr),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr_train,Xr_test,yr_train,yr_test = train_test_split(Xr,yr,test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xr_train)\n",
    "Xr_train = scaler.transform(Xr_train)\n",
    "Xr_test = scaler.transform(Xr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates the mean squared error (mse) between the true and predicted labels\n",
    "Input: \n",
    "    true: array_like type vector of true labels\n",
    "    pred: array_like type vector of predicted labels\n",
    "Output:\n",
    "    mean squared error: accuracy expressed in decimal\n",
    "\"\"\"\n",
    "def MSE(true,pred):\n",
    "    diff = true - pred\n",
    "    error = 0.5*np.sum(np.square(diff))/true.shape[0]\n",
    "    return error\n",
    "\n",
    "def MSE_l2(true,pred,Theta_1, Theta_2, lambd):\n",
    "    diff = true - pred\n",
    "    error = MSE(true,pred) +(0.5*lambd*np.sum(np.square(Theta_1[1:]))+ 0.5*lambd*np.sum(np.square(Theta_2[1:])))/true.shape[0]\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Forward Propogation\n",
    "def Forward_prop_reg(Xi,W_1,W_2,activation='logistic'):\n",
    "    \n",
    "    x_0 = np.ones((Xi.shape[0],1))\n",
    "    a1 = np.concatenate((x_0,Xi), axis=1)\n",
    "    z2 =np.dot(a1,W_1) \n",
    "    if activation =='logistic':\n",
    "        a2 = logistic(z2)\n",
    "                    \n",
    "    elif activation =='relu':\n",
    "        a2 = relu(z2)\n",
    "                    \n",
    "    elif activation == 'tanh':\n",
    "        a2 = tanh(z2)\n",
    "    \n",
    "    x_0 = np.ones((a2.shape[0],1))\n",
    "    a2 = np.concatenate((x_0,a2), axis=1)              \n",
    "    \n",
    "    z3 =np.dot(a2,W_2)\n",
    "    a3=z3\n",
    "    \n",
    "    return a1,z2,a2,z3,a3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Propogation\n",
    "def Backward_prop_reg(Xi,Yi,W_2,z2,z3,a3,activation='logistic'):\n",
    "  \n",
    "    delta_3 = (a3 - Yi)\n",
    "                          \n",
    "    if activation =='logistic':\n",
    "        d_activ_z2 = d_logistic(z2)\n",
    "                    \n",
    "    elif activation =='relu':\n",
    "        d_activ_z2 = d_relu(z2)\n",
    "                    \n",
    "    elif activation == 'tanh':\n",
    "        d_activ_z2 = d_tanh(z2)\n",
    "                    \n",
    "    delta_2 = np.dot(delta_3,(W_2[1:]).T)*d_activ_z2\n",
    "    return delta_2,delta_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Forward Propogation\n",
    "def predict_reg(Xi,W_1,W_2,activation='logistic'):\n",
    "    \n",
    "    x_0 = np.ones((Xi.shape[0],1))\n",
    "    a1 = np.concatenate((x_0,Xi), axis=1)\n",
    "    z2 =np.dot(a1,W_1) \n",
    "    if activation =='logistic':\n",
    "        a2 = logistic(z2)\n",
    "                    \n",
    "    elif activation =='relu':\n",
    "        a2 = relu(z2)\n",
    "                    \n",
    "    elif activation == 'tanh':\n",
    "        a2 = tanh(z2)\n",
    "    \n",
    "    x_0 = np.ones((a2.shape[0],1))\n",
    "    a2 = np.concatenate((x_0,a2), axis=1)              \n",
    "    \n",
    "    z3 =np.dot(a2,W_2)\n",
    "    \n",
    "    a3 = z3\n",
    "      \n",
    "    return a3    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor():\n",
    "    def __init__(self, hidden_layer_neurons=2, activation= 'logistic', regularizer=None, \n",
    "                 alpha=0.0001, learning_rate='constant', learning_rate_scheduler='timeBasedDecay',learning_rate_init=0.001, \n",
    "                 tol = 0.0001, early_stopping=False, n_iter_no_change=10, momentum=False, beta=0.9, lambda_plateau=0.5, **kwargs):\n",
    "        self.hidden_layer_neurons = hidden_layer_neurons\n",
    "        self.activation = activation\n",
    "        self.regularizer = regularizer\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.W_1 = None\n",
    "        self.W_2= None\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_plateau = lambda_plateau #lambda is value between 0 and 1 to reduce eta during plateau\n",
    "        self.learning_rate_scheduler = learning_rate_scheduler\n",
    "        self.loss_train = []\n",
    "        self.loss_validation = []\n",
    "        self.early_stopping = early_stopping\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.momentum = momentum\n",
    "        if \"Decay\" in kwargs:\n",
    "            self.decay = kwargs[\"Decay\"]\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, Y, validation_fraction=0.1, max_iter=1000, verbose=False,**kwargs):\n",
    "        \n",
    "\n",
    "        output_layer_neurons = 1\n",
    "\n",
    "        #Extract number of input layer neurons (excluding bias)    \n",
    "        input_layer_neurons = X.shape[1]\n",
    "        valid_samples = math.floor(X.shape[0]*validation_fraction)\n",
    "        validation_X = X[:valid_samples]\n",
    "        X = X[valid_samples:]\n",
    "        validation_Y = Y[:valid_samples]\n",
    "        Y = Y[valid_samples:]\n",
    "        \n",
    "        # Set number of training data\n",
    "        n = X.shape[0]        \n",
    "        \n",
    "        #Weights Initialization\n",
    "        self.W_1 = initializeWeights(input_layer_neurons+1, self.hidden_layer_neurons) \n",
    "        self.W_2 = initializeWeights(self.hidden_layer_neurons+1, output_layer_neurons)\n",
    "        \n",
    "        iteration=0\n",
    "        \n",
    "        for epoch in range(max_iter):\n",
    "            loss = []\n",
    "            accu = []\n",
    "            for i in range(n):\n",
    "                iteration = iteration +1\n",
    "                random_index = np.random.randint(n)\n",
    "                Yi=Y[random_index:random_index+1]\n",
    "                Xi=X[random_index:random_index+1]\n",
    "                \n",
    "                if self.learning_rate == 'constant':\n",
    "                    eta = self.learning_rate_init\n",
    "                    \n",
    "                if self.learning_rate == 'adaptive':\n",
    "                    if self.learning_rate_scheduler == 'timeBasedDecay':\n",
    "                        eta=timeBasedDecay(self.learning_rate_init,self.decay,iteration)\n",
    "                    elif self.learning_rate_scheduler == 'exponentialDecay':\n",
    "                        eta=exponentialDecay(self.learning_rate_init,self.decay,iteration)\n",
    "                    elif self.learning_rate_scheduler == 'reduceLearningRateOnPlateau':\n",
    "                        if epoch > self.n_iter_no_change:\n",
    "                            count = self.n_iter_no_change\n",
    "                            temp = self.loss_validation[-self.n_iter_no_change:]\n",
    "                            for j in range(len(temp)-1):\n",
    "                                if (temp[j]-temp[j+1]) >= self.tol:\n",
    "                                    count= count-1\n",
    "                            if count ==self.n_iter_no_change:        \n",
    "                                eta = self.learning_rate_init*self.lambda_plateau\n",
    "                                    \n",
    "        \n",
    "                a1,z2,a2,z3,a3 = Forward_prop_reg(Xi,self.W_1,self.W_2,activation=self.activation)\n",
    "                \n",
    "                delta_2,delta_3 = Backward_prop_reg(Xi,Yi,self.W_2,z2,z3,a3,activation=self.activation)\n",
    "            \n",
    "                if self.regularizer=='l2':\n",
    "                    L = MSE_l2(Yi,a3,self.W_1,self.W_2,self.alpha) \n",
    "                else:\n",
    "                    L = MSE(Yi,a3) \n",
    "                       \n",
    "                # Store the training loss in a list\n",
    "                loss.append(L)\n",
    "                \n",
    "                # Gradient Computation and Weight Updates\n",
    "                if self.regularizer == 'l2':\n",
    "                    regularized_term_1 = self.alpha*self.W_1\n",
    "                    regularized_term_1[0] = 0  # Exclude the bias term\n",
    "                    regularized_term_2 = self.alpha*self.W_2\n",
    "                    regularized_term_2[0] = 0  # Exclude the bias term\n",
    "                    grad_L_for_W_2 = np.dot(a2.T,delta_3) + regularized_term_2\n",
    "                    grad_L_for_W_1 = np.dot(a1.T,delta_2) + regularized_term_1\n",
    "                else:\n",
    "                    grad_L_for_W_2 = np.dot(a2.T,delta_3)\n",
    "                    grad_L_for_W_1 = np.dot(a1.T,delta_2)\n",
    "                \n",
    "                if self.momentum is True:\n",
    "                    if epoch == 0:\n",
    "                        m_t1_1 = np.zeros((grad_L_for_W_1.shape[0],grad_L_for_W_1.shape[1]))\n",
    "                        m_t2_1 = np.zeros((grad_L_for_W_2.shape[0],grad_L_for_W_2.shape[1]))\n",
    "                    \n",
    "                    m_1 = self.beta*m_t1_1 - eta*grad_L_for_W_1\n",
    "                    m_2 = self.beta*m_t2_1 - eta*grad_L_for_W_2\n",
    "                    \n",
    "                    self.W_2 = self.W_2 + m_2 \n",
    "                    self.W_1 = self.W_1 + m_1\n",
    "                    \n",
    "                    m_t1_1 = m_1\n",
    "                    m_t2_1 = m_2\n",
    "                \n",
    "                else:\n",
    "                    self.W_2 = self.W_2 - eta* grad_L_for_W_2 \n",
    "                    self.W_1 = self.W_1 - eta* grad_L_for_W_1\n",
    "              \n",
    "            self.loss_train.append(np.mean(loss))\n",
    "            \n",
    "            y_pred = predict_reg(validation_X,self.W_1,self.W_2,activation=self.activation)\n",
    "    \n",
    "            if self.regularizer=='l2':\n",
    "                L = MSE_l2(validation_Y,y_pred,self.W_1,self.W_2,self.alpha) \n",
    "            else:\n",
    "                L = MSE(validation_Y,y_pred)  \n",
    "        \n",
    "            self.loss_validation.append(L)\n",
    "            if verbose is True:\n",
    "                print(\"\\nEpoch %d of %d: Train Loss = %f | Val Loss = %f | Eta = %f\" %(epoch,max_iter,self.loss_train[epoch],self.loss_validation[epoch],eta))\n",
    "\n",
    "            if self.early_stopping is True:\n",
    "                if epoch > self.n_iter_no_change:\n",
    "                    temp = self.loss_validation[-self.n_iter_no_change:]\n",
    "                    count=self.n_iter_no_change\n",
    "                    for j in range(len(temp)-1):\n",
    "                        if (temp[j]-temp[j+1]) >= self.tol:\n",
    "                            count= count-1\n",
    "                    if count==self.n_iter_no_change:\n",
    "                        print(\"\\nEarly Stopping because the validation loss change between two consecutive epochs is less than %f over the last %d iterations\" %(self.tol,self.n_iter_no_change))\n",
    "                        break\n",
    "        \n",
    "            \n",
    "    \n",
    "    def  predict(self, X):\n",
    "        y_pred = predict_reg(X,self.W_1,self.W_2,activation=self.activation)\n",
    "        return y_pred\n",
    "    \n",
    "    def plotLearningCurve(self):\n",
    "        # Plotting Learning Curve            \n",
    "        epochs_xaxis=np.linspace(1.0,len(self.loss_train),num=len(self.loss_train))\n",
    "        plt.figure(figsize=(12, 9), dpi=80)\n",
    "        plt.plot(epochs_xaxis,self.loss_train,\"r--\", alpha=1.0, linewidth=3.0, label = \"Training\")\n",
    "        plt.plot(epochs_xaxis,self.loss_validation,\"b-\", alpha=1.0, linewidth=3.0, label = \"Validation\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Average Loss\")\n",
    "        plt.title(\"Loss VS Epochs\")\n",
    "        plt.legend()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Hyperparameter tuning based on certain fixed-values hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=MLPRegressor(hidden_layer_neurons=7, activation= 'logistic', regularizer='l2', \n",
    "                 alpha=0.001, learning_rate='adaptive', learning_rate_scheduler='timeBasedDecay',learning_rate_init=0.01, \n",
    "                 tol = 0.001, early_stopping=True, n_iter_no_change=20, momentum=True, beta=0.9, lambda_plateau=0.5, Decay =0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 of 1000: Train Loss = 0.486296 | Val Loss = 0.221502 | Eta = 0.004647\n",
      "\n",
      "Epoch 1 of 1000: Train Loss = 0.259219 | Val Loss = 0.222719 | Eta = 0.003027\n",
      "\n",
      "Epoch 2 of 1000: Train Loss = 0.228224 | Val Loss = 0.263455 | Eta = 0.002244\n",
      "\n",
      "Epoch 3 of 1000: Train Loss = 0.245069 | Val Loss = 0.201193 | Eta = 0.001783\n",
      "\n",
      "Epoch 4 of 1000: Train Loss = 0.230541 | Val Loss = 0.230749 | Eta = 0.001479\n",
      "\n",
      "Epoch 5 of 1000: Train Loss = 0.224443 | Val Loss = 0.205288 | Eta = 0.001264\n",
      "\n",
      "Epoch 6 of 1000: Train Loss = 0.208461 | Val Loss = 0.209129 | Eta = 0.001103\n",
      "\n",
      "Epoch 7 of 1000: Train Loss = 0.216990 | Val Loss = 0.226793 | Eta = 0.000979\n",
      "\n",
      "Epoch 8 of 1000: Train Loss = 0.212608 | Val Loss = 0.207815 | Eta = 0.000880\n",
      "\n",
      "Epoch 9 of 1000: Train Loss = 0.225643 | Val Loss = 0.195201 | Eta = 0.000799\n",
      "\n",
      "Epoch 10 of 1000: Train Loss = 0.221346 | Val Loss = 0.197090 | Eta = 0.000731\n",
      "\n",
      "Epoch 11 of 1000: Train Loss = 0.230647 | Val Loss = 0.198469 | Eta = 0.000675\n",
      "\n",
      "Epoch 12 of 1000: Train Loss = 0.214440 | Val Loss = 0.196090 | Eta = 0.000626\n",
      "\n",
      "Epoch 13 of 1000: Train Loss = 0.210041 | Val Loss = 0.192290 | Eta = 0.000584\n",
      "\n",
      "Epoch 14 of 1000: Train Loss = 0.210978 | Val Loss = 0.190059 | Eta = 0.000547\n",
      "\n",
      "Epoch 15 of 1000: Train Loss = 0.199392 | Val Loss = 0.195360 | Eta = 0.000515\n",
      "\n",
      "Epoch 16 of 1000: Train Loss = 0.201663 | Val Loss = 0.196836 | Eta = 0.000486\n",
      "\n",
      "Epoch 17 of 1000: Train Loss = 0.213134 | Val Loss = 0.198333 | Eta = 0.000460\n",
      "\n",
      "Epoch 18 of 1000: Train Loss = 0.213323 | Val Loss = 0.193647 | Eta = 0.000437\n",
      "\n",
      "Epoch 19 of 1000: Train Loss = 0.221823 | Val Loss = 0.191909 | Eta = 0.000416\n",
      "\n",
      "Epoch 20 of 1000: Train Loss = 0.185363 | Val Loss = 0.193928 | Eta = 0.000397\n",
      "\n",
      "Epoch 21 of 1000: Train Loss = 0.197643 | Val Loss = 0.188620 | Eta = 0.000380\n",
      "\n",
      "Epoch 22 of 1000: Train Loss = 0.201878 | Val Loss = 0.190238 | Eta = 0.000364\n",
      "\n",
      "Epoch 23 of 1000: Train Loss = 0.202019 | Val Loss = 0.186179 | Eta = 0.000349\n",
      "\n",
      "Epoch 24 of 1000: Train Loss = 0.212648 | Val Loss = 0.193558 | Eta = 0.000336\n",
      "\n",
      "Epoch 25 of 1000: Train Loss = 0.216003 | Val Loss = 0.188781 | Eta = 0.000323\n",
      "\n",
      "Epoch 26 of 1000: Train Loss = 0.199050 | Val Loss = 0.185938 | Eta = 0.000311\n",
      "\n",
      "Epoch 27 of 1000: Train Loss = 0.196366 | Val Loss = 0.184330 | Eta = 0.000301\n",
      "\n",
      "Epoch 28 of 1000: Train Loss = 0.201635 | Val Loss = 0.189918 | Eta = 0.000291\n",
      "\n",
      "Epoch 29 of 1000: Train Loss = 0.216153 | Val Loss = 0.186604 | Eta = 0.000281\n",
      "\n",
      "Epoch 30 of 1000: Train Loss = 0.211710 | Val Loss = 0.188972 | Eta = 0.000272\n",
      "\n",
      "Epoch 31 of 1000: Train Loss = 0.214625 | Val Loss = 0.186250 | Eta = 0.000264\n",
      "\n",
      "Epoch 32 of 1000: Train Loss = 0.205026 | Val Loss = 0.185158 | Eta = 0.000256\n",
      "\n",
      "Epoch 33 of 1000: Train Loss = 0.206583 | Val Loss = 0.182280 | Eta = 0.000249\n",
      "\n",
      "Epoch 34 of 1000: Train Loss = 0.198218 | Val Loss = 0.187578 | Eta = 0.000242\n",
      "\n",
      "Epoch 35 of 1000: Train Loss = 0.198480 | Val Loss = 0.186530 | Eta = 0.000235\n",
      "\n",
      "Epoch 36 of 1000: Train Loss = 0.201648 | Val Loss = 0.186097 | Eta = 0.000229\n",
      "\n",
      "Epoch 37 of 1000: Train Loss = 0.204186 | Val Loss = 0.184797 | Eta = 0.000223\n",
      "\n",
      "Epoch 38 of 1000: Train Loss = 0.227609 | Val Loss = 0.187720 | Eta = 0.000218\n",
      "\n",
      "Epoch 39 of 1000: Train Loss = 0.198425 | Val Loss = 0.186570 | Eta = 0.000212\n",
      "\n",
      "Epoch 40 of 1000: Train Loss = 0.202582 | Val Loss = 0.183872 | Eta = 0.000207\n",
      "\n",
      "Epoch 41 of 1000: Train Loss = 0.201649 | Val Loss = 0.186441 | Eta = 0.000202\n",
      "\n",
      "Epoch 42 of 1000: Train Loss = 0.202541 | Val Loss = 0.185668 | Eta = 0.000198\n",
      "\n",
      "Epoch 43 of 1000: Train Loss = 0.198753 | Val Loss = 0.185926 | Eta = 0.000193\n",
      "\n",
      "Epoch 44 of 1000: Train Loss = 0.209062 | Val Loss = 0.187566 | Eta = 0.000189\n",
      "\n",
      "Epoch 45 of 1000: Train Loss = 0.204767 | Val Loss = 0.186947 | Eta = 0.000185\n",
      "\n",
      "Epoch 46 of 1000: Train Loss = 0.208902 | Val Loss = 0.189466 | Eta = 0.000181\n",
      "\n",
      "Epoch 47 of 1000: Train Loss = 0.221597 | Val Loss = 0.185310 | Eta = 0.000178\n",
      "\n",
      "Epoch 48 of 1000: Train Loss = 0.207785 | Val Loss = 0.189900 | Eta = 0.000174\n",
      "\n",
      "Epoch 49 of 1000: Train Loss = 0.194669 | Val Loss = 0.186040 | Eta = 0.000171\n",
      "\n",
      "Epoch 50 of 1000: Train Loss = 0.200991 | Val Loss = 0.186298 | Eta = 0.000167\n",
      "\n",
      "Epoch 51 of 1000: Train Loss = 0.205450 | Val Loss = 0.185058 | Eta = 0.000164\n",
      "\n",
      "Epoch 52 of 1000: Train Loss = 0.199183 | Val Loss = 0.189212 | Eta = 0.000161\n",
      "\n",
      "Epoch 53 of 1000: Train Loss = 0.214225 | Val Loss = 0.183979 | Eta = 0.000158\n",
      "\n",
      "Epoch 54 of 1000: Train Loss = 0.211810 | Val Loss = 0.183586 | Eta = 0.000155\n",
      "\n",
      "Epoch 55 of 1000: Train Loss = 0.179175 | Val Loss = 0.184536 | Eta = 0.000153\n",
      "\n",
      "Epoch 56 of 1000: Train Loss = 0.214060 | Val Loss = 0.183458 | Eta = 0.000150\n",
      "\n",
      "Epoch 57 of 1000: Train Loss = 0.212406 | Val Loss = 0.185155 | Eta = 0.000147\n",
      "\n",
      "Epoch 58 of 1000: Train Loss = 0.198247 | Val Loss = 0.185460 | Eta = 0.000145\n",
      "\n",
      "Epoch 59 of 1000: Train Loss = 0.198570 | Val Loss = 0.186283 | Eta = 0.000143\n",
      "\n",
      "Epoch 60 of 1000: Train Loss = 0.197904 | Val Loss = 0.185824 | Eta = 0.000140\n",
      "\n",
      "Epoch 61 of 1000: Train Loss = 0.187003 | Val Loss = 0.183704 | Eta = 0.000138\n",
      "\n",
      "Epoch 62 of 1000: Train Loss = 0.206608 | Val Loss = 0.186941 | Eta = 0.000136\n",
      "\n",
      "Epoch 63 of 1000: Train Loss = 0.198460 | Val Loss = 0.186197 | Eta = 0.000134\n",
      "\n",
      "Epoch 64 of 1000: Train Loss = 0.197161 | Val Loss = 0.186684 | Eta = 0.000132\n",
      "\n",
      "Epoch 65 of 1000: Train Loss = 0.206644 | Val Loss = 0.187666 | Eta = 0.000130\n",
      "\n",
      "Epoch 66 of 1000: Train Loss = 0.216249 | Val Loss = 0.186847 | Eta = 0.000128\n",
      "\n",
      "Epoch 67 of 1000: Train Loss = 0.192757 | Val Loss = 0.184684 | Eta = 0.000126\n",
      "\n",
      "Epoch 68 of 1000: Train Loss = 0.191896 | Val Loss = 0.188447 | Eta = 0.000124\n",
      "\n",
      "Epoch 69 of 1000: Train Loss = 0.187005 | Val Loss = 0.184348 | Eta = 0.000122\n",
      "\n",
      "Epoch 70 of 1000: Train Loss = 0.185909 | Val Loss = 0.184540 | Eta = 0.000121\n",
      "\n",
      "Epoch 71 of 1000: Train Loss = 0.194300 | Val Loss = 0.186410 | Eta = 0.000119\n",
      "\n",
      "Epoch 72 of 1000: Train Loss = 0.214289 | Val Loss = 0.185338 | Eta = 0.000118\n",
      "\n",
      "Epoch 73 of 1000: Train Loss = 0.207823 | Val Loss = 0.186256 | Eta = 0.000116\n",
      "\n",
      "Epoch 74 of 1000: Train Loss = 0.198611 | Val Loss = 0.186369 | Eta = 0.000114\n",
      "\n",
      "Epoch 75 of 1000: Train Loss = 0.194872 | Val Loss = 0.188439 | Eta = 0.000113\n",
      "\n",
      "Epoch 76 of 1000: Train Loss = 0.203047 | Val Loss = 0.185735 | Eta = 0.000111\n",
      "\n",
      "Epoch 77 of 1000: Train Loss = 0.221082 | Val Loss = 0.186602 | Eta = 0.000110\n",
      "\n",
      "Epoch 78 of 1000: Train Loss = 0.218298 | Val Loss = 0.187866 | Eta = 0.000109\n",
      "\n",
      "Epoch 79 of 1000: Train Loss = 0.218617 | Val Loss = 0.186774 | Eta = 0.000107\n",
      "\n",
      "Epoch 80 of 1000: Train Loss = 0.206343 | Val Loss = 0.185165 | Eta = 0.000106\n",
      "\n",
      "Epoch 81 of 1000: Train Loss = 0.212049 | Val Loss = 0.184742 | Eta = 0.000105\n",
      "\n",
      "Epoch 82 of 1000: Train Loss = 0.214179 | Val Loss = 0.187291 | Eta = 0.000104\n",
      "\n",
      "Epoch 83 of 1000: Train Loss = 0.210478 | Val Loss = 0.184583 | Eta = 0.000102\n",
      "\n",
      "Epoch 84 of 1000: Train Loss = 0.211685 | Val Loss = 0.183794 | Eta = 0.000101\n",
      "\n",
      "Epoch 85 of 1000: Train Loss = 0.197835 | Val Loss = 0.185615 | Eta = 0.000100\n",
      "\n",
      "Epoch 86 of 1000: Train Loss = 0.193825 | Val Loss = 0.184986 | Eta = 0.000099\n",
      "\n",
      "Epoch 87 of 1000: Train Loss = 0.192004 | Val Loss = 0.186055 | Eta = 0.000098\n",
      "\n",
      "Epoch 88 of 1000: Train Loss = 0.203212 | Val Loss = 0.185392 | Eta = 0.000097\n",
      "\n",
      "Epoch 89 of 1000: Train Loss = 0.209959 | Val Loss = 0.185766 | Eta = 0.000096\n",
      "\n",
      "Epoch 90 of 1000: Train Loss = 0.210069 | Val Loss = 0.182064 | Eta = 0.000094\n",
      "\n",
      "Epoch 91 of 1000: Train Loss = 0.199663 | Val Loss = 0.182899 | Eta = 0.000093\n",
      "\n",
      "Epoch 92 of 1000: Train Loss = 0.190153 | Val Loss = 0.184665 | Eta = 0.000092\n",
      "\n",
      "Epoch 93 of 1000: Train Loss = 0.212400 | Val Loss = 0.184257 | Eta = 0.000092\n",
      "\n",
      "Epoch 94 of 1000: Train Loss = 0.208727 | Val Loss = 0.184224 | Eta = 0.000091\n",
      "\n",
      "Epoch 95 of 1000: Train Loss = 0.193904 | Val Loss = 0.185513 | Eta = 0.000090\n",
      "\n",
      "Epoch 96 of 1000: Train Loss = 0.215567 | Val Loss = 0.184133 | Eta = 0.000089\n",
      "\n",
      "Epoch 97 of 1000: Train Loss = 0.194807 | Val Loss = 0.184328 | Eta = 0.000088\n",
      "\n",
      "Epoch 98 of 1000: Train Loss = 0.193834 | Val Loss = 0.184032 | Eta = 0.000087\n",
      "\n",
      "Epoch 99 of 1000: Train Loss = 0.208173 | Val Loss = 0.184134 | Eta = 0.000086\n",
      "\n",
      "Epoch 100 of 1000: Train Loss = 0.209688 | Val Loss = 0.183605 | Eta = 0.000085\n",
      "\n",
      "Epoch 101 of 1000: Train Loss = 0.195217 | Val Loss = 0.184424 | Eta = 0.000084\n",
      "\n",
      "Epoch 102 of 1000: Train Loss = 0.228446 | Val Loss = 0.184168 | Eta = 0.000084\n",
      "\n",
      "Epoch 103 of 1000: Train Loss = 0.215234 | Val Loss = 0.183303 | Eta = 0.000083\n",
      "\n",
      "Epoch 104 of 1000: Train Loss = 0.200506 | Val Loss = 0.182565 | Eta = 0.000082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 105 of 1000: Train Loss = 0.196622 | Val Loss = 0.184021 | Eta = 0.000081\n",
      "\n",
      "Epoch 106 of 1000: Train Loss = 0.202427 | Val Loss = 0.183888 | Eta = 0.000080\n",
      "\n",
      "Epoch 107 of 1000: Train Loss = 0.192231 | Val Loss = 0.184459 | Eta = 0.000080\n",
      "\n",
      "Epoch 108 of 1000: Train Loss = 0.212814 | Val Loss = 0.182692 | Eta = 0.000079\n",
      "\n",
      "Epoch 109 of 1000: Train Loss = 0.208989 | Val Loss = 0.181601 | Eta = 0.000078\n",
      "\n",
      "Epoch 110 of 1000: Train Loss = 0.185947 | Val Loss = 0.185109 | Eta = 0.000078\n",
      "\n",
      "Epoch 111 of 1000: Train Loss = 0.210520 | Val Loss = 0.183993 | Eta = 0.000077\n",
      "\n",
      "Epoch 112 of 1000: Train Loss = 0.225236 | Val Loss = 0.182629 | Eta = 0.000076\n",
      "\n",
      "Epoch 113 of 1000: Train Loss = 0.205437 | Val Loss = 0.181615 | Eta = 0.000076\n",
      "\n",
      "Epoch 114 of 1000: Train Loss = 0.193911 | Val Loss = 0.182211 | Eta = 0.000075\n",
      "\n",
      "Epoch 115 of 1000: Train Loss = 0.207580 | Val Loss = 0.181544 | Eta = 0.000074\n",
      "\n",
      "Epoch 116 of 1000: Train Loss = 0.209943 | Val Loss = 0.181170 | Eta = 0.000074\n",
      "\n",
      "Epoch 117 of 1000: Train Loss = 0.211536 | Val Loss = 0.182312 | Eta = 0.000073\n",
      "\n",
      "Epoch 118 of 1000: Train Loss = 0.204402 | Val Loss = 0.182068 | Eta = 0.000072\n",
      "\n",
      "Epoch 119 of 1000: Train Loss = 0.205380 | Val Loss = 0.182575 | Eta = 0.000072\n",
      "\n",
      "Epoch 120 of 1000: Train Loss = 0.186359 | Val Loss = 0.181433 | Eta = 0.000071\n",
      "\n",
      "Epoch 121 of 1000: Train Loss = 0.211468 | Val Loss = 0.183701 | Eta = 0.000071\n",
      "\n",
      "Epoch 122 of 1000: Train Loss = 0.211614 | Val Loss = 0.183696 | Eta = 0.000070\n",
      "\n",
      "Epoch 123 of 1000: Train Loss = 0.195501 | Val Loss = 0.181712 | Eta = 0.000070\n",
      "\n",
      "Epoch 124 of 1000: Train Loss = 0.199953 | Val Loss = 0.182096 | Eta = 0.000069\n",
      "\n",
      "Epoch 125 of 1000: Train Loss = 0.206422 | Val Loss = 0.183388 | Eta = 0.000068\n",
      "\n",
      "Epoch 126 of 1000: Train Loss = 0.192749 | Val Loss = 0.182625 | Eta = 0.000068\n",
      "\n",
      "Epoch 127 of 1000: Train Loss = 0.200878 | Val Loss = 0.182488 | Eta = 0.000067\n",
      "\n",
      "Epoch 128 of 1000: Train Loss = 0.208784 | Val Loss = 0.181846 | Eta = 0.000067\n",
      "\n",
      "Epoch 129 of 1000: Train Loss = 0.183957 | Val Loss = 0.181221 | Eta = 0.000066\n",
      "\n",
      "Epoch 130 of 1000: Train Loss = 0.190600 | Val Loss = 0.181566 | Eta = 0.000066\n",
      "\n",
      "Epoch 131 of 1000: Train Loss = 0.198776 | Val Loss = 0.181847 | Eta = 0.000065\n",
      "\n",
      "Epoch 132 of 1000: Train Loss = 0.202852 | Val Loss = 0.181298 | Eta = 0.000065\n",
      "\n",
      "Epoch 133 of 1000: Train Loss = 0.214399 | Val Loss = 0.181614 | Eta = 0.000064\n",
      "\n",
      "Epoch 134 of 1000: Train Loss = 0.189228 | Val Loss = 0.182170 | Eta = 0.000064\n",
      "\n",
      "Epoch 135 of 1000: Train Loss = 0.201160 | Val Loss = 0.182435 | Eta = 0.000063\n",
      "\n",
      "Epoch 136 of 1000: Train Loss = 0.202357 | Val Loss = 0.182158 | Eta = 0.000063\n",
      "\n",
      "Epoch 137 of 1000: Train Loss = 0.220144 | Val Loss = 0.182314 | Eta = 0.000063\n",
      "\n",
      "Epoch 138 of 1000: Train Loss = 0.193769 | Val Loss = 0.182035 | Eta = 0.000062\n",
      "\n",
      "Epoch 139 of 1000: Train Loss = 0.194337 | Val Loss = 0.183477 | Eta = 0.000062\n",
      "\n",
      "Epoch 140 of 1000: Train Loss = 0.188033 | Val Loss = 0.183048 | Eta = 0.000061\n",
      "\n",
      "Epoch 141 of 1000: Train Loss = 0.202502 | Val Loss = 0.183455 | Eta = 0.000061\n",
      "\n",
      "Epoch 142 of 1000: Train Loss = 0.198419 | Val Loss = 0.182844 | Eta = 0.000060\n",
      "\n",
      "Early Stopping because the validation loss change between two consecutive epochs is less than 0.001000 over the last 20 iterations\n"
     ]
    }
   ],
   "source": [
    "m2.fit(Xr_train, yr_train, validation_fraction=0.1, max_iter=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 960x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m2.plotLearningCurve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2001722249356165"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=m2.predict(Xr_test)\n",
    "MSE(yr_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
