{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "In this Jupyter Notebook, we code a Multi-Layer Perceptron (MLP) for classification and regression tasks using Stochastic Gradient Descent for training the model. The MLP would be able to perform multiclass as well as binary classification. <br>\n",
    "\n",
    "The model would also allow customization of activation function used, as well as training schedule for optimization purposes. The MLP is designed with a single hidden layer architecture. <br>\n",
    "\n",
    "This notebook is dvided into three sections\n",
    "1. Section 1 - Helper functions and multiclass classification\n",
    "1. Section 2 - Binary classification\n",
    "1. Section 3 - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "from math import exp\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the following function that creates a weight matrix and initializes it with small random real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeights(input_neurons,output_neurons):\n",
    "    W = np.random.randn(output_neurons,input_neurons) * 0.01\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement the logistic sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement the ReLU (rectified linear unit) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement the tanh (hyperbolic tangent) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implement a MLP Classifier model class for performing multi-class classification.\n",
    "\n",
    "### The MLP Classifier has a single hidden layer. It should have the following four methods.The model uses the back-propagation algorithm for learning the weights of the features/neurons. Note the that “fit” method should implement the Stochastic Gradient Descent algorithm for optimizing the weight update process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function for one_hot_labels\n",
    "Input:\n",
    "    Y - target vector\n",
    "Output:\n",
    "    one_hot_matrix\n",
    "'''\n",
    "def one_hot_labels(Y):\n",
    "    \n",
    "    # Get unique labels in Y and number of observations\n",
    "    unique_labels = np.unique(Y)\n",
    "    n = len(Y)\n",
    "    \n",
    "    # Create one hot matrix\n",
    "    labels = np.array(list(unique_labels)).reshape(len(unique_labels),1)\n",
    "    one_hot_matrix = np.apply_along_axis(lambda x: np.full((n,),x),1,labels)\n",
    "    one_hot_matrix = np.apply_along_axis(lambda x: (x==Y).astype(int),1,one_hot_matrix).T\n",
    "    \n",
    "    return one_hot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function for softmax calculation\n",
    "Given matrix A (output from hidden layer), outputs probabilties   \n",
    "'''\n",
    "def softmax(A):\n",
    "    A = np.apply_along_axis(np.exp,0,A)\n",
    "    A = np.apply_along_axis(lambda x: x/(sum(x)),1,A)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function that determines the number of input layers\n",
    "and the number of layers\n",
    "Input:\n",
    "    X - feature matrix\n",
    "    Y - one_hot_matrix\n",
    "    n_h - number of hidden layers\n",
    "Output:\n",
    "    n_o - tuple containing the values\n",
    "'''\n",
    "def size_of_layers(X,Y,n_h):\n",
    "    n_i = X.shape[1]\n",
    "    n_o = Y.shape[1]\n",
    "    \n",
    "    return n_i, n_h, n_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function to initialize parameters of the MLP\n",
    "This assumes a single hidden layer architecture\n",
    "Bias is included\n",
    "Input:\n",
    "    n_i - size of input layer (# of X_train columns)\n",
    "    n_h - size of hidden layer neurons\n",
    "    n_o - size of output layer neurons (# number of classes)\n",
    "Output:\n",
    "    parameters - Dictionary of Parameters\n",
    "'''\n",
    "def initializeParameters(n_i, n_h, n_o):\n",
    "    # Weights of each layer\n",
    "    W_i = initializeWeights(n_i,n_h)\n",
    "    W_h = initializeWeights(n_h,n_o)\n",
    "    \n",
    "    # Bias of each layer\n",
    "    b_i = np.zeros((n_h,1))\n",
    "    b_h = np.zeros((n_o,1))\n",
    "    \n",
    "    parameters = {\n",
    "        'W_input': W_i,\n",
    "        'b_input': b_i,\n",
    "        'W_hidden': W_h,\n",
    "        'b_hidden': b_h\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = one_hot_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_i, n_h, n_o = size_of_layers(X,Y,4)\n",
    "params = initializeParameters(n_i, n_h, n_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Forward propagation function given parameters, feature matrix and\n",
    "activation function and function to calculate probabilities\n",
    "Returns value after passing through hidden layer A2 and caching for backprop\n",
    "\n",
    "Input:\n",
    "    parameters - parameters\n",
    "    X - feature matrix\n",
    "    activation_function\n",
    "    classifier - sigmoid or softmax for output layer\n",
    "Output:\n",
    "    A_h - output of hidden layer\n",
    "    cache\n",
    "'''\n",
    "def forward_propagation(parameters, X, activation_function, classifier=softmax):\n",
    "    # Parameters definition\n",
    "    W_i = parameters['W_input']\n",
    "    b_i = parameters['b_input']\n",
    "    W_h = parameters['W_hidden']\n",
    "    b_h = parameters['b_hidden']\n",
    "    \n",
    "    # Computation of Input Layer\n",
    "    Z_i = np.dot(W_i,X.T) + b_i\n",
    "    A_i = activation_function(Z_i)\n",
    "    \n",
    "    # Computation of Hidden Layer\n",
    "    Z_h = np.dot(W_h,A_i) + b_h\n",
    "    A_h = classifier(Z_h.T)\n",
    "    \n",
    "    # Caching \n",
    "    cache = {\n",
    "        'Z_i': Z_i,\n",
    "        'A_i': A_i,\n",
    "        'Z_h': Z_h,\n",
    "        'A_h': A_h\n",
    "    }\n",
    "    \n",
    "    return A_h, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, cache = forward_propagation(params, X, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function to calculate the cross entropy cost function\n",
    "Input:\n",
    "    pred - predicted values\n",
    "    Y - actual values\n",
    "Output:\n",
    "    cost\n",
    "'''\n",
    "def cost(pred, Y):\n",
    "    cost = np.multiply(Y, np.log(pred)) + np.multiply((1-Y), np.log(pred))\n",
    "    cost = -np.sum(cost)/Y.shape[0]\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    # Number of samples\n",
    "    n = Y.shape[0]\n",
    "    \n",
    "    # Parameters of model\n",
    "    W_i = parameters['W_input']\n",
    "    b_i = parameters['b_input']\n",
    "    W_h = parameters['W_hidden']\n",
    "    b_h = parameters['b_hidden']\n",
    "    \n",
    "    # Retrieve Cached Calculations\n",
    "    A_i = cache['A_i']\n",
    "    A_h = cache['A_h']\n",
    "    b_i = cache['b_i']\n",
    "    b_h = cache['b_h']\n",
    "    \n",
    "    # Calculation of gradients \n",
    "    dZ_h = A_h - Y\n",
    "    dW_h = np.dot(dZ_h, A_i.T)/n\n",
    "    #db_h = np.sum(dZ_h)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Read the handwritten digits datasetusing the sklearn.datasets.load_digits function for performing multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Partition the data into train and test set. Use the “Partition” function from your previous assignment or from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Hyperparameter tuning based on certain fixed-values hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Report on performance of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Implement binary classification module in theMLPClassifier. Then, performbinary classification on the handwritten digits dataset to recognize the digits “5” and “not-5”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Hyperparameter tuning based on certain fixed-values hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Report performance on model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 (Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implement a Multi-Layer Perceptron regressor model (a MLP Regressor class) with a single hidden layer. The model implements the backpropagation algorithm. To optimize the process of updating the weight matrices, it uses the Stochastic Gradient Descent (SGD) algorithm with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Hyperparameter tuning based on certain fixed-values hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Report performance on model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
